---
title: 大数据
date: 2019-08-09 20:32:59
password: 123
abstract: 欢迎来到test, 请输入密码.
message: 欢迎来到test, 请输入密码.
toc: false
mathjax: false
tags:  [汇总] 
category: 汇总
---

# scala

## 一.scala的maven在IDE中的搭建

1.开始创建项目体系结构
File --> Project 

![](https://gitee.com/chenjinhua_939598604/resources/raw/master/static/20190904175253.png)



![](https://gitee.com/chenjinhua_939598604/resources/raw/master/static/20190904175319.png)

![](https://gitee.com/chenjinhua_939598604/resources/raw/master/static/20190904175417.png)




![输入图片说明](https://static.oschina.net/uploads/img/201802/11153427_h2XW.png "在这里输入图片标题")

2.修改pom.xml

```
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>scala_maven</groupId>
  <artifactId>com.chen</artifactId>
  <version>1.0-SNAPSHOT</version>
  <inceptionYear>2008</inceptionYear>
  <properties>
    <scala.version>2.11.7</scala.version>
  </properties>

  <dependencies>
    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-library</artifactId>
      <version>${scala.version}</version>
    </dependency>

    <dependency>
      <groupId>com.typesafe.akka</groupId>
      <artifactId>akka-actor_2.11</artifactId>
      <version>2.5.3</version>
    </dependency>
    <dependency>
      <groupId>org.specs</groupId>
      <artifactId>specs</artifactId>
      <version>1.2.5</version>
      <scope>test</scope>
    </dependency>
  </dependencies>

  <build>
    <sourceDirectory>src/main/scala</sourceDirectory>
    <testSourceDirectory>src/test/scala</testSourceDirectory>
    <plugins>
      <plugin>
        <groupId>org.scala-tools</groupId>
        <artifactId>maven-scala-plugin</artifactId>
        <executions>
          <execution>
            <goals>
              <goal>compile</goal>
              <goal>testCompile</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <scalaVersion>${scala.version}</scalaVersion>
          <args>
            <arg>-target:jvm-1.5</arg>
          </args>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-eclipse-plugin</artifactId>
        <configuration>
          <downloadSources>true</downloadSources>
          <buildcommands>
            <buildcommand>ch.epfl.lamp.sdt.core.scalabuilder</buildcommand>
          </buildcommands>
          <additionalProjectnatures>
            <projectnature>ch.epfl.lamp.sdt.core.scalanature</projectnature>
          </additionalProjectnatures>
          <classpathContainers>
            <classpathContainer>org.eclipse.jdt.launching.JRE_CONTAINER</classpathContainer>
            <classpathContainer>ch.epfl.lamp.sdt.launching.SCALA_CONTAINER</classpathContainer>
          </classpathContainers>
        </configuration>
      </plugin>
    </plugins>
  </build>
  <reporting>
    <plugins>
      <plugin>
        <groupId>org.scala-tools</groupId>
        <artifactId>maven-scala-plugin</artifactId>
        <configuration>
          <scalaVersion>${scala.version}</scalaVersion>
        </configuration>
      </plugin>
    </plugins>
  </reporting>
</project>

```

> **注意：如果有akka-actor的话  要和scala的版本相对应**
>

![](https://static.oschina.net/uploads/space/2018/0211/155017_Suab_3005534.png)





## 二.scala的单词拆分

```
var word=Array("hello tom hello jelly","tom jelly","hello world hello tom","hello jelly","hello tom")
var b=word.flatMap(_.split(" ")).map((_,1)).groupBy(_._1).map(m=>(m._1,m._2.map(x=>x._2).sum)).toArray
b.sortWith(_._2 > _._2).toMap
res33: scala.collection.immutable.Map[String,Int] = Map(hello -> 6, tom -> 4, jelly -> 3, world -> 1)
```

或者
```
scala> b.toSeq.sortWith(_._2>_._2).toMap
res32: scala.collection.immutable.Map[String,Int] = Map(hello -> 6, tom -> 4, jelly -> 3, world -> 1)
```

扩展：
```
a=Map()//数据清空使用再次new
println(a.size)
a.toSeq.sortBy(_._1)//升序排序 key
a.toSeq.sortBy(_._2)//升序排序 value
a.toSeq.sortWith(_._1>_._1) //降序排序 key
a.toSeq.sortWith(_._2>_._2) //降序排序 value
```

```
scala> var word=Array("hello tom hello jelly","tom jelly","hello world hello tom","hello jelly","hello tom")

scala> word.flatMap(_.split(" "))  //将数组中每个元素，按照空格切分并且扁平化
res34: Array[String] = Array(hello, tom, hello, jelly, tom, jelly, hello, world, hello, tom, hello, jelly, hello, tom)

scala> word.flatMap(_.split(" ")).map((_,1))  //将数组中每个单词，转成元组，并标记1
res35: Array[(String, Int)] = Array((hello,1), (tom,1), (hello,1), (jelly,1), (tom,1), (jelly,1), (hello,1), (world,1), (hello,1), (tom,1), (hello,1), (jelly,1), (hello,1), (tom,1))

scala> word.flatMap(_.split(" ")).map((_,1)).groupBy(_._1)//将元组的数组中按照元组的第一个元素排序
res36: scala.collection.immutable.Map[String,Array[(String, Int)]] = Map(world -> Array((world,1)), tom -> Array((tom,1), (tom,1), (tom,1), (tom,1)), hello -> Array((hello,1), (hello,1), (hello,1), (hello,1), (hello,1), (hello,1)), jelly -> Array((jelly,1), (jelly,1), (jelly,1)))

scala> word.flatMap(_.split(" ")).map((_,1)).groupBy(_._1).map(m=>(m._1,m._2.map(x=>x._2).sum)) //将排序后的元组进行数值求和
res37: scala.collection.immutable.Map[String,Int] = Map(world -> 1, tom -> 4, hello -> 6, jelly -> 3)

scala> word.flatMap(_.split(" ")).map((_,1)).groupBy(_._1).map(m=>(m._1,m._2.map(x=>x._2).sum)).toArray
//将排序求和后的map转化成元组方便排序
res38: Array[(String, Int)] = Array((world,1), (tom,4), (hello,6), (jelly,3))

scala> word.flatMap(_.split(" ")).map((_,1)).groupBy(_._1).map(m=>(m._1,m._2.map(x=>x._2).sum)).toArray.sortWith(_._2 > _._2) //将排序求和后的map转化成元组排序
res39: Array[(String, Int)] = Array((hello,6), (tom,4), (jelly,3), (world,1))

scala> word.flatMap(_.split(" ")).map((_,1)).groupBy(_._1).map(m=>(m._1,m._2.map(x=>x._2).sum)).toArray.sortWith(_._2 > _._2).toMap //转化回来map
res40: scala.collection.immutable.Map[String,Int] = Map(hello -> 6, tom -> 4, jelly -> 3, world -> 1)



```

##  三.数组常见方法汇总

**array学习笔记**

数组要点
1.若长度固定则使用Array，若长度可能有变化则使用ArrayBuffer
2.提供初始值时不要使用new
3.用()来访问元素
4.用for(elem <- arr) 来遍历元素
5.用for(elem <- array if ...) ... yield 来将原数组转型为新数组
6.Scala数组和Java数组可以互操作，用ArrayBuffer，使用scala.collection.JavaConversions中的转换函数。
定长数组 如果数组长度不变，则可使用scala中的Array，例如：

```
val nums = new Array[Int](10)    //10个整数的数组，所有元素初始化为0
val string = new Array[String](10) //10个元素的字符串数组，所有元素被初始化为null
val s = Array("hello","scala")  //长度为2的Array[String]——类型是推断出来的。已提供初始值，不需要new
```
变长数组
对于那种长度按需要变化的数组，Java有ArrayList，C++有vector。Scalable中有等效的数据结构为：ArrayBuffer
```
import scala.collection.mutable.ArrayBuffer
val b = ArrayBuffer[Int]()
//或者new ArrayBuffer[Int]
//一个空的数组缓冲，准备存放整数
b += 1   //ArrayBuffer(1),用+=在尾部添加元素
b += (1,2,3,4)     //在尾部添加多个元素
 
b ++= Array(8,12,13)   //可以用++=操作符追加任何集合
b.trimEnd(5)  //移除最后5个元素

//在任意 位置添加元素
b.insert(2,6)   //在下标2之前插入
b.insert(2,6,7,8)   //在下标2之前插入6,7,8

b.remove(2)   //移除下标为2的位置开始移除元素
b.remove(2,3)  //从下标为2开始移除3个元素；第二个参数是表示移除元素的个数
```
在使用时，有时不确定数组需要装元素的个数。此时，可以先构建一个数组缓冲，然后调用
```
b.toArray   //将缓冲数组转换为定长数组
```
定长数组也可以转换为缓冲数组
```
a.toBuffer
```
遍历数组和数组缓冲

使用for循环遍历数组和数组缓冲
使用下标的方式
```
for (i <- 0 until a.length){
    println( i + ":" + a(i))
}
```
until用法扩展：这只步长--> 0 until (a.length,2)  从数组尾部开始-->(0 until a.length).reverse
不使用下标访问数组元素
```
for (elem <- arrName) {println(elem)}
```
数组转换
```
val a = Array(2,3,4)
val result = for (elem <- a) yield 2 * elem
for (elem <-  a if elem %2==0) yield 2 * elem
```
常用算法
```
sum: Array(1,2,3).sum
max/min : Array(1,2,3).max/min
sorted : Array(1,2,3).sorted(_ < ) ; Array(1,2,3).sorted( > _) //不能对缓冲数组排序
quickSort方法排序：scala.util.Sorting.quickSort(a)
显示数组内容：mkString; a.mkString(" and ") //可以设置分隔符
```

1、定长数组定义：
```
//定义一个长度为10的数值数组
scala> val numberArray = new Array[int](10)
numberArray:Array[Int] = Array(0,0,0,0,0,0,0,0,0,0)
//定义一个长度为10的String类数组
scala> val strArray = new Array[String](10)
strArray:Array[String] = Array(null, null, null, null, null, null, null, null, null, null)

//由上可以看出，复杂对象类型在数组定义时被初始化为null，数值型呗初始化为0，并且上面复杂类型定义的时候必须加new，否则会报错

//提供初始值的定义数组
scala> val strArray2 = Array("First", "Second")  //这里说明已提供初始值就不需要new
strArray2:Array[String] = Array(First, Second)

scala> strArray2(0) = "Goodbye"
strArray2:Array[String] = Array(Goodbye, Second)

```
2、变长数组定义

```
对于长度需要变化的数组，Java有ArrayList,C++有vector。Scala中的等效数据结构为ArrayBuffer

//导入可变包，Scala中的可变集合都是放在mutable中，使用时要导入
scala> import scala.collection.mutable.ArrayBuffer
import scala.collection.mutable.ArrayBuffer

scala> val arrayBuffer = ArrayBuffer[Int]()
arrayBuffer: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()

//在尾部添加一个值
scala> arrayBuffer += 1
res17: arrayBuffer.type = ArrayBuffer(1)

//在尾部添加多个元素
scala> arrayBuffer += (2, 3, 4, 5)
res19: arrayBuffer.type = ArrayBuffer(1, 2, 3, 4, 5)

//在尾部添加一个集合
scala> arrayBuffer ++= Array(6, 7, 8, 9)
res20: arrayBuffer.type = ArrayBuffer(1, 2, 3, 4, 5, 6, 7, 8, 9)

//移除最后2个元素
scala> arrayBuffer.trimEnd(2)

//在开头移除1一个元素
scala> arrayBuffer.trimStart(2)

scala> arrayBuffer
res23: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(2, 3, 4, 5, 6, 7)


//在任意位置插入或者删除元素
scala> arrayBuffer.insert(2, 6)
//ArrayBuffer(2, 3, 6, 4, 5, 6, 7)

scala> arrayBuffer.insert(1, 2, 3, 4)
//ArrayBuffer(2, 1, 2, 3, 4, 3, 6, 4, 5, 6, 7)

scala> arrayBuffer.remove(2)
//ArrayBuffer(2, 1, 3, 4, 3, 6, 4, 5, 6, 7)

scala> arrayBuffer.remover(1, 8)
//ArrayBuffer(2, 7)

```
3、变长数组和定长数组转换

```
//变长转换长定长
scala > arrayBuffer.toArray
//Array(2, 7)

//定长转换成变长
scala>res7.toBuffer
//ArrayBuffer(2, 7)

```
4、遍历定长和变长数组

```
for(i <- 0 until.arrayBuffer.length)
    println(i + ": " + a(i))
0 until.arrayBuffer.length实际上是一个方法调用，返回的是一个区间Range： 0.until(arrayBuffer.length)
for(i <- 区间)会让变量i遍历该区间的所有值
如果想要在区间中步长不为1，则：0 until (arrayBuffer.length, 2)
如果想要数组从尾端开始，则遍历的写法为:(0 until (arrayBuffer.length, 2)).reverse

Scala也提供了一个和Java增强for循环类似的for

//增强for
for(i <- arrayBuffer)
    println(i + ": " + a(i))

```
5、数组转换

在《Scala入门学习笔记二-基本数据类型、程序控制结构》提到在for循环推导式，可以利用原来的数组产生一个新的数组。
```
scala> val a = Array(2, 3, 5, 7, 11)
a: Array[Int] = Array(2, 3, 5, 7, 11)
//这里产生了一个新的数组，原来的数组也在
scala> val result = for(elem <- a) yield 2 * elem
result: Array[Int] = Array(4, 6, 10, 14, 22)
如果for中使用的是定长数组，则for(...)...yield之后得到的是定长数组;如果使用的是变长数组，则会得到变长数组

Scala也提供了另外一种做法
scala> a.filter(_ % 2 == 0).map(2 * _)

甚至
scala>a.filter(_ % 2 == 0).map{2 * _}
例子：
给定一个整数的缓冲数组，我们想要移除第一个负数之外的所有负数。有几种做法

//第一种做法：
var first = true
var n = a.length
var i = 0
while(i < n){
    if(a(i) > 0) i += 1
    else{
        if(first) {first = false; i += 1}
        else {a.remove(i); n-= 1}
    }
}

//第二种做法：
//首先使用一个新数组用于记录满足条件的数组的下标
val first = true
val indexes = for(i <- 0 until a.length if first || a(i) > 0) yield {
    if(a(i) < 0) first = false; i
}
//然后将元素移动到该去的位置，截断尾端
for(j <- o until indexes.length) a(j) = a(indexes(j))
a.trimEnd(a.length-indexes.length)
```
6、常用算法
Scala针对数组提供了一个常用的函数
```
//定义一个整型数组
scala> val intArr=Array(1,2,3,4,5,6,7,8,9,10)
intArr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

//求和
scala> intArr.sum
res87: Int = 55

//求最大值
scala> intArr.max
res88: Int = 10

scala> ArrayBuffer("Hello","Hell","Hey","Happy").max
res90: String = Hey

//求最小值
scala> intArr.min
res89: Int = 1

//排序
//sorted方法将数组或数组缓冲排序并返回经过排序的数组或数组缓冲，原始数组被保留
scala>val b = ArrayBuffer(1, 7, 2, 9)
b:ArrayBuffer[Int] = ArrayBuffer(1, 7, 2, 9)
scala>val bSorted = b.sorted(_<_) 
bSorted: ArrayBuffer[Int] = ArrayBuffer(1, 2, 7, 9)

//toString()方法
scala> intArr.toString()
res94: String = [I@141aba8

//mkString()方法
scala> intArr.mkString(",")
res96: String = 1,2,3,4,5,6,7,8,9,10

scala> intArr.mkString("<",",",">")
res97: String = <1,2,3,4,5,6,7,8,9,10>
```
7、ArrayBuffer Scaladoc解析
```
初学者在查看sacaladoc时常常会感到困惑，不用担心，随着学习的深入，api文档中的内容将逐渐清晰
下面给出两个示例：
++=方法传入的参数类型是TraversableOnce Trait的子类，它返回的是更新好的ArrayBuffer

++=方法解析

dropWhile传入的是一个函数，该函数返回值是布尔类型，dropWhile反回的是操作后的ArrayBuffer

dropWith方法解析
```
8、多维数组
和Java一样，多维数组是通过数组的数组来实现的。
```
//第一种构造方式
val metrix = Array.ofDim[Double](3, 4) //3行 4列

//访问其中的元素
metrix(row)(column)  =42

//可以创建不规则的数组，每一行的长度不相同
val triangle = new Array[Array[Int]](10)
for(i <- 0 until triangle.length)
    trianglr(i) = new Array[Int](i+1)

//在创建的时候赋值
scala> val metrix = Array(Array(1, 2, 3), Array(2.3, 3.4), Array("asdf", "asdfas"))
metrix: Array[Array[_ >: String with Double with Int]] = Array(Array(1, 2, 3), Array(2.3, 3.4), Arra
y(asdf, asdfas))

//打印输出数组
scala> for(i <- metrix) println(i.mkString(" "))
1 2 3
2.3 3.4
asdf asdfas

//输出二维数组的每个值
scala> for(i <- metrix; from = i; j <- from) println(j)
1
2
3
2.3
3.4
asdf
asdfas
```

## 四.数组操作(二)

Scala数组操作：

1.定长数组
 长度不变的数组的声明：

```
//长度为10的整数数组，所有元素初始化为0
 val numArr = new Array[Int](10)

//长度为10的字符串数组，所有元素初始化为null
val numArr = new Array[String](10)

//长度为2的数组，数据类型自动推断出来，已经提供初始值就不需要new关键字
val s = Array("cai","yong")

//通过ArrayName(index)访问数组元素和更改数组元素
val s = Array("cai","yong")
 println(s(0))
s(0) = "haha"
println(s(0))
输出：
 cai
 haha

```
2.变长数组：数组缓冲
 Scala也支持长度变化的数组，支持的数据结构是ArrayBuffer
```
//一个空的数组缓冲，准备存放整数
 val ab = ArrayBuffer[Int]()
 val ab2 = new ArrayBuffer[Int]

//用+=在尾部添加元素
ab += 2

//在尾部添加多个元素
ab += (1,2,3,4,5)

//通过++=往数组缓冲后面追加集合
 ab ++= Array(6,7,8,9)

//使用trimEnd(n)移除尾部n个元素
ab.trimEnd(3)

//在下标3之前插入元素
ab.insert(3, 33)

//插入多个元素，第一个值为index，后面所有的值为要插入的值
ab.insert(3,3,4,5,6)

//移除某个位置的元素
ab.remove(3)

//移除从下标为n开始（包括n）的count个元素
ab.remove(n, count)
```
 有时候需要构造一个Array，但是不知道具体要存放多少元素，可以先构造ArrayBuffer,再调用toArray方法转化成Array，同样，对Array调用toBuffer方法可以转成ArrayBuffer.

 注：在数组缓冲的尾部进行元素添加移除操作的效率很高，但是在任意位置插入或移除元素的效率并不太高效，因为涉及到数组元素的移动。

3.遍历数组
```
//for循环遍历
for(i <- 0 until ab.length){
 print(ab(i) + ", ")
 }

//根据特定步长遍历数组
for(i <- 0 until (ab.length, 2)){
 print(ab(i) + ", ")
 }

//从数组的尾部开始向前遍历数组
for(i <- (0 until ab.length).reverse){
 print(ab(i) + ", ")
}

//类似于Java中的foreach遍历数组
 for(elem <- ab){
 print(elem + ", ")
}
```

4.数组转换
```
//进行数组转换会生成一个新的数组，而不会修改原始数组
 val change = for(elem <- ab) yield elem * 2
for(elem <- change){
print(elem + ", ")
 }

//添加一个守卫的数组转换
val change = for(elem <- ab if elem%2 == 0) yield elem * 2
```
5.数组操作常用算法
```
//sum求和(数组与阿奴必须是数值型数据)
println(change.sum)

//min max 输出数组中最小和最大元素
println(change.min)
println(change.max)

//使用sorted方法对数组或数组缓冲进行升序排序，这个过程不会修改原始数组
 val sortArr = ab.sorted 
 for(elem <- sortArr)
 print(elem + ", ")

//使用比较函数sortWith进行排序
val sortArr = ab.sortWith(_>_)

//数组显示
 println(sortArr.mkString("|"))
 println(sortArr.mkString("startFlag","|","endFlag"))
```
6.多维数组
```
//构造一个2行3列的数组
val arr = Array.ofDim[Int](2,3)
println(arr.length)
println(arr(0).length)
arr(0)(0) = 20
println(arr(0)(0))

//创建长度不规则的数组
val arr = new Array[Array[Int]](3)
      
 for(i <- 0 until arr.length){
arr(i) = new Array[Int](i + 2)
}
      
for(i <- 0 until arr.length){
println(arr(i).length)
}
```

## 五.Tuple常见方法汇总

```
tuple的定义

对偶是元组(tuple)的最简单形态——元组是不同类型的值的聚集。
元组的值是通过将单个值包含在圆括号中构成。Example：（1，1.3415，“Fred”)
tuple的访问

可以通过_1,_2,_3访问元组的元素
val first = tuple._1 //元组的位置从1开始，而非从0开始
```
与列表一样，元组也是不可变的，但与列表不同的是元组可以包含不同类型的元素。
元组的值是通过将单个的值包含在圆括号中构成的。例如：
```
val t = (1, 3.14, "Fred")  
```
以上实例在元组中定义了三个元素，对应的类型分别为[Int, Double, java.lang.String]。
此外我们也可以使用以上方式来定义：
```
val t = new Tuple3(1, 3.14, "Fred")
```
元组的实际类型取决于它的元素的类型，比如 (99, "runoob") 是 Tuple2[Int, String]。 ('u', 'r', "the", 1, 4, "me") 为 Tuple6[Char, Char, String, Int, Int, String]。
目前 Scala 支持的元组最大长度为 22。对于更大长度你可以使用集合，或者扩展元组。
访问元组的元素可以通过数字索引，如下一个元组：
```
val t = (4,3,2,1)
val sum = t._1 + t._2 + t._3 + t._4
println( "元素之和为: "  + sum )//10
```
迭代元组
```
val t = (4,3,2,1)
t.productIterator.foreach{ i =>println("Value = " + i )}
Value = 4
Value = 3
Value = 2
Value = 1
```
元组转为字符串
你可以使用 Tuple.toString() 方法将元组的所有元素组合成一个字符串，实例如下：
```
val t = new Tuple3(1, "hello", Console)
println("连接后的字符串为: " + t.toString() )
连接后的字符串为: (1,hello,scala.Console$@4dd8dc3)
```



## 六.List常见方法汇总

List的4种操作符的区别和联

(1):+和+: 两者的区别在于:+方法用于在尾部追加元素，+:方法用于在头部追加元素，和::很类似，但是::可以用于pattern match ，而+:则不行. 关于+:和:+,只要记住冒号永远靠近集合类型就OK了。
```
scala> a
res23: List[Int] = List(1, 2, 3, 4)

scala> var b=a:+9
b: List[Int] = List(1, 2, 3, 4, 9)

scala> var c=9:+a
<console>:15: error: value :+ is not a member of Int
       var c=9:+a
              ^
scala> var c=9+:a
c: List[Int] = List(9, 1, 2, 3, 4)

scala> var r1="A"+:"B"+:Nil
r1: List[String] = List(A, B)

scala>var r2=Nil:+"A":+"B"
r2: List[String] = List(A, B)
```
(2):: 该方法被称为cons，意为构造，向队列的头部追加数据，创造新的列表。用法为 x::list,其中x为加入到头部的元素，无论x是列表与否，它都只将成为新生成列表的第一个元素，也就是说新生成的列表长度为list的长度＋1(btw, x::list等价于list.::(x))
```
scala>"A"::"B"::Nil
res0: List[String] = List(A, B)

scala>List("A","B")::List("C","D")
res1: List[java.io.Serializable] = List(List(A, B), C, D)
```
(3) ++ 该方法用于连接两个集合，list1++list2
```
scala>List("A","B") ++ List("C","D")
res2: List[String] = List(A, B, C, D)
```
(4)::: 该方法只能用于连接两个List类型的集合
```
scala>List("A","B") ::: List("C","D")
res3: List[String] = List(A, B, C, D)
```
**List常用用法**

1）List类型定义以及List的特点：
```
//字符串类型List
scala> val fruit=List("Apple","Banana","Orange")
fruit: List[String] = List(Apple, Banana, Orange)

//前一个语句与下面语句等同
scala> val fruit=List.apply("Apple","Banana","Orange")
fruit: List[String] = List(Apple, Banana, Orange)

//数值类型List
scala> val nums=List(1,2,3,4,5)
nums: List[Int] = List(1, 2, 3, 4, 5)

//多重List，List的子元素为List
scala> val list = List(List(1, 2, 3), List("adfa", "asdfa", "asdf"))
list: List[List[Any]] = List(List(1, 2, 3), List(adfa, asdfa, asdf))

//遍历List
scala> for(i <- list; from=i; j<-from)println(j)
1
2
3
adfa
asdfa
asdf
```
（2）List与Array的区别：
```
1、List一旦创建，已有元素的值不能改变，可以使用添加元素或删除元素生成一个新的集合返回。
如前面的nums，改变其值的话，编译器就会报错。而Array就可以成功

scala>nums(3)=4
<console>:10: error: value update is not a member of List[Int]
              nums(3)=4
              ^
2、List具有递归结构(Recursive Structure),例如链表结构
List类型和气他类型集合一样，它具有协变性(Covariant),即对于类型S和T，如果S是T的子类型，则List[S]也是List[T]的子类型。
例如:

scala>var listStr:List[Object] = List("This", "Is", "Covariant", "Example")
listStr:List[Object] = List(This, Is, Covariant, Example)

//空的List,其类行为Nothing,Nothing在Scala的继承层次中的最底层
//,即Nothing是任何Scala其它类型如String,Object等的子类
scala> var listStr = List()
listStr:List[Nothing] = List()

scala>var listStr:List[String] = List()
listStr:List[String] = List()
```
（3）List常用构造方法
```
//1、常用::及Nil进行列表构建
scala> val nums = 1 :: (2:: (3:: (4 :: Nil)))
nums: List[Int] = List(1, 2, 3, 4)


//由于::操作符的优先级是从右向左的，因此上一条语句等同于下面这条语句
scala> val nums = 1::2::3::4::Nil
nums:List[Int] = List(1, 2, 3, 4)
至于::操作符的使用将在下面介绍
```
（4）List常用操作
```
//判断是否为空
scala> nums.isEmpty
res5: Boolean = false

//取第一个元素
scala> nums.head
res6: Int = 1

//取列表第二个元素
scala>nums.tail.head
res7: Int = 2

//取第三个元素
scala>nums.tail.tail.head
res8: Int = 3

//插入操作
//在第二个位置插入一个元素
scala>nums.head::(3::nums.tail)
res11: List[Int] = List(1, 3, 2, 3, 4)

scala> nums.head::(nums.tail.head::(4::nums.tail.tail))
res12: List[Int] = List(1, 2, 4, 3, 4)

//插入排序算法实现
def isort(xs: List[Int]):List[Int] = {
    if(xs.isEmpty) Nil
    else insert(xs.head, issort(xs.tail))
}

def insert(x:Int, xs:List[Int]):List[Int] = {
    if(xs.isEmpty || x <= xs.head) x::xs
    else xs.head :: insert(x, xs.tail)
}

//连接操作
scala>List(1, 2, 3):::List(4, 5, 6)
res13: List[Int] = List(1, 2, 3, 4, 5, 6)

//去除最后一个元素外的元素，返回的是列表
scala> nums.init
res13: List[Int] = List(1, 2, 3)

//取出列表最后一个元素
scala>nums.last
res14: Int = 4

//列表元素倒置
scala> nums.reverse
res15: List[Int] = List(4, 3, 2, 1)

//一些好玩的方法调用
scala> nums.reverse.reverse == nums


//丢弃前面n个元素
scala>nums drop 3
res16: List[Int] = List(4)

//获取前面n个元素
scala>nums take 1
res17: List[Int] = List[1]

//将列表进行分割
scala> nums.splitAt(2)
res18: (List[Int], List[Int]) = (List(1, 2),List(3, 4))

//前一个操作与下列语句等同
scala> (nums.take(2),nums.drop(2))
res19: (List[Int], List[Int]) = (List(1, 2),List(3, 4))

//Zip操作
scala> val nums=List(1,2,3,4)
nums: List[Int] = List(1, 2, 3, 4)

scala> val chars=List('1','2','3','4')
chars: List[Char] = List(1, 2, 3, 4)

//返回的是List类型的元组(Tuple），返回的元素个数与最小的List集合的元素个数一样
scala> nums zip chars
res20: List[(Int, Char)] = List((1,1), (2,2), (3,3), (4,4))

//List toString方法
scala> nums.toString
res21: String = List(1, 2, 3, 4)

//List mkString方法
scala> nums.mkString
res22: String = 1234

//转换成数组
scala> nums.toArray
res23: Array[Int] = Array(1, 2, 3, 4)
```
（5）List伴生对象方法
```
//apply方法
scala>  List.apply(1, 2, 3)
res24: List[Int] = List(1, 2, 3)

//range方法，构建某一值范围内的List
scala>  List.range(2, 6)
res25: List[Int] = List(2, 3, 4, 5)

//步长为2
scala>  List.range(2, 6,2)
res26: List[Int] = List(2, 4)

//步长为-1
scala>  List.range(2, 6,-1)
res27: List[Int] = List()

scala>  List.range(6,2 ,-1)
res28: List[Int] = List(6, 5, 4, 3)

//构建相同元素的List
scala> List.make(5, "hey")
res29: List[String] = List(hey, hey, hey, hey, hey)

//unzip方法
scala> List.unzip(res20)
res30: (List[Int], List[Char]) = (List(1, 2, 3, 4),List(1, 2, 3, 4))

//list.flatten，将列表平滑成第一个无素
scala> val xss =
     | List(List('a', 'b'), List('c'), List('d', 'e'))
xss: List[List[Char]] = List(List(a, b), List(c), List(d, e))
scala> xss.flatten
res31: List[Char] = List(a, b, c, d, e)

//列表连接
scala> List.concat(List('a', 'b'), List('c'))
res32: List[Char] = List(a
, b, c)
```
（6）::和:::操作符介绍
```
List中常用'::',发音为"cons"。Cons把一个新元素组合到已有元素的最前端，然后返回结果List。

scala> val twoThree = List(2, 3)
scala> val oneTwoThree = 1 :: twoThree
scala> oneTwoThree
oneTwoThree: List[Int] = List(1, 2, 3)
上面表达式"1::twoThree"中，::是右操作数，列表twoThree的方法。可能会有疑惑。表达式怎么是右边参数的方法，这是Scala语言的一个例外的情况:如果一个方法操作符标注，如a * b,那么方法被左操作数调用，就像a.* (b)--除非方法名以冒号结尾。这种情况下，方法被右操作数调用。
List有个方法叫":::"，用于实现叠加两个列表。

scala> val one = List('A', 'B')
val one = List('A', 'B')
scala> val two = List('C', 'D')

scala> one:::two
res1: List[Char] = List(A, B, C, D)
```
创建列表
```
scala> val days = List("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
days: List[String] = List(Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday)
```
创建空列表
```
scala> val l = Nil //scala.collection.immutable.Nil继承了List[Nothing]  这是空列表
l: scala.collection.immutable.Nil.type = List()

scala> val l = List()
l: List[Nothing] = List()
```
用字符串创建列表
```
scala> val l = "Hello" :: "Hi" :: "Hah" :: "WOW" :: "WOOW" :: Nil
l: List[String] = List(Hello, Hi, Hah, WOW, WOOW)
```
用“:::”叠加创建新列表
```
scala> val wow = l ::: List("WOOOW", "WOOOOW")
wow: List[String] = List(Hello, Hi, Hah, WOW, WOOW, WOOOW, WOOOOW)
```
通过索引获取列表值
```
scala> l(3)
res0: String = WOW
```
获取值长度为3的元素数目
```
scala> l.count(s => s.length == 3)
res1: Int = 2
```
返回去掉l头两个元素的新列表
```
scala> l.drop(2)
res2: List[String] = List(Hah, WOW, WOOW)

scala> l
res3: List[String] = List(Hello, Hi, Hah, WOW, WOOW)
```
返回去掉l后两个元素的新列表
```
scala> l.dropRight(2)
res5: List[String] = List(Hello, Hi, Hah)

scala> l
res6: List[String] = List(Hello, Hi, Hah, WOW, WOOW)
```
判断l是否存在某个元素
```
scala> l.exists(s => s == "Hah")
res7: Boolean = true
```
滤出长度为3的元素
```
scala> l.filter(s => s.length == 3)
res8: List[String] = List(Hah, WOW)
```
判断所有元素是否以“H”打头
```
scala> l.forall(s => s.startsWith("H"))
res10: Boolean = false
```
判断所有元素是否以“H”结尾
```
scala> l.forall(s => s.endsWith("W"))
res11: Boolean = false
```
打印每个元素
```
scala> l.foreach(s => print(s + ' '))
Hello Hi Hah WOW WOOW
```
取出第一个元素
```
scala> l.head
res17: String = Hello
```
取出最后一个元素
```
scala> l.last
res20: String = WOOW
```
剔除最后一个元素，生成新列表
```
scala> l.init
res18: List[String] = List(Hello, Hi, Hah, WOW)
```
剔除第一个元素，生成新列表
```
scala> l.tail
res49: List[String] = List(Hi, Hah, WOW, WOOW)
```
判断列表是否为空
```
scala> l.isEmpty
res19: Boolean = false
```
获得列表长度
```
scala> l.length
res21: Int = 5
```
修改每个元素，再反转每个元素形成新列表
```
scala> l.map(s => {val s1 = s + " - 01"; s1.reverse})
res29: List[String] = List(10 - olleH, 10 - iH, 10 - haH, 10 - WOW, 10 - WOOW)
```
生成用逗号隔开的字符串
```
scala> l.mkString(", ")
res30: String = Hello, Hi, Hah, WOW, WOOW
```
反序生成新列表
```
scala> l.reverse
res41: List[String] = List(WOOW, WOW, Hah, Hi, Hello)
```
按字母递增排序
```
scala> l.sortWith(_.compareTo(_) < 0)
res48: List[String] = List(Hah, Hello, Hi, WOOW, WOW)
```

**List定义的方法**

```
def  ++[B >: A, That](that: GenTraversableOnce[B])(implicit bf: CanBuildFrom[List[A], B, That]): That
Returns a new list containing the elements from the left hand operand followed by the elements from the right hand operand.
```
```
def  ++:[B >: A, That](that: collection.Traversable[B])(implicit bf: CanBuildFrom[List[A], B, That]): That
As with ++, returns a new collection containing the elements from the left operand followed by the elements from the right operand.
```
```
def  ++:[B](that: TraversableOnce[B]): List[B]
[use case] As with ++, returns a new collection containing the elements from the left operand followed by the elements from the right operand.
```
```
def  +:(elem: A): List[A]
[use case]
A copy of the list with an element prepended.

Note that :-ending operators are right associative (see example). A mnemonic for +: vs. :+ is: the COLon goes on the COLlection side.

Also, the original list is not modified, so you will want to capture the result.

Example:

scala> val x = List(1)
x: List[Int] = List(1)

scala> val y = 2 +: x
y: List[Int] = List(2, 1)

scala> println(x)
List(1)
elem
the prepended element
returns
a new list consisting of elem followed by all elements of this list.
```
```
def inition Classes
List → SeqLike → GenSeqLike
 Full Signature
```
```
def  /:[B](z: B)(op: (B, A) ⇒ B): B
Applies a binary operator to a start value and all elements of this traversable or iterator, going left to right.
```
```
def  :+(elem: A): List[A]
[use case] A copy of this list with an element appended.
```
```
def  ::(x: A): List[A]
[use case] Adds an element at the beginning of this list.
```
```
def  :::(prefix: List[A]): List[A]
[use case] Adds the elements of a given list in front of this list.
```
```
def  :\[B](z: B)(op: (A, B) ⇒ B): B
Applies a binary operator to all elements of this traversable or iterator and a start value, going right to left.
```
```
def  addString(b: StringBuilder): StringBuilder
Appends all elements of this traversable or iterator to a string builder.
```
```
def  addString(b: StringBuilder, sep: String): StringBuilder
Appends all elements of this traversable or iterator to a string builder using a separator string.
```
```
def  addString(b: StringBuilder, start: String, sep: String, end: String): StringBuilder
Appends all elements of this traversable or iterator to a string builder using start, end, and separator strings.
```
```
def  aggregate[B](z: ⇒ B)(seqop: (B, A) ⇒ B, combop: (B, B) ⇒ B): B
Aggregates the results of applying an operator to subsequent elements.
```
```
def  andThen[C](k: (A) ⇒ C): PartialFunction[Int, C]
Composes this partial function with a transformation function that gets applied to results of this partial function.
```
```
def  apply(n: Int): A
Selects an element by its index in the sequence.
```
```
def  applyOrElse[A1 <: Int, B1 >: A](x: A1, ```
```
def ault: (A1) ⇒ B1): B1
Applies this partial function to the given argument when it is contained in the function domain.
```

```
def  canEqual(that: Any): Boolean
Method called from equality methods, so that user-```
```
def ined subclasses can refuse to be equal to other collections of the same kind.
final ```
```
def  collect[B](pf: PartialFunction[A, B]): List[B]
[use case] Builds a new collection by applying a partial function to all elements of this list on which the function is ```
```
def ined.
```
```
def  collectFirst[B](pf: PartialFunction[A, B]): Option[B]
Finds the first element of the traversable or iterator for which the given partial function is ```
```
def ined, and applies the partial function to it.
```

```
def  combinations(n: Int): Iterator[List[A]]
Iterates over combinations.
```

```
def  companion: GenericCompanion[List]
The factory companion object that builds instances of class List.
```

```
def  compose[A](g: (A) ⇒ Int): (A) ⇒ A
Composes two instances of Function1 in a new Function1, with this function applied last.
```

```
def  contains[A1 >: A](elem: A1): Boolean
Tests whether this sequence contains a given value as an element.
```

```
def  containsSlice[B](that: GenSeq[B]): Boolean
Tests whether this sequence contains a given sequence as a slice.
```

```
def  copyToArray(xs: Array[A], start: Int, len: Int): Unit
[use case] Copies the elements of this list to an array.
```

```
def  copyToArray(xs: Array[A]): Unit
[use case] Copies the elements of this list to an array.
```

```
def  copyToArray(xs: Array[A], start: Int): Unit
[use case] Copies the elements of this list to an array.
```

```
def  copyToBuffer[B >: A](dest: Buffer[B]): Unit
Copies all elements of this traversable or iterator to a buffer.
final ```
```
def  corresponds[B](that: GenSeq[B])(p: (A, B) ⇒ Boolean): Boolean
Tests whether every element of this sequence relates to the corresponding element of another sequence by satisfying a test predicate.
```
```
def  count(p: (A) ⇒ Boolean): Int
Counts the number of elements in the traversable or iterator which satisfy a predicate.
```
```
def  diff(that: collection.Seq[A]): List[A]
[use case] Computes the multiset difference between this list and another sequence.
```
```
def  distinct: List[A]
Builds a new sequence from this sequence without any duplicate elements.
```
```
def  drop(n: Int): List[A]
Selects all elements except first n ones.
```
```
def  dropRight(n: Int): List[A]
Selects all elements except last n ones.
final ```
```
def  dropWhile(p: (A) ⇒ Boolean): List[A]
Drops longest prefix of elements that satisfy a predicate.
```

```
def  endsWith[B](that: GenSeq[B]): Boolean
Tests whether this sequence ends with the given sequence.
```

```
def  equals(that: Any): Boolean
The equals method for arbitrary sequences.
```

```
def  exists(p: (A) ⇒ Boolean): Boolean
Tests whether a predicate holds for at least one element of this sequence.
```

```
def  filter(p: (A) ⇒ Boolean): List[A]
Selects all elements of this traversable collection which satisfy a predicate.
```

```
def  filterNot(p: (A) ⇒ Boolean): List[A]
Selects all elements of this traversable collection which do not satisfy a predicate.
```

```
def  find(p: (A) ⇒ Boolean): Option[A]
Finds the first element of the sequence satisfying a predicate, if any.
final ```
```
def  flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): List[B]
[use case] Builds a new collection by applying a function to all elements of this list and using the elements of the resulting collections.
```
```
def  flatten[B]: List[B]
[use case] Converts this list of traversable collections into a list formed by the elements of these traversable collections.
```
```
def  fold[A1 >: A](z: A1)(op: (A1, A1) ⇒ A1): A1
Folds the elements of this traversable or iterator using the specified associative binary operator.
```
```
def  foldLeft[B](z: B)(op: (B, A) ⇒ B): B
Applies a binary operator to a start value and all elements of this sequence, going left to right.
```
```
def  foldRight[B](z: B)(op: (A, B) ⇒ B): B
Applies a binary operator to all elements of this list and a start value, going right to left.
```
```
def  forall(p: (A) ⇒ Boolean): Boolean
Tests whether a predicate holds for all elements of this sequence.
final ```
```
def  foreach(f: (A) ⇒ Unit): Unit
[use case] Applies a function f to all elements of this list.
```

```
def  genericBuilder[B]: Builder[B, List[B]]
The generic builder that builds instances of Traversable at arbitrary element types.
```

```
def  groupBy[K](f: (A) ⇒ K): Map[K, List[A]]
Partitions this traversable collection into a map of traversable collections according to some discriminator function.
```

```
def  grouped(size: Int): Iterator[List[A]]
Partitions elements in fixed size iterable collections.
```

```
def  has```
```
def initeSize: Boolean
Tests whether this traversable collection is known to have a finite size.
```
```
def  hashCode(): Int
Hashcodes for Seq produce a value from the hashcodes of all the elements of the sequence.
```
```
def  head: A
Selects the first element of this iterable collection.
```
```
def  headOption: Option[A]
Optionally selects the first element.
```
```
def  indexOf(elem: A, from: Int): Int
[use case] Finds index of first occurrence of some value in this list after or at some start index.
```
```
def  indexOf(elem: A): Int
[use case] Finds index of first occurrence of some value in this list.
```
```
def  indexOfSlice[B >: A](that: GenSeq[B], from: Int): Int
Finds first index after or at a start index where this sequence contains a given sequence as a slice.
```
```
def  indexOfSlice[B >: A](that: GenSeq[B]): Int
Finds first index where this sequence contains a given sequence as a slice.
```
```
def  indexWhere(p: (A) ⇒ Boolean, from: Int): Int
Finds index of the first element satisfying some predicate after or at some start index.
```
```
def  indexWhere(p: (A) ⇒ Boolean): Int
Finds index of first element satisfying some predicate.
```
```
def  indices: Range
Produces the range of all indices of this sequence.
```
```
def  init: List[A]
Selects all elements except the last.
```
```
def  inits: Iterator[List[A]]
Iterates over the inits of this traversable collection.
```
```
def  intersect(that: collection.Seq[A]): List[A]
[use case] Computes the multiset intersection between this list and another sequence.
```
```
def  is```
```
def inedAt(x: Int): Boolean
Tests whether this sequence contains given index.
```

```
def  isEmpty: Boolean
Tests whether this sequence is empty.
final ```
```
def  isTraversableAgain: Boolean
Tests whether this traversable collection can be repeatedly traversed.
```
```
def  iterator: Iterator[A]
Creates a new iterator over all elements contained in this iterable object.
```
```
def  last: A
Selects the last element.
```
```
def  lastIndexOf(elem: A, end: Int): Int
[use case] Finds index of last occurrence of some value in this list before or at a given end index.
```
```
def  lastIndexOf(elem: A): Int
[use case] Finds index of last occurrence of some value in this list.
```
```
def  lastIndexOfSlice[B >: A](that: GenSeq[B], end: Int): Int
Finds last index before or at a given end index where this sequence contains a given sequence as a slice.
```
```
def  lastIndexOfSlice[B >: A](that: GenSeq[B]): Int
Finds last index where this sequence contains a given sequence as a slice.
```
```
def  lastIndexWhere(p: (A) ⇒ Boolean, end: Int): Int
Finds index of last element satisfying some predicate before or at given end index.
```
```
def  lastIndexWhere(p: (A) ⇒ Boolean): Int
Finds index of last element satisfying some predicate.
```
```
def  lastOption: Option[A]
Optionally selects the last element.
```
```
def  length: Int
The length of the sequence.
```
```
def  lengthCompare(len: Int): Int
Compares the length of this sequence to a test value.
```
```
def  lift: (Int) ⇒ Option[A]
Turns this partial function into a plain function returning an Option result.
final ```
```
def  map[B](f: (A) ⇒ B): List[B]
[use case] Builds a new collection by applying a function to all elements of this list.
final ```
```
def  mapConserve(f: (A) ⇒ A): List[A]
[use case] Builds a new list by applying a function to all elements of this list.
```
```
def  max: A
[use case] Finds the largest element.
```
```
def  maxBy[B](f: (A) ⇒ B): A
[use case] Finds the first element which yields the largest value measured by function f.
```
```
def  min: A
[use case] Finds the smallest element.
```
```
def  minBy[B](f: (A) ⇒ B): A
[use case] Finds the first element which yields the smallest value measured by function f.
```
```
def  mkString: String
Displays all elements of this traversable or iterator in a string.
```
```
def  mkString(sep: String): String
Displays all elements of this traversable or iterator in a string using a separator string.
```
```
def  mkString(start: String, sep: String, end: String): String
Displays all elements of this traversable or iterator in a string using start, end, and separator strings.
```
```
def  nonEmpty: Boolean
Tests whether the traversable or iterator is not empty.
```
```
def  orElse[A1 <: Int, B1 >: A](that: PartialFunction[A1, B1]): PartialFunction[A1, B1]
Composes this partial function with a fallback partial function which gets applied where this partial function is not ```
```
def ined.
```

```
def  padTo(len: Int, elem: A): List[A]
[use case] A copy of this list with an element value appended until a given target length is reached.
```

```
def  par: ParSeq[A]
Returns a parallel implementation of this collection.
```

```
def  partition(p: (A) ⇒ Boolean): (List[A], List[A])
Partitions this traversable collection in two traversable collections according to a predicate.
```

```
def  patch(from: Int, that: GenSeq[A], replaced: Int): List[A]
[use case] Produces a new list where a slice of elements in this list is replaced by another sequence.
```

```
def  permutations: Iterator[List[A]]
Iterates over distinct permutations.
```

```
def  prefixLength(p: (A) ⇒ Boolean): Int
Returns the length of the longest prefix whose elements all satisfy some predicate.
```

```
def  product: A
[use case] Multiplies up the elements of this collection.
```

```
def  productIterator: scala.Iterator[Any]
An iterator over all the elements of this product.
```

```
def  productPrefix: String
A string used in the toString methods of derived classes.
```

```
def  reduce[A1 >: A](op: (A1, A1) ⇒ A1): A1
Reduces the elements of this traversable or iterator using the specified associative binary operator.
```

```
def  reduceLeft[B >: A](op: (B, A) ⇒ B): B
Applies a binary operator to all elements of this sequence, going left to right.
```

```
def  reduceLeftOption[B >: A](op: (B, A) ⇒ B): Option[B]
Optionally applies a binary operator to all elements of this traversable or iterator, going left to right.
```

```
def  reduceOption[A1 >: A](op: (A1, A1) ⇒ A1): Option[A1]
Reduces the elements of this traversable or iterator, if any, using the specified associative binary operator.
```

```
def  reduceRight[B >: A](op: (A, B) ⇒ B): B
Applies a binary operator to all elements of this sequence, going right to left.
```

```
def  reduceRightOption[B >: A](op: (A, B) ⇒ B): Option[B]
Optionally applies a binary operator to all elements of this traversable or iterator, going right to left.
```

```
def  repr: List[A]
The collection of type traversable collection underlying this TraversableLike object.
```

```
def  reverse: List[A]
Returns new list with elements in reversed order.
```

```
def  reverseIterator: Iterator[A]
An iterator yielding elements in reversed order.
```

```
def  reverseMap[B](f: (A) ⇒ B): List[B]
[use case] Builds a new collection by applying a function to all elements of this list and collecting the results in reversed order.
```

```
def  reverse_:::(prefix: List[A]): List[A]
[use case] Adds the elements of a given list in reverse order in front of this list.
```

```
def  runWith[U](action: (A) ⇒ U): (Int) ⇒ Boolean
Composes this partial function with an action function which gets applied to results of this partial function.
```

```
def  sameElements(that: GenIterable[A]): Boolean
[use case] Checks if the other iterable collection contains the same elements in the same order as this list.
```

```
def  scan[B >: A, That](z: B)(op: (B, B) ⇒ B)(implicit cbf: CanBuildFrom[List[A], B, That]): That
Computes a prefix scan of the elements of the collection.
```

```
def  scanLeft[B, That](z: B)(op: (B, A) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That
Produces a collection containing cumulative results of applying the operator going left to right.
```

```
def  scanRight[B, That](z: B)(op: (A, B) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That
Produces a collection containing cumulative results of applying the operator going right to left.
```

```
def  segmentLength(p: (A) ⇒ Boolean, from: Int): Int
Computes length of longest segment whose elements all satisfy some predicate.
```

```
def  seq: LinearSeq[A]
A version of this collection with all of the operations implemented sequentially (i.e., in a single-threaded manner).
```

```
def  size: Int
The size of this sequence, equivalent to length.
```

```
def  slice(from: Int, until: Int): List[A]
```

```
def  sliding(size: Int, step: Int): Iterator[List[A]]
Groups elements in fixed size blocks by passing a "sliding window" over them (as opposed to partitioning them, as is done in grouped.)
```

```
def  sliding(size: Int): Iterator[List[A]]
Groups elements in fixed size blocks by passing a "sliding window" over them (as opposed to partitioning them, as is done in grouped.) The "sliding window" step is set to one.
```

```
def  sortBy[B](f: (A) ⇒ B)(implicit ord: math.Ordering[B]): List[A]
Sorts this Seq according to the Ordering which results from transforming an implicitly given Ordering with a transformation function.
```

```
def  sortWith(lt: (A, A) ⇒ Boolean): List[A]
Sorts this sequence according to a comparison function.
```

```
def  sorted[B >: A](implicit ord: math.Ordering[B]): List[A]
Sorts this sequence according to an Ordering.
final ```
```
def  span(p: (A) ⇒ Boolean): (List[A], List[A])
Splits this list into a prefix/suffix pair according to a predicate.
```
```
def  splitAt(n: Int): (List[A], List[A])
Splits this list into two at a given position.
```
```
def  startsWith[B](that: GenSeq[B], offset: Int): Boolean
Tests whether this sequence contains the given sequence at a given index.
```
```
def  startsWith[B](that: GenSeq[B]): Boolean
Tests whether this general sequence starts with the given sequence.
```
```
def  stringPrefix: String
```
```
def ines the prefix of this object's toString representation.
```
```
def  sum: A
[use case] Sums up the elements of this collection.
```
```
def  tail: List[A]
Selects all elements except the first.
```
```
def  tails: Iterator[List[A]]
Iterates over the tails of this traversable collection.
```
```
def  take(n: Int): List[A]
Selects first n elements.
```
```
def  takeRight(n: Int): List[A]
Selects last n elements.
final ```
```
def  takeWhile(p: (A) ⇒ Boolean): List[A]
Takes longest prefix of elements that satisfy a predicate.
```

```
def  to[Col[_]]: Col[A]
[use case] Converts this list into another by copying all elements.
```

```
def  toArray: Array[A]
[use case] Converts this list to an array.
```

```
def  toBuffer[B >: A]: Buffer[B]
Uses the contents of this traversable or iterator to create a new mutable buffer.
```

```
def  toIndexedSeq: IndexedSeq[A]
Converts this traversable or iterator to an indexed sequence.
```

```
def  toIterable: collection.Iterable[A]
Returns this iterable collection as an iterable collection.
```

```
def  toIterator: Iterator[A]
Returns an Iterator over the elements in this iterable collection.
```

```
def  toList: List[A]
Converts this list to a list.
```

```
def  toMap[T, U]: collection.Map[T, U]
[use case] Converts this list to a map.
```

```
def  toParArray: ParArray[T]
```

```
def  toSeq: Seq[A]
Converts this immutable sequence to a sequence.
```

```
def  toSet[B >: A]: Set[B]
Converts this traversable or iterator to a set.
```

```
def  toStream: Stream[A]
Converts this list to a stream.
```

```
def  toString(): String
Converts this sequence to a string.
```

```
def  toTraversable: collection.Traversable[A]
Converts this traversable collection to an unspecified Traversable.
```

```
def  toVector: scala.Vector[A]
Converts this traversable or iterator to a Vector.
```

```
def  transpose[B](implicit asTraversable: (A) ⇒ GenTraversableOnce[B]): List[List[B]]
Transposes this collection of traversable collections into a collection of collections.
```

```
def  union(that: collection.Seq[A]): List[A]
[use case] Produces a new sequence which contains all elements of this list and also all elements of a given sequence.
```

```
def  unzip[A1, A2](implicit asPair: (A) ⇒ (A1, A2)): (List[A1], List[A2])
Converts this collection of pairs into two collections of the first and second half of each pair.
```

```
def  unzip3[A1, A2, A3](implicit asTriple: (A) ⇒ (A1, A2, A3)): (List[A1], List[A2], List[A3])
Converts this collection of triples into three collections of the first, second, and third element of each triple.
```

```
def  updated(index: Int, elem: A): List[A]
[use case] A copy of this list with one single replaced element.
```

```
def  view(from: Int, until: Int): SeqView[A, List[A]]
Creates a non-strict view of a slice of this sequence.
```

```
def  view: SeqView[A, List[A]]
Creates a non-strict view of this sequence.
```

```
def  withFilter(p: (A) ⇒ Boolean): FilterMonadic[A, List[A]]
Creates a non-strict filter of this traversable collection.
```

```
def  zip[B](that: GenIterable[B]): List[(A, B)]
[use case] Returns a list formed from this list and another iterable collection by combining corresponding elements in pairs.
```

```
def  zipAll[B](that: collection.Iterable[B], thisElem: A, thatElem: B): List[(A, B)]
[use case] Returns a list formed from this list and another iterable collection by combining corresponding elements in pairs.
```

```
def  zipWithIndex: List[(A, Int)]
[use case] Zips this list with its indices.
```


```

## 七.Map的常见方法汇总

（1）不可变Map

```
var a:Map[String,Int]=Map("k1"->1,"k2"->2)//初始化构造函数
a += ("k3"->3)//添加元素
a += ("k4"->4)//添加元素
a += ("k1"->100)//已经存在添加元素会覆盖
a -= ("k2","k1")//删除元素    //a("k1") = "foo"//不支持
println(a.contains("k6"))//是否包含某元素
println(a.size)//打印大小
println(a.get("k1").getOrElse("default")) //根据key读取元素，不存在就替换成默认值
a.foreach{case (e,i) => println(e,i)} //遍历打印1
for( (k,v)<-a ) println(k,v) //遍历打印2
println(a.isEmpty)//判断是否为空
a.keys.foreach(println)//只打印key
a.values.foreach(println)//只打印value

a=Map()//数据清空使用再次new
println(a.size)
a.toSeq.sortBy(_._1)//升序排序 key
a.toSeq.sortBy(_._2)//升序排序 value
a.toSeq.sortWith(_._1>_._1) //降序排序 key
a.toSeq.sortWith(_._2>_._2) //降序排序 value
    
//下面自定义按英文字母或数字排序
implicit  val KeyOrdering=new Ordering[String] {
      override def compare(x: String, y: String): Int = {
        x.compareTo(y)
      }
}
println(a.toSeq.sorted)
```

2）可变Map例子
```
var a:scala.collection.mutable.Map[String,Int]=scala.collection.mutable.Map("k1"->1,"k2"->2)//初始化构造函数
  a += ("k3"->3)//添加元素
  a += ("k4"->4)//添加元素
  a += ("k1"->100)//已经存在添加元素会覆盖
  a += ("k1"->100,"k9"->9)//添加多个元素
  a -= ("k2","k1")//删除元素
  a ++= List("CA" -> 23, "CO" -> 25)//追加集合
  a --= List("AL", "AZ")//删除集合

  a.retain((k,v)=> k=="k1")//只保留等于k1元素，其他的删除
  a.put("put1",200)//put
  a.remove("k2")//remove
  a.clear()//清空
  a("k3")=100//支持

  println(a.contains("k6"))//是否包含某元素
  println(a.size)//打印大小
  println(a.get("k1").getOrElse("default")) //根据key读取元素，不存在就替换成默认值
  a.foreach{case (e,i) => println(e,i)} //遍历打印1
  for( (k,v)<-a ) println(k,v) //遍历打印2
  println(a.isEmpty)//判断是否为空
  a.keys.foreach(println)//只打印key
  a.values.foreach(println)//只打印value
  a=scala.collection.mutable.Map()//引用能变
  println(a.size)
  a.toSeq.sortBy(_._1)//排序 key
  a.toSeq.sortBy(_._2)//排序 value
  a.toSeq.sortWith(_._1>_._1) //降序排序 key
  a.toSeq.sortWith(_._2>_._2) //降序排序 value
  
//下面自定义按英文字母或数字排序
  implicit  val KeyOrdering=new Ordering[String] {
    override def compare(x: String, y: String): Int = {
      x.compareTo(y)
    }
  }
  println(a.toSeq.sorted)
}
```

默认情况下，Scala使用不可变映射(Map)。如果要使用可变集合(Set)，则必须明确导入scala.collection.mutable.Map类。如果想同时使用可变的和不可变映射(Map)，那么可以继续引用不可变映射(Map)，但是可以将mutable集合引用mutable.Map。
以下是声明不可变映射(Map)的示例声明 
集合基本操作
```
scala> val colors = Map("red" -> "#FF0000", "azure" -> "#F0FFFF", "peru" -> "#CD853F")
colors: scala.collection.immutable.Map[String,String] = Map(red -> #FF0000, azure -> #F0FFFF, peru -> #CD853F)

scala> val nums: Map[Int, Int] = Map()
nums: Map[Int,Int] = Map()

scala>println( "Keys in colors : " + colors.keys )
Keys in colors : Set(red, azure, peru)

scala>println( "Values in colors : " + colors.values )
Values in colors : MapLike(#FF0000, #F0FFFF, #CD853F)

scala>println( "Check if colors is empty : " + colors.isEmpty )
Check if colors is empty : false

scala>println( "Check if nums is empty : " + nums.isEmpty )
Check if nums is empty : true
```

连接映射
```
scala>val colors1 = Map("red" -> "#FF0000", "azure" -> "#F0FFFF", "peru" -> "#CD853F")
colors1: scala.collection.immutable.Map[String,String] = Map(red -> #FF0000, azure -> #F0FFFF, peru -> #CD853F)

scala>val colors2 = Map("blue" -> "#0033FF", "yellow" -> "#FFFF00", "red" -> "#FF0000")
colors2: scala.collection.immutable.Map[String,String] = Map(blue -> #0033FF, yellow -> #FFFF00, red -> #FF0000)

scala>var colors = colors1 ++ colors2
colors: scala.collection.immutable.Map[String,String] = Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0000)

scala>println( "colors1 ++ colors2 : " + colors )
colors1 ++ colors2 : Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0000)

scala>colors = colors1.++(colors2)
colors: scala.collection.immutable.Map[String,String] = Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0000)

scala>println( "colors1.++(colors2)) : " + colors )
colors1.++(colors2)) : Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0000)

```

打印映射的键和值
```
val colors = Map("red" -> "#FF0000", "azure" -> "#F0FFFF","peru" -> "#CD853F")
colors.keys.foreach{ i =>  
    print( "Key = " + i )
    println(" Value = " + colors(i) )}
}

Key = red Value = #FF0000
Key = azure Value = #F0FFFF
Key = peru Value = #CD853F
```
查找检查映射中的键
```
val colors = Map("red" -> "#FF0000", "azure" -> "#F0FFFF", "peru" -> "#CD853F")
if( colors.contains( "red" )) {
    println("Red key exists with value :"  + colors("red"))
} else {
    println("Red key does not exist")
}
if( colors.contains( "maroon" )) {
    println("Maroon key exists with value :"  + colors("maroon"))
} else {
    println("Maroon key does not exist")
}
```


scala - Map基础
构造Map:不可变：
```
val map = Map("sa" -> 1, "s" -> 2)
map("sa") = 3 // error
val emptyMap = new scala.collection.immutable.HashMap[String, Int]
```
可变：
```
val map2 = scala.collection.mutable.Map("sa" -> 2)
map2("sa") = 3
val emptyMap = new scala.collection.mutable.HashMap[String, Int]
```
注：->用来创建元组， "sa" -> 1即("sa", 1) 初始化完全可以 val map = Map(("sa", 1), ("s", 2))
获取Map中的值：
```
如果map中不包含请求中使用的key值，则抛异常。NoSuchElementException
map("sa") // 类似于java中的map.get("sa")
```

要检查map中是否包含某个key，使用contains方法。
```
val sa = if (map2.contains("sa3")) map2("sa3") else 0;
快捷的方式：
val sa2 = map.getOrElse("sa2", 0)
一次得到是否包含key，并获取值：
val sa3 = map.get("sa3"); // Option类型，
println(sa3.isEmpty)
```
更新Map中的值：
```
添加或更新：map("sa") = 3
添加或更新多个：map += ("aa" -> 4, "bb" -> 5)
```
移除某个key和对应的值：
```
map -= "aa"
不可变的map也可以使用+和-操作，但是会生成新的map
var map = Map("aa" -> 1)
map = map + ("bb" -> 2)
map += ("cc" -> 2)
map -= "aa"
```
迭代map：
```
for ((k, v) <- map) {

}
所有key：map.keySet
所有值：map.values
反转：map2 = for((k, v) <- map) yield (v, k)
```
已排序Map：
按key排序：SortedMap
按添加顺序：LinkedHashMap
Map与Java互操作：
Java Properties转为scala.collection.Map：
```
import scala.collection.JavaConversions.propertiesAsScalaMap
val prop: scala.collection.Map[String, String] = System.getProperties();
```
Java Map转为scala.collection.mutable.Map[String, Int]：
```
import scala.collection.JavaConversions.mapAsScalaMap
val map: scala.collection.mutable.Map[String, Int] = new TreeMap[String, Int]
```
Scala Map转为Java Map:
```
import scala.collection.JavaConversions.mapAsJavaMap
import java.awt.font.TextAttribute._
var fs = Map(FAMILY -> "Serif", SIZE -> 12)
var fonts = new Font(fs)
```

**Map的操作方法**

```
def ++(xs: Map[(A, B)]): Map[A, B]
返回一个新的 Map，新的 Map xs 组成
```
```	
def -(elem1: A, elem2: A, elems: A*): Map[A, B]
返回一个新的 Map, 移除 key 为 elem1, elem2 或其他 elems。
```
```	
def --(xs: GTO[A]): Map[A, B]
返回一个新的 Map, 移除 xs 对象中对应的 key
```
```	
def get(key: A): Option[B]
返回指定 key 的值
```
```	
def iterator: Iterator[(A, B)]
创建新的迭代器，并输出 key/value 对
```
```	
def addString(b: StringBuilder): StringBuilder
将 Map 中的所有元素附加到StringBuilder，可加入分隔符
```
```	
def addString(b: StringBuilder, sep: String): StringBuilder
将 Map 中的所有元素附加到StringBuilder，可加入分隔符
```
```	
def apply(key: A): B
返回指定键的值，如果不存在返回 Map 的默认方法
```
```	
def clear(): Unit
清空 Map
```
```	
def clone(): Map[A, B]
从一个 Map 复制到另一个 Map
```
```	
def contains(key: A): Boolean
如果 Map 中存在指定 key，返回 true，否则返回 false。
```
```	
def copyToArray(xs: Array[(A, B)]): Unit
复制集合到数组
```
```	
def count(p: ((A, B)) => Boolean): Int
计算满足指定条件的集合元素数量
```
```	
def default(key: A): B
定义 Map 的默认值，在 key 不存在时返回。
```
```	
def drop(n: Int): Map[A, B]
返回丢弃前n个元素新集合
```
```	
def dropRight(n: Int): Map[A, B]
返回丢弃最后n个元素新集合
```
```	
def dropWhile(p: ((A, B)) => Boolean): Map[A, B]
从左向右丢弃元素，直到条件p不成立
```
```	
def empty: Map[A, B]
返回相同类型的空 Map
```
```	
def equals(that: Any): Boolean
如果两个 Map 相等(key/value 均相等)，返回true，否则返回false
```
```	
def exists(p: ((A, B)) => Boolean): Boolean
判断集合中指定条件的元素是否存在
```
```
def filter(p: ((A, B))=> Boolean): Map[A, B]
返回满足指定条件的所有集合
```
```	
def filterKeys(p: (A) => Boolean): Map[A, B]
返回符合指定条件的的不可变 Map
```
```	
def find(p: ((A, B)) => Boolean): Option[(A, B)]
查找集合中满足指定条件的第一个元素
```
```	
def foreach(f: ((A, B)) => Unit): Unit
将函数应用到集合的所有元素
```
```	
def init: Map[A, B]
返回所有元素，除了最后一个
```
```	
def isEmpty: Boolean
检测 Map 是否为空
```
```	
def keys: Iterable[A]
返回所有的key/p>
```
```	
def last: (A, B)
返回最后一个元素
```
```	
def max: (A, B)
查找最大元素
```
```	
def min: (A, B)
查找最小元素
```
```	
def mkString: String
集合所有元素作为字符串显示
```
```	
def product: (A, B)
返回集合中数字元素的积。
```
```	
def remove(key: A): Option[B]
移除指定 key
```
```	
def retain(p: (A, B) => Boolean): Map.this.type
如果符合满足条件的返回 true
```
```	
def size: Int
返回 Map 元素的个数
```
```	
def sum: (A, B)
返回集合中所有数字元素之和
```
```	
def tail: Map[A, B]
返回一个集合中除了第一元素之外的其他元素
```
```
def take(n: Int): Map[A, B]
返回前 n 个元素
```
```	
def takeRight(n: Int): Map[A, B]
返回后 n 个元素
```
```	
def takeWhile(p: ((A, B)) => Boolean): Map[A, B]
返回满足指定条件的元素
```
```
def toArray: Array[(A, B)]
集合转数组
```
```
def toBuffer[B >: A]: Buffer[B]
返回缓冲区，包含了 Map 的所有元素
```
```
def toList: List[A]
返回 List，包含了 Map 的所有元素
```
```	
def toSeq: Seq[A]
返回 Seq，包含了 Map 的所有元素
```
```	
def toSet: Set[A]
返回 Set，包含了 Map 的所有元素
```
```
def toString(): String
返回字符串对象
```

## 八.类的定义及构造器

**类和对象之基础**

**定义**

Scala 中以 class 来作为类的声明，在类中可以定义成员和方法，成员和方法可以有不同的可见性（这个会在后文详述）
```
scala> class Company {
     |   private var employeeCount = 0
     |   def getEmployeeCount(): Int = employeeCount
     |   def setEmployeeCount( count: Int)= {
     |     employeeCount = count
     |   }
     |
     |   def m( i: Int ) {}
     |   def m( str: String ) {}
     | }
defined class Company
```
**构造器**

Scala 中，类有一个主构造器，主构造器必须包含所需的所有参数。除了一个主构造器，还可以有0个或多个辅助构造器，辅助构造器又称次构造器。辅助构造器命名为 this，其第一条语句必须调用主构造器或其他辅助构造器，来看下面的例子：
```
scala> class T ( x1: Int, y1: String, z1: Double ) {
     |   private val xx1 = x1
     |   private val yy1 = y1
     |   private val zz1 = z1
     |
     |   def this ( x1: Int, y1: String ) {
     |     this( x1, y1, 1.0 )
     |   }
     |
     |   def this ( x1: Int ) {
     |     this( x1, "" )
     |   }
     | }
defined class T
```
还有一点需要注意的是，被调用的辅助构造函数的定义必须放在主动调用的辅助构造函数前面，不然会报错：
```
scala> class T ( x1: Int, y1: String, z1: Double ) {
     |   private val xx1 = x1
     |   private val yy1 = y1
     |   private val zz1 = z1
     |
     |   def this ( x1: Int ) {
     |     this( x1, "" )
     |   }
     |
     |   def this ( x1: Int, y1: String ) {
     |     this( x1, y1, 1.0 )
     |   }
     | }
<console>:13: error: called constructor's definition must precede calling constructor's definition
           this( x1, "" )
           ^
```
不管辅助函数调来调去，最终都还是要调用到主构造函数，这确保了新实例的初始化逻辑一致。

如果在主构造函数的参数前加 var 或 val，该参数就成为实例的一个成员，这部分知识在Scala case class那些你不知道的知识有更详细的介绍

**重载**

Scala 类方法允许重载，如类 Company 中的 m 方法。重载要求参数列表和返回类型不完全相同，但参数名可相同，这是因为编译后是通过方法名、参数列表、返回类型综合来区分各个方法的。

在方法重载时，有一点需要注意：对于『高级类型』，存在类型擦除机制，所谓的高级类型就是包含类型参数的类型，比如 List[A]，下面这个例子可以展示了类型擦除：

```
scala> class Tmp {
     |   def m( data: List[Int] ) {}
     |   def m( data: List[String] ) {}
     | }
<console>:9: error: double definition:
method m:(data: List[String])Unit and
method m:(data: List[Int])Unit at line 8
have same type after erasure: (data: List)Unit
         def m( data: List[String] ) {}
             ^
```
报了有相同类型的参数的错误。

**类型成员**

Scala 允许你在类内部定义类型成员，在构造类实例的时候指定该类型成员对应的具体类型。类型成员可用于类内部的成员或函数，提供了更好的泛华能力，从下面这个简单的例子可以看出：
```
scala> class T {
     |   type X
     |
     |   def getClassName( x: X): String = {
     |     x.getClass.getTypeName
     |   }
     | }
defined class T

scala> val x1 = new T{ type X = Int }
x1: T{type X = Int} = $anon$1@515f550a

scala> x1.getClassName(10)
res0: String = java.lang.Integer

scala> val x2 = new T{ type X = String }
x2: T{type X = String} = $anon$1@61a52fbd

scala> x2.getClassName("string")
res1: String = java.lang.String
```
当然，也可以在类外部定义类型变量，如：
```
scala> type L = List[Int]
defined type alias L
```
**方法与成员同名**

与 JAVA 不同，如果方法参数列表不为空，该方法可以与成员同名，如：
```
scala> class T {
     |   private val m = 0
     |
     |   def m( i: Int ): Int = m + i
     | }
defined class T

```

##  九.类和对象之进阶（一）

 1、Scala中的类是公有可见性的，且多个类可以包含在同一个源文件中；

```
class Counter{
    private var value = 0　　//类成员变量必须初始化，否则报错
    def increment(){    //类中的方法默认是公有可见性
        value += 1
    }
    def current() = value //对于类中的“取值方法”，在定义时可省略掉括号，直接 def current = value
}
```

继承

只能有一个父类

与其他支持面向对象的语言一样，Scala 也支持继承，并且子类只能有一个父类，不能继承于多个父类，如果希望实现类似继承多个父类的功能，应该考虑引入 trait。虽然只支持一个父类，但是父类还可以有父类，也就是爷爷类，对于类继承的层数是没有具体要求的，这几点在下面这个例子中都有体现：
```
scala> class A {
     | }
defined class A

scala> class B {
     | }
defined class B

scala> class AA extends A {
     | }
defined class AA

scala> class AB extends A with B {
     | }
<console>:9: error: class B needs to be a trait to be mixed in
       class AB extends A with B {
                               ^

scala> class AAA extends AA {
     | }
defined class AAA

scala> class AAAA extends AAA {
     | }
defined class AAAA
```
都继承了什么

子类继承父类时都会继承些什么呢，这里结合可见性（可见性的详细内容会在下文介绍）进行分析，先定义这样一组父子类：
```
scala> class Parent ( x: Int, y: String, z: Double ) {
     |   val xx = x
     |   protected val yy = y
     |   private val zz = z
     |
     |   def getXX = xx
     |   protected def getYY = yy
     |   private def getZZ = zz
     |
     |   def testYY = yy
     |   def testZZ = zz
     |   def testGetYY = getYY
     |   def testGetZZ = getZZ
     | }
defined class Parent
```
在 Scala 类继承中，允许在子类内部直接访问父类的 public 及 protected 成员及方法，但不允许子类直接访问父类的 private 成员及方法，如下例：
```
scala> class Child1 ( x: Int, y: String, z: Double ) extends Parent(x, y, z ) {
     |   println( xx )
     |   println( yy )
     |   println( getXX )
     |   println( getYY )
     | }
defined class Child1

scala> class Child2 ( x: Int, y: String, z: Double ) extends Parent(x, y, z ) {
     |   println( zz )
     |   println( getZZ )
     | }
<console>:9: error: value zz in class Parent cannot be accessed in Child2
         println( zz )
                  ^
<console>:10: error: method getZZ in class Parent cannot be accessed in Child2
         println( getZZ )
                  ^
```
在类外部，只有 public 的方法和成员能被直接访问，protected 及 private 均不予许：
```
scala> class Child3 ( x: Int, y: String, z: Double ) extends Parent(x, y, z ) {
     | }
defined class Child3

scala> val child = new Child3( 1, "hello", 3.1415926 )
child: Child3 = Child3@39529185

scala> child.xx
res6: Int = 1

scala> child.yy
<console>:11: error: value yy in class Parent cannot be accessed in Child3
 Access to protected value yy not permitted because
 enclosing object $iw is not a subclass of
 class Parent where target is defined
              child.yy
                    ^

scala> child.zz
<console>:11: error: value zz in class Parent cannot be accessed in Child3
              child.zz
                    ^

scala>

scala> child.getXX
res9: Int = 1

scala> child.getYY
<console>:11: error: method getYY in class Parent cannot be accessed in Child3
 Access to protected method getYY not permitted because
 enclosing object $iw is not a subclass of
 class Parent where target is defined
              child.getYY
                    ^

scala> child.getZZ
<console>:11: error: method getZZ in class Parent cannot be accessed in Child3
              child.getZZ
                    ^
```
但我们可以通过父类提供的方法来间接访问 protected 和 private 的成员和方法：
```
scala> child.testYY
res20: String = hello

scala> child.testZZ
res21: Double = 3.1415926

scala> child.testGetYY
res22: String = hello

scala> child.testGetZZ
res23: Double = 3.1415926
```
单例对象

在 Scala 中，使用关键字 object 来定义单例对象：
```
scala> object T {}
defined module T
```
单例对象将在其首次被调用时初始化，且没有参数。单例对象一旦定义完毕，它的名字就代表了该单例对象的唯一实例。

当单例对象与某个类的名字相同且两者定义在同一文件中，就形成了特殊的单例对象-伴生对象，对应的类称为伴生类，若单例没有相同名字的类的话成为孤立对象（好惨）。我们经常使用在伴生对象中对应 apply 方法来创建新的伴生类实例并且将半身列的可见性设置为 private，以便能方便的创建伴生类实例，更重要的是可以在伴生类对象中管理所有伴生类实例，例子如下：
```
class Q ( qParam: String ) {
  private val q = qParam
}

object Q {
  private val qList = ListBuffer[ Q ]()

  def apply( qParam: String ) {
    val qInstance = new Q( qParam )
    qList.append( qInstance )
    qInstance
  }

  def qListSize = qList.size
}

object Test {
  def main (args: Array[String]) {
    val qIns1 = Q( "q1" )
    val qIns2 = Q( "q2" )
    println( Q.qListSize )
  }
}
```
输出：
```
2
```
另外伴生对象与伴生类可以互相访问 private 成员和方法，object 也可以继承父类或混入特质
## 十.类和对象之进阶（二）

Scala 中的可见性非常灵活且复杂，这篇文章希望通过大量的示例来说清楚各种情况下的可见性是怎么样的。
默认可见性
Scala 中的默认可见性为 public，所谓默认即你没有在类或者成员前显示加 private 或 protected 可见性关键字。虽然默认可见性为 public，但这是逻辑上的，实际上 Scala 中并没有 public 这个关键字，如果你用 public 来声明一个类或成员，编译器会报错。
可见性作用域
在 Scala 中，可以在类型的 class 或 trait 关键字之前、字段的 val 或 var 之前，方法定义的 def 关键字之前指定可见性。
公有可见性
对于公有可见性，任何作用域内都可以访问公有成员或公有类型。
Protected 可见性
对于受保护可见性，用 protected 声明，受保护成员对本类型、继承类型可见。而受保护的类型则只对包含该类的包内可见。
下面例子是关于 protected 成员的：
```
package P1 {
  class C1 {
    protected val c = 0

    //< 受保护可见性中,嵌套类可访问 protected 成员
    class C11 {
      println( c )
    }
  }

  package P11 {
    //< 继承类客房为父类 protected 成员
    class C1Child extends C1 {
      println( c )
    }
  }

}

package P2 {
  //< 继承类客房为父类 protected 成员
  class C2Child extends P1.C1 {
    println( c )
  }
}
```
接下来是 protected 类型的：
```
package P1 {
  protected class C1 {
  }

  //< 对于 protected 类型,相同包内可见
  class C1Child extends C1 {
  }

  package P11 {
    //< 对于 protected 类型,子包内可见
    class C11Child extends C1 {
    }
  }
}

package P2 {
//< 对于 protected 类型,外部包不可见
  class C2Child extends P1.C1 {
  }
}
```
编译报错如下，这是因为 protected 类型只在包含该类的包内可见
```
Error:(22, 28) class C1 in package P1 cannot be accessed in package P1
 Access to protected class C1 not permitted because
 enclosing package P2 is not a subclass of 
 package P1 where target is defined
  class C2Child extends P1.C1 {
```
私有可见性

私有可见性将实现细节完全隐藏起来，即便是继承类也无法访问这些细节。声明中包含了 private 关键字的所有成员只对该类可见，该类型的其他实例也能访问这些成员。如果类型被声明为私有可见性类型，那么该类型的可见性将仅限于包含该类型的包内
```
package P1 {
  class C1 {
    private val c = 0
  }

  //< 对于 private 类型,相同包内的子类都不可见
  class C1Child extends C1 {
    println( c )
  }
}

package P2 {
  //< 对于 private 类型,外部包内的子类也不可见
  class C2Child extends P1.C1 {
    println( c )
  }
}
```
编译报错：
```
Error:(12, 14) value c in class C1 cannot be accessed in P1.C1Child
    println( c )
             ^

Error:(19, 14) value c in class C1 cannot be accessed in P2.C2Child
    println( c )
             ^
```
另外，嵌套类中的私有成员也是无法访问的。在私有可见性中，私有类型只在包含该类型的包中可见，在子包或外部包中均不可见。我们用下面的例子进一步说明，具体说明见代码中的注释：
```
package P1 {
  private class C1

  class C11 extends C1            //< 错误,这样相当于变相改变了C1的可见性,子包和外部包都能访问C11,也就间接能访问C1
  protected class C12 extends C1  //< 错误这样相当于变相改变了C1的可见性,子包能访问C11,也就间接能访问C1
  private class C13 extends C1    //< 正确,由于C13也为 private,是的C1的 private 可见性不会

  class C14 {
    val c14_1 = new C1            //< 正确,私有类型在其所在包内可见
  }
}

package P2 {

  //< 对于私有类型,外部包内不可见
  class C2 {
    val c1 = new P1.C1
  }
}
```
编译报错：
```
Error:(8, 21) private class C1 escapes its defining scope as part of type P1.C1
  class C11 extends C1            //< 错误
                    ^

Error:(9, 31) private class C1 escapes its defining scope as part of type P1.C1
  protected class C12 extends C1  //< 错误
                              ^

Error:(22, 21) class C1 in package P1 cannot be accessed in package P1
    val c1 = new P1.C1
                    ^
```
作用域内私有和作用域内受保护可见性

所谓作用域内私有/受保护可见性，就是你可以更细粒度指定某个类或某个成员在某个作用域（可以是包或类）私有或受保护可见性

成员在类和包中的 private/protected 可见性
该可见性可以有16种组合，下面的例子列举除了这些组合
```
package P1 {
  class C1 {
    private[C1] val m1 = 1
    private[this] val m2 = 2
    private[P1] val m3 = 3
    private[P2] val m4 = 4

    protected[C1] val n1 = 1
    protected[this] val n2 = 2
    protected[P1] val n3 = 3
    protected[P2] val n4 = 4

    //< 不管什么样的作用域内 private 或 protected,在自身类中都是可见的
    println( m1 )
    println( m2 )
    println( m3 )
    println( m4 )

    println( n1 )
    println( n2 )
    println( n3 )
    println( n4 )
  }

  class C11 extends C1 {
    println( m1 )   //< 1, 错误
    println( m2 )   //< 2, 错误
    println( m3 )   //< 3, 正确
    println( m4 )   //< 4, 正确

    println( n1 )   //< 5, 正确
    println( n2 )   //< 6, 正确
    println( n3 )   //< 7, 正确
    println( n4 )   //< 8, 正确
  }
}

package P2 {
  class C21 extends P1.C1 {
    println( m1 )   //< 9, 错误
    println( m2 )   //< 10, 错误
    println( m3 )   //< 11, 错误
    println( m4 )   //< 12, 正确

    println( n1 )   //< 13, 正确
    println( n2 )   //< 14, 正确
    println( n3 )   //< 15, 正确
    println( n4 )   //< 16, 正确
  }
}
```

下面我们对每一项进行解释，并穿插介绍一些规则：

private[C1]指定成员在自身类作用域 private，在该类所在的包内和包外均不可见（9也是这个道理）
private[this]比 private[C1]更加严格，前者只对相同实例可见，相同类的不同实例都不可见；而后者对相同类的不同实例也可见
private[P1]指定在包 P1 内 private，则在 P1 包中的类中均可见，而在 P1外的包均不可见
private[P2]指定在包 P2 内 private，则在包 P2 及该类所在包内均可见
protected[C1]指定在 C1 中 protected，则在 C1 所在包内的继承类及外部包内所在的继承类均可见

类型在类和包中的 private/protected 可见性
类型的情况就会少一点：
```
package P1 {

  private[P1] class C1
  protected[P1] class C2

  package P11 {
    private[P1] class C3
    protected[P1] class C4
    private[P11] class C5
    protected[P11] class C6
  }


  class C11 extends C1  //< 1, 正确
  class C12 extends C2  //< 2, 正确

  import P11._
  class C13 extends C3  //< 3, 正确
  class C14 extends C4  //< 4, 正确
  class C15 extends C5  //< 5, 错误
  class C16 extends C6  //< 6, 正确
}

package P2 {
  import P1._
  import P1.P11._


  class C21 extends C1  //< 7, 错误
  class C22 extends C2  //< 8, 正确

  class C23 extends C3  //< 9, 错误
  class C24 extends C4  //< 10, 正确
  class C25 extends C5  //< 11, 错误
  class C26 extends C6  //< 12, 正确
}
```
从上面的例子我们可以得出以下结论：

对于 private[package] 声明的类型，在 package 包内及 package 子包内可见；在外部包内不可见
对于 protected[package] 声明的类型，在 package 包内、package 子包内及外部包均可见
有包 package 的子包为 package1，对于 private[package1]，在 package1 包内、package1 子包及其父包即 package 内可见，在外部包不可见
有包 package 的子包为 package1，对于 protected[package1]，在 package1包内、package1子包、package1父包及外部包可见

## 十一.trait

这是我以前在知乎上看到关于类继承作用的回答，虽不完全正确，却十分明确的表达出了好的代码应避免类继承而尽量使用类组合。Scala 显然也非常赞同这一点，以至于有了 trait，又叫做特质。当我们定义特质时，应该要遵循这样的原则：一个 trait 只干一件事，如果要干多件事，就定义多个 trait，然后使用一个类来 extends 这些 traits

**定义 trait**

trait 的定义与 class 类似:
```
scala> trait T {
     | }
defined trait T
```
当然，trait 可以包含成员和方法，并且：

trait 中的成员可以仅声明，也可以声明并指定值
trait 中的方法可以有实现，也可以只有声明而没有实现
```
scala> trait T {
     |   val a: Int
     |   val b: Int = 1
     |
     |   def getA(): Int
     |   def getB() = b
     | }
defined trait T
```
对比而言，类一旦包含未定义的方法就必须声明为 abstract；而 Java 的接口中的方法是不能实现的，必须是抽象方法。如果 trait 既为实现它所声明的方法，也没有定义或声明其他成员，那么在字节码级别，该 trait 其实是接口是相同的

另一个与类不同的是，trait 主构造函数不允许有参数列表，并且不允许为 trait 定义辅助构造函数

混入多个 trait

Scala 类只能有一个父类，但可以混入多个 trait，当要混入多个 traits 或已经继承了某个父类时，需要使用关键字 with，如下例：
```
scala> trait T {
     |   val a: Int
     |   val b: Int = 1
     |
     |   def getA(): Int
     |   def getB() = b
     | }
defined trait T

scala>

scala> trait Q {
     |   def currentTime: String = System.currentTimeMillis().toString
     | }
defined trait Q

scala>

scala> class X extends T with Q {
     |   override val a = 1
     |   override def getA(): Int = a
     | }
defined class X
```
当类混入 trait 时，需要实现 trait 中为实现的成员和方法。要混入多个 trait 是为了保证『高内聚』，通俗说就是一个 trait 只干一件事，如果要干多件事，就定义多个 trait 然后混入它们

当你继承的父类和混入的特质或混入的不同特质之间有同名方法时可能会有冲突，分为以下几种情况：

trait 中的方法未实现：不会冲突
```
scala> class C {
     |   def a: String = "a"
     | }
defined class C

scala>

scala> trait T {
     |   def a: String
     | }
defined trait T

scala>

scala> trait Q extends C with T {}
defined trait Q
```
trait 中的方法实现了且与父类中的方法参数列表及返回类型相同：会冲突
```
scala> class C {
     |   def a: String = "a"
     | }
defined class C

scala>

scala> trait T {
     |   def a: String = ""
     | }
defined trait T

scala>

scala> trait Q extends C with T {}
<console>:9: error: trait Q inherits conflicting members:
  method a in class C of type => String  and
  method a in trait T of type => String
(Note: this can be resolved by declaring an override in trait Q.)
       trait Q extends C with T {}
             ^
```
trait 中的方法实现了且与父类中的参数列表相同，返回类型不同：会冲突
```
scala> class C {
     |   def a: String = "a"
     | }
defined class C

scala>

scala> trait T {
     |   def a: Int = 1
     | }
defined trait T

scala>

scala> trait Q extends C with T {}
<console>:9: error: trait Q inherits conflicting members:
  method a in class C of type => String  and
  method a in trait T of type => Int
(Note: this can be resolved by declaring an override in trait Q.)
       trait Q extends C with T {}
             ^
```
trait 中的方法实现了且与父类的参数列表不同，返回类型相同：不会冲突
```
scala> class C {
     |   def a: String = "a"
     | }
defined class C

scala>

scala> trait T {
     |   def a( i: Int ): String = i.toString
     | }
defined trait T

scala>

scala> trait Q extends C with T {}
defined trait Q
```
**trait 的继承**

一个 trait 同样可以混入其他 trait 或继承类：
```
scala> class C {
     |   def currentTime: String = System.currentTimeMillis().toString
     | }
defined class C

scala>

scala> trait T {
     |   def random: Int
     | }
defined trait T

scala>

scala> trait Q extends C with T {}
defined trait Q
```

## 十二.case class样例类

当你声明了一个 case class，Scala 编译器为你做了这些：
创建 case class 和它的伴生 object
实现了 apply 方法让你不需要通过 new 来创建类实例
```
scala> case class Person(lastname: String, firstname: String, birthYear: Int)
defined class Person

scala> val p = Person("Lacava", "Alessandro", 1976)
p: Person = Person(Lacava,Alessandro,1976)
```
默认为主构造函数参数列表的所有参数前加 val
```
scala> println( p.lastname )
Lacava

scala> p.lastname = "jhon"
<console>:10: error: reassignment to val
   p.lastname = "jhon"
              ^
```
添加天然的 hashCode、equals 和 toString 方法。由于 == 在 Scala 中总是代表 equals，所以 case class 实例总是可比较的
```
scala> val p_1 = new Person( "Brown", "John", 1969 )
p_1: Person = Person(Brown,John,1969)

scala>val p_2 = new Person( "Lacave", "Alessandro", 1976)
p_2: Person = Person(Lacave,Alessandro,1976)

scala> p_1.hashCode
res1: Int = -1362628729

scala> p_1.toString
res2: String = Person(Brown,John,1969)

scala> p_1.equals(p_2)
res3: Boolean = false

scala> p_1 == p_2
res4: Boolean = false
```

生成一个 copy 方法以支持从实例 a 生成另一个实例 b，实例 b 可以指定构造函数参数与 a 一致或不一致
```
//< 保留 lastname 一致，修改 firstname 和 birthYear
scala> val p_3 = p.copy(firstname = "Michele", birthYear = 1972)
p_3: Person = Person(Lacava,Michele,1972)
```
由于编译器实现了 unapply 方法，一个 case class 支持模式匹配
```
scala> case class A( a: Int )
defined class A

scala> case class B( b: String )
defined class B

scala> def classMath( x: AnyRef ): Unit = {
     |   x match {
     |     case A(a) => println( "A:" + a )
     |     case B(b) => println( "B:" + b )
     |     case A => println( A.apply(100) )
     |   }
     | }
classMath: (x: AnyRef)Unit

scala> val a = A( 1 )
a: A = A(1)

scala> val b = B( "b" )
b: B = B(b)

scala> classMath( a )
A:1

scala> classMath( b )
B:b
```

也许你已经知道，在模式匹配中，当你的 case class 没有参数的时候，你是在使用 case object 而不是一个空参数列表的 case class
```
scala> classMath( A )
A(100)
```
除了在模式匹配中使用之外，unapply 方法可以让你结构 case class 来提取它的字段，如：
```
scala> val Person(lastname, _, _) = p
lastname: String = Lacava
```

case class 接收一个 tuple 作为参数，该 tuple 的元素类型与个数与某 case class 相同，那么可以将该tuple 作为 case class 的 tuple 方法参数来构造 case class 实例
```
scala> val meAsTuple: (String, String, Int) = ("Lacava", "Alessandro", 1976)
meAsTuple: (String, String, Int) = (Lacava,Alessandro,1976)

scala> Person.tupled( meAsTuple )
res2: Person = Person(Lacava,Alessandro,1976)
```
相对用 tuple 来创建 case class 实例，还可以从 case class 实例中解构并提取出 tuple 对象
```
scala> val transform: Person => Option[ (String, String, Int) ] = {
 |   Person.unapply _
 | }
transform: Person => Option[(String, String, Int)] = <function1>

scala> transform( p )
res0: Option[(String, String, Int)] = Some((Lacava,Alessandro,1976))

```

**另一种定义 case class 的方式**

还有另一种很少人知道的定义 case class 的方式，如：
```
case class Person( lastname: String )( firstname: String, birthYear: Int )
```
这种方式有点像偏函数，有两个参数列表，要注意的是，对这两个参数列表是区别对待的。上文提到的所有 case class 的特性在这种定义方式下只作用于第一个参数列表中的参数（比如在参数前自动加 val，模式匹配，copy 支持等等），第二个及之后的参数列表中的参数和普通的 class 参数列表参数无异。

firstname和birthYear前不再自动添加 val，不再是类的成员
```
scala> val p = Person("Lacava")("Alessandro", 1976)
p: Person = Person(Lacava)

scala> p.lastname
res0: String = Lacava

scala> p.firstname
<console>:11: error: value firstname is not a member of Person
              p.firstname
                ^

scala> p.birthYear
<console>:11: error: value birthYear is not a member of Person
              p.birthYear
                ^
```
copy 时，当不指定birthYear的值时，不会使用 p 中的birthYear，因为根本没这个值，会报错
```
scala> p.copy()(firstname = "Jhon")
<console>:11: error: not enough arguments for method copy: (firstname: String, birthYear: Int)Person.
Unspecified value parameter birthYear.
              p.copy()(firstname = "Jhon")
```
equals 和 toString 方法也发生了改变：
```
scala> val p_1 = Person("Lacava")("Jhon", 2001)
p_1: Person = Person(Lacava)

scala> p.equals(p_1)
res9: Boolean = true

scala> p == p_1
res10: Boolean = true

scala> println ( p.toString )
Person(Lacava)
```

## 十三.对象

 1、Scala中没有静态方法和静态字段，但是可以用object语法来实现类似的功能。对象定义了某个类的单个实例。

Scala的object中可以用来实现类似的功能，用来存放工具函数或常量等。如

```
object Sequence{
    private var next_num = 0
    val threshold = 100

    def getSequence() = {
        next_num += 1
        next_num
    }
}
```

## 十四.常用操作符

一、常用操作符（操作符其实也是函数）

++ ++[B](that: GenTraversableOnce[B]): List[B] 从列表的尾部添加另外一个列表
++: ++:[B >: A, That](that: collection.Traversable[B])(implicit bf: CanBuildFrom[List[A], B, That]): That 在列表的头部添加一个列表
+: +:(elem: A): List[A] 在列表的头部添加一个元素
:+ :+(elem: A): List[A] 在列表的尾部添加一个元素
:: ::(x: A): List[A] 在列表的头部添加一个元素
::: :::(prefix: List[A]): List[A] 在列表的头部添加另外一个列表
:\ :[B](z: B)(op: (A, B) ⇒ B): B 与foldRight等价

val left = List(1,2,3)
val right = List(4,5,6)

//以下操作等价
left ++ right   // List(1,2,3,4,5,6)
left ++: right  // List(1,2,3,4,5,6)
right.++:(left)    // Listval left = List(1,2,3)(1,2,3,4,5,6)
right.:::(left)  // List(1,2,3,4,5,6)

//以下操作等价
0 +: left    //List(0,1,2,3)
left.+:(0)   //List(0,1,2,3)

//以下操作等价
left :+ 4    //List(1,2,3,4)
left.:+(4)   //List(1,2,3,4)

//以下操作等价
0 :: left      //List(0,1,2,3)
left.::(0)     //List(0,1,2,3)


看到这里大家应该跟我一样有一点晕吧，怎么这么多奇怪的操作符，这里给大家一个提示，任何以冒号结果的操作符，都是右绑定的，即 0 :: List(1,2,3) = List(1,2,3).::(0) = List(0,1,2,3) 从这里可以看出操作::其实是右边List的操作符，而非左边Int类型的操作符

二、常用变换操作
1.map
map[B](f: (A) ⇒ B): List[B]
定义一个变换,把该变换应用到列表的每个元素中,原列表不变，返回一个新的列表数据
Example1 平方变换
```
val nums = List(1,2,3)
val square = (x: Int) => x*x   
val squareNums1 = nums.map(num => num*num)    //List(1,4,9)
val squareNums2 = nums.map(math.pow(_,2))    //List(1,4,9)
val squareNums3 = nums.map(square)            //List(1,4,9)1
```
Example2 保存文本数据中的某几列
```
val text = List("Homeway,25,Male","XSDYM,23,Female")
val usersList = text.map(_.split(",")(0))    
val usersWithAgeList = text.map(line => {
    val fields = line.split(",")
    val user = fields(0)
    val age = fields(1).toInt
    (user,age)
})
```
2.flatMap, flatten
flatten: flatten[B]: List[B] 对列表的列表进行平坦化操作 flatMap: flatMap[B](f: (A) ⇒ GenTraversableOnce[B]): List[B] map之后对结果进行flatten

定义一个变换f, 把f应用列表的每个元素中，每个f返回一个列表，最终把所有列表连结起来。
```
val text = List("A,B,C","D,E,F")
val textMapped = text.map(_.split(",").toList) // List(List("A","B","C"),List("D","E","F"))
val textFlattened = textMapped.flatten          // List("A","B","C","D","E","F")
val textFlatMapped = text.flatMap(_.split(",").toList) // List("A","B","C","D","E","F")
```

3.reduce
reduce[A1 >: A](op: (A1, A1) ⇒ A1): A1
定义一个变换f, f把两个列表的元素合成一个，遍历列表，最终把列表合并成单一元素
Example 列表求和
```
val nums = List(1,2,3)
val sum1 = nums.reduce((a,b) => a+b)   //6
val sum2 = nums.reduce(_+_)            //6
val sum3 = nums.sum                 //6
```
4.reduceLeft,reduceRight
reduceLeft: reduceLeft[B >: A](f: (B, A) ⇒ B): B
reduceRight: reduceRight[B >: A](op: (A, B) ⇒ B): B
reduceLeft从列表的左边往右边应用reduce函数，reduceRight从列表的右边往左边应用reduce函数
Example
```
val nums = List(2.0,2.0,3.0)
val resultLeftReduce = nums.reduceLeft(math.pow)  // = pow( pow(2.0,2.0) , 3.0) = 64.0
val resultRightReduce = nums.reduceRight(math.pow) // = pow(2.0, pow(2.0,3.0)) = 256.0
```
5.fold,foldLeft,foldRight
fold: fold[A1 >: A](z: A1)(op: (A1, A1) ⇒ A1): A1 带有初始值的reduce,从一个初始值开始，从左向右将两个元素合并成一个，最终把列表合并成单一元素。
foldLeft: foldLeft[B](z: B)(f: (B, A) ⇒ B): B 带有初始值的reduceLeft
foldRight: foldRight[B](z: B)(op: (A, B) ⇒ B): B 带有初始值的reduceRight
```
val nums = List(2,3,4)
val sum = nums.fold(1)(_+_)  // = 1+2+3+4 = 9

val nums = List(2.0,3.0)
val result1 = nums.foldLeft(4.0)(math.pow) // = pow(pow(4.0,2.0),3.0) = 4096
val result2 = nums.foldRight(1.0)(math.pow) // = pow(1.0,pow(2.0,3.0)) = 8.0
```
6.sortBy,sortWith,sorted
sortBy: sortBy[B](f: (A) ⇒ B)(implicit ord: math.Ordering[B]): List[A] 按照应用函数f之后产生的元素进行排序
sorted： sorted[B >: A](implicit ord: math.Ordering[B]): List[A] 按照元素自身进行排序
sortWith： sortWith(lt: (A, A) ⇒ Boolean): List[A] 使用自定义的比较函数进行排序
```
val nums = List(1,3,2,4)
val sorted = nums.sorted  //List(1,2,3,4)

val users = List(("HomeWay",25),("XSDYM",23))
val sortedByAge = users.sortBy{case(user,age) => age}  //List(("XSDYM",23),("HomeWay",25))
val sortedWith = users.sortWith{case(user1,user2) => user1._2 < user2._2} //List(("XSDYM",23),("HomeWay",25))
```
7.filter, filterNot
filter: filter(p: (A) ⇒ Boolean): List[A]
filterNot: filterNot(p: (A) ⇒ Boolean): List[A]
filter 保留列表中符合条件p的列表元素 ， filterNot，保留列表中不符合条件p的列表元素
```
val nums = List(1,2,3,4)
val odd = nums.filter( _ % 2 != 0) // List(1,3)
val even = nums.filterNot( _ % 2 != 0) // List(2,4)
```

8.count
count(p: (A) ⇒ Boolean): Int
计算列表中所有满足条件p的元素的个数，等价于 filter(p).length
```
val nums = List(-1,-2,0,1,2) 
val plusCnt1 = nums.count(_> 0) 
val plusCnt2 = nums.filter(_> 0).length 
```
9. diff, union, intersect
  diff:diff(that: collection.Seq[A]): List[A] 保存列表中那些不在另外一个列表中的元素，即从集合中减去与另外一个集合的交集
  union : union(that: collection.Seq[A]): List[A] 与另外一个列表进行连结
  intersect: intersect(that: collection.Seq[A]): List[A] 与另外一个集合的交集
```
val nums1 = List(1,2,3)
val nums2 = List(2,3,4)
val diff1 = nums1 diff nums2   // List(1)
val diff2 = nums2.diff(num1)   // List(4)
val union1 = nums1 union nums2  // List(1,2,3,2,3,4)
val union2 = nums2 ++ nums1        // List(2,3,4,1,2,3)
val intersection = nums1 intersect nums2  //List(2,3)
```
10.distinct

distinct: List[A] 保留列表中非重复的元素，相同的元素只会被保留一次
```
val list = List("A","B","C","A","B") val distincted = list.distinct // List("A","B","C")1
```
11.groupBy, grouped
groupBy : groupBy[K](f: (A) ⇒ K): Map[K, List[A]] 将列表进行分组，分组的依据是应用f在元素上后产生的新元素 
grouped: grouped(size: Int): Iterator[List[A]] 按列表按照固定的大小进行分组
```
val data = List(("HomeWay","Male"),("XSDYM","Femail"),("Mr.Wang","Male"))
val group1 = data.groupBy(_._2) // = Map("Male" -> List(("HomeWay","Male"),("Mr.Wang","Male")),"Female" -> List(("XSDYM","Femail")))
val group2 = data.groupBy{case (name,sex) => sex} // = Map("Male" -> List(("HomeWay","Male"),("Mr.Wang","Male")),"Female" -> List(("XSDYM","Femail")))
val fixSizeGroup = data.grouped(2).toList // = Map("Male" -> List(("HomeWay","Male"),("XSDYM","Femail")),"Female" -> List(("Mr.Wang","Male")))
```
12.scan
scan[B >: A, That](z: B)(op: (B, B) ⇒ B)(implicit cbf: CanBuildFrom[List[A], B, That]): That
由一个初始值开始，从左向右，进行积累的op操作，这个比较难解释，具体的看例子吧。
```
val nums = List(1,2,3)
val result = nums.scan(10)(_+_)   // List(10,10+1,10+1+2,10+1+2+3) = List(10,11,13,16)
```
13.scanLeft,scanRight
scanLeft: scanLeft[B, That](z: B)(op: (B, A) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That
scanRight: scanRight[B, That](z: B)(op: (A, B) ⇒ B)(implicit bf: CanBuildFrom[List[A], B, That]): That
scanLeft: 从左向右进行scan函数的操作，scanRight：从右向左进行scan函数的操作
```
val nums = List(1.0,2.0,3.0)
val result = nums.scanLeft(2.0)(math.pow)   // List(2.0,pow(2.0,1.0), pow(pow(2.0,1.0),2.0),pow(pow(pow(2.0,1.0),2.0),3.0) = List(2.0,2.0,4.0,64.0)
val result = nums.scanRight(2.0)(math.pow)  // List(2.0,pow(3.0,2.0), pow(2.0,pow(3.0,2.0)), pow(1.0,pow(2.0,pow(3.0,2.0))) = List(1.0,512.0,9.0,2.0)
```

14.take,takeRight,takeWhile
take : takeRight(n: Int): List[A] 提取列表的前n个元素 takeRight: takeRight(n: Int): List[A] 提取列表的最后n个元素 takeWhile: takeWhile(p: (A) ⇒ Boolean): List[A] 从左向右提取列表的元素，直到条件p不成立
```
val nums = List(1,1,1,1,4,4,4,4)
val left = nums.take(4)   // List(1,1,1,1)
val right = nums.takeRight(4) // List(4,4,4,4)
val headNums = nums.takeWhile( _ == nums.head)  // List(1,1,1,1)
```

15.drop,dropRight,dropWhile
drop: drop(n: Int): List[A] 丢弃前n个元素，返回剩下的元素 dropRight: dropRight(n: Int): List[A] 丢弃最后n个元素，返回剩下的元素 dropWhile: dropWhile(p: (A) ⇒ Boolean): List[A] 从左向右丢弃元素，直到条件p不成立
```
val nums = List(1,1,1,1,4,4,4,4)
val left = nums.drop(4)   // List(4,4,4,4)
val right = nums.dropRight(4) // List(1,1,1,1)
val tailNums = nums.dropWhile( _ == nums.head)  // List(4,4,4,4)
```

16.span, splitAt, partition
span : span(p: (A) ⇒ Boolean): (List[A], List[A]) 从左向右应用条件p进行判断，直到条件p不成立，此时将列表分为两个列表
splitAt: splitAt(n: Int): (List[A], List[A]) 将列表分为前n个，与，剩下的部分
partition: partition(p: (A) ⇒ Boolean): (List[A], List[A]) 将列表分为两部分，第一部分为满足条件p的元素，第二部分为不满足条件p的元素
```
val nums = List(1,1,1,2,3,2,1)
val (prefix,suffix) = nums.span( _ == 1) // prefix = List(1,1,1), suffix = List(2,3,2,1)
val (prefix,suffix) = nums.splitAt(3)  // prefix = List(1,1,1), suffix = List(2,3,2,1)
val (prefix,suffix) = nums.partition( _ == 1) // prefix = List(1,1,1,1), suffix = List(2,3,2)
```

17.padTo
```
padTo(len: Int, elem: A): List[A]
将列表扩展到指定长度，长度不够的时候，使用elem进行填充，否则不做任何操作。
 val nums = List(1,1,1)
 val padded = nums.padTo(6,2)   // List(1,1,1,2,2,2)
```

18.combinations,permutations
```
combinations: combinations(n: Int): Iterator[List[A]] 取列表中的n个元素进行组合，返回不重复的组合列表，结果一个迭代器
permutations: permutations: Iterator[List[A]] 对列表中的元素进行排列，返回不重得的排列列表，结果是一个迭代器
val nums = List(1,1,3)
val combinations = nums.combinations(2).toList //List(List(1,1),List(1,3))
val permutations = nums.permutations.toList        // List(List(1,1,3),List(1,3,1),List(3,1,1))
```
19.zip, zipAll, zipWithIndex, unzip,unzip3
zip: zip[B](that: GenIterable[B]): List[(A, B)] 与另外一个列表进行拉链操作，将对应位置的元素组成一个pair，返回的列表长度为两个列表中短的那个
zipAll: zipAll[B](that: collection.Iterable[B], thisElem: A, thatElem: B): List[(A, B)] 与另外一个列表进行拉链操作，将对应位置的元素组成一个pair，若列表长度不一致，自身列表比较短的话使用thisElem进行填充，对方列表较短的话使用thatElem进行填充
zipWithIndex：zipWithIndex: List[(A, Int)] 将列表元素与其索引进行拉链操作，组成一个pair
unzip: unzip[A1, A2](implicit asPair: (A) ⇒ (A1, A2)): (List[A1], List[A2]) 解开拉链操作
unzip3: unzip3[A1, A2, A3](implicit asTriple: (A) ⇒ (A1, A2, A3)): (List[A1], List[A2], List[A3]) 3个元素的解拉链操作
```
val alphabet = List("A",B","C")
val nums = List(1,2)
val zipped = alphabet zip nums   // List(("A",1),("B",2))
val zippedAll = alphabet.zipAll(nums,"*",-1)   // List(("A",1),("B",2),("C",-1))
val zippedIndex = alphabet.zipWithIndex  // List(("A",0),("B",1),("C",3))
val (list1,list2) = zipped.unzip        // list1 = List("A","B"), list2 = List(1,2)
val (l1,l2,l3) = List((1, "one", '1'),(2, "two", '2'),(3, "three", '3')).unzip3   // l1=List(1,2,3),l2=List("one","two","three"),l3=List('1','2','3')
```
20.slice
slice(from: Int, until: Int): List[A] 提取列表中从位置from到位置until(不含该位置)的元素列表
```
val nums = List(1,2,3,4,5)
val sliced = nums.slice(2,4)  //List(3,4)
```
21.sliding
sliding(size: Int, step: Int): Iterator[List[A]] 将列表按照固定大小size进行分组，步进为step，step默认为1,返回结果为迭代器
```
val nums = List(1,1,2,2,3,3,4,4)
val groupStep2 = nums.sliding(2,2).toList  //List(List(1,1),List(2,2),List(3,3),List(4,4))
val groupStep1 = nums.sliding(2).toList //List(List(1,1),List(1,2),List(2,2),List(2,3),List(3,3),List(3,4),List(4,4)) 
```

22.updated
updated(index: Int, elem: A): List[A] 对列表中的某个元素进行更新操作
```
val nums = List(1,2,3,3)
val fixed = nums.updated(3,4)  // List(1,2,3,4)
```

## 十五.快学scala练习题

练习：

1.设置一个映射,其中包含你想要的一些装备，以及它们的价格。然后构建另一个映射，采用同一组键，但是价格上打9折

```
scala> val price = Map("ipad" -> 4000,"iPhone" -> 6000, "iWatch" -> 3000)
price: scala.collection.immutable.Map[String,Int] = Map(ipad -> 4000, iPhone ->
6000, iWatch -> 3000)

scala> val newprice = for((k,v) <- price) yield (k, v * 0.9)
newprice: scala.collection.immutable.Map[String,Double] = Map(ipad -> 3600.0, iP
hone -> 5400.0, iWatch -> 2700.0)
```
2.编写一段程序，从文件中读取单词。用一个可变映射来清点每个单词出现的频率。读取这些单词的操作可以使用java.util.Scanner:
val in = new java.util.Scanner(new java.io.File("myfile.txt")) while(in.hasNext()) 处理 in.next()最后，打印出所有单词和它们出现的次数。
```
import scala.io.Source
import scala.collection.mutable.HashMap
object Pract {
   def main(args: Array[String])  = {
      val source = Source.fromFile("file.txt").mkString
      val tokens = source.split("\s+")
      val map = new HashMap[String,Int]
  for(key <- tokens){
    map(key) = map.getOrElse(key, 0) + 1
  }
  println(map.mkString(","))      
   }
}
```
3.重复前一个练习，这次用不可变的映射
不可变映射与可变映射的区别就是每次添加新的元素时都会返回一个新的映射。

```
import scala.io.Source
object Pract {
   def main(args: Array[String])  = {
      val source = Source.fromFile("file.txt").mkString
      val tokens = source.split("\s+")
      var map = MapString,Int //注意这里用的 var 了

  for(key <- tokens){
      map += (key -> (map.getOrElse(key, 0) + 1))
  }
  println(map.mkString(","))      
   }
}
```
4.重复前一个练习，这次使用已排序的映射，以便单词可以按顺序打印出来
```
import scala.io.Source
import scala.collection.SortedMap

object Pract {
   def main(args: Array[String])  = {
      val source = Source.fromFile("file.txt").mkString
      val tokens = source.split("\s+")
      var sortedmap = SortedMapString,Int //注意这里用的 var 了

  for(key <- tokens){
      sortedmap += (key -> (sortedmap.getOrElse(key, 0) + 1))
  }
  println(sortedmap.mkString(","))      
   }
}
```
5.重复前一个练习，这次使用java.util.TreeMap并使之适用于Scala API
```
import scala.io.Source
import scala.collection.mutable.Map
import scala.collection.JavaConversions.mapAsScalaMap
import java.util.TreeMap

object Pract {
   def main(args: Array[String])  = {
      val source = Source.fromFile("file.txt").mkString
      val tokens = source.split("\s+")
      val map:Map[String,Int] = new TreeMap[String,Int]

  for(key <- tokens){
     map(key) = map.getOrElse(key, 0) + 1
  }
  println(map.mkString(","))   
   }
}
```
6.定义一个链式哈希映射,将"Monday"映射到java.util.Calendar.MONDAY,依次类推加入其他日期。展示元素是以插入的顺序被访问的
```
import scala.collection.mutable.LinkedHashMap
import java.util.Calendar

object Pract {
   def main(args: Array[String])  = {
      val map = new LinkedHashMap[String, Int]
      map += ("MONDAY" -> Calendar.MONDAY)
      map += ("TUESDAY" -> Calendar.TUESDAY)
      map += ("WENDSDAY" -> Calendar.WEDNESDAY)
      map += ("THURSDAY" -> Calendar.THURSDAY)
      map += ("FRIDAY" -> Calendar.FRIDAY)
      map += ("SATURDAY" -> Calendar.SATURDAY)
      map += ("SUNDAY" -> Calendar.SUNDAY)
      println(map.mkString(","))
   }
}
```

7.打印出所有Java系统属性的表格

JAVA系统属性转scala map的使用

```
import scala.collection.JavaConversions.propertiesAsScalaMap
import scala.collection.Map

object Pract {
   def main(args: Array[String])  = {
      val props: Map[String,String] = System.getProperties
      val keys = props.keySet
      val keylength = for( key <- keys) yield key.length
      val maxlength = keylength.max
      for( key <- keys) {
        print(key)
        print(" " * (maxlength - key.length))
        print("| ")
        println(props(key))
      }

   }
}
```
8.编写一个函数minmax(values:Array[Int]),返回数组中最小值和最大值的对偶
```
def minmax(values:Array[Int])  = {
     (values.max,values.min)
   }
```
9.编写一个函数Iteqgt(values:Array[int],v:Int),返回数组中小于v,等于v和大于v的数量，要求三个值一起返回

```
def Iteqgt(values:Array[Int],v:Int) = {
     var a,b,c=0
     for(value <- values){
       if(value > v) a += 1
       else if(value == v) b += 1
       else c += 1
     }
     (a,b,c)
   }

   def Iteqgt1(values:Array[Int],v:Int) = {
     (values.count( > v),values.count( == v), values.count(_ < v))
   }
```
10.当你将两个字符串拉链在一起，比如"Hello".zip("World")，会是什么结果？想出一个讲得通的用例
```
scala> "Hello".zip("world")
res0: scala.collection.immutable.IndexedSeq[(Char, Char)] = Vector((H,w), (e,o),
 (l,r), (l,l), (o,d))

```

## 十六.构造器

Scala的类可以有一个主构造器和多个辅助构造器。每个辅助构造器的名称为this，每一个辅助构造器都必须以调用已经定义的辅助构造器或主构造器开始定义

- 主构造器


> 如果一个类没有显示定义主构造器，则有一个默认的无参主构造器     如定义一个Student类

```
class Student(val name:String, var age:Int = 0, address:String = "", private var school:String = ""){
    var grade:Int = if( age>7 ) age -7 else 0
    println(" I'm in main constructor. ")
    def info() = " name is "+name+", age is "+age+", address is "+address
}
```
　　对于Scala类，主构造器的参数放置在类名后，由括号括起来。且对于主构造器中var、val、private 等标注的参数，都会成为类的对应字段，并生成对应的默认getter、setter方法。如Student类中的name、age、school等。对于主构造器中的未用var、val标注的参数，如果在类的任何一个方法用用到该参数，该参数将会转换为类的字段，否则不会，如Student类的address属性。

　　由于在Student类中的info方法中用到了参数address，所以Student共有name、age、address、school、grade等5个属性，且Scala根据对应属性的特点生成了默认的getter和setter方法。

　　对于主构造器的参数，也可以提供参数默认值。通过为主构造器提供默认值可减少辅助构造器的个数
　　主构造器的函数体，是类中除了方法定义以外的其他语句，如在Student类的主构造器中，包含grade属性的初始化和prinln这两行语句。

![](https://static.oschina.net/uploads/space/2018/1116/110347_10HB_3005534.png)

- 辅助构造器
>  辅助构造器通过this来定义，且必须首先调用主构造器或者其他已经定义的辅助构造器。

```
class Person(val name:String){
    var age = 0
    var sex:Char = 'f'
    println("main constructor...")

    def this(name:String,  age:Int){
        this(name)        //调用主构造器
        this.age = age     //使用this关键字
        println(" auxiliary constructor1 ")
    }

    def this(name:String, age:Int, sex:Char){
        this(name, age)
        this.sex = sex
        println(" auxiliary constructor2 ")
    }
}
```

> 【注：辅助构造器的参数前不能添加val、var标志，否则会报错。】

![](https://static.oschina.net/uploads/space/2018/1116/110550_hUV6_3005534.png)

- 私有主构造器


```
class Person private(val name:String){
    var age:Int = 1
    def this(name: String, age:Int){
        this(name)
        this.age = age
    }
}
```





# hadoop

## 一.Nodepad远程linux插件NppFTP

**1.在该github上下载自己notepad++对应版本位数的插件**
[NppFTP下载地址](https://github.com/ashkulz/NppFTP/releases/tag/v0.27.6 )：https://github.com/ashkulz/NppFTP/releases/tag/v0.27.6

> 下载时可以因为被墙的原因下载不了，如果有跨网服务器，直接wget 实际下载地址
> ![输入图片说明](https://static.oschina.net/uploads/img/201901/11105326_7IeQ.png "在这里输入图片标题")

> 软件如果下载不了 可以到百度网盘qq939598604/我的软件/NppFTP目录下下载

**2.下载之后进行解压，然后将bin目录下的dll文件拷贝到notepad++的安装目录下的插件目录**
notepad++的安装目录可以右键notepad++的快捷方式，找到安装目录
![输入图片说明](https://static.oschina.net/uploads/img/201901/11105635_bvCw.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201901/11105728_rL2U.png "在这里输入图片标题") 

**3.重启notepad++**

**4.重启后，在插件菜单中会显示该插件**
![输入图片说明](https://static.oschina.net/uploads/img/201901/11105829_SBf5.png "在这里输入图片标题")

**5.NppFTP使用**
![输入图片说明](https://static.oschina.net/uploads/img/201901/11110929_I4h9.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201901/11110936_ZoEY.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201901/11110948_7ATB.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201901/11110955_AfYH.png "在这里输入图片标题")

![输入图片说明](https://static.oschina.net/uploads/img/201901/11112540_Mc1p.png "在这里输入图片标题")



##  二.实现linux集群所有机器免密钥登录

**1.先安装expect** 
```
yum install expect
```
**2.生成密钥 **
```
ssh-keygen(注,一路回车,不用管)
```
**3.修改host文件 /etc/hosts**
```
192.168.197.21 master
192.168.197.25 slave1
192.168.197.27 slave2 
```
**4.编写shell脚本 vim.ssh_copy_id_to_all.sh**
```
#!/bin/bash
SERVERS="master slave1 slave2"
PASSWORD=root
auto_ssh_copy_id() {
    expect -c "set timeout -1;
        spawn ssh-copy-id $1;
        expect {
            *(yes/no)* {send -- yes\r;exp_continue;}
            *assword:* {send -- $2\r;exp_continue;}
            eof        {exit 0;}
        }";
}

ssh_copy_id_to_all() {
    for SERVER in $SERVERS
    do
        auto_ssh_copy_id $SERVER $PASSWORD
    done
}

ssh_copy_id_to_all
```
**5.chomd +x ssh_copy_id_to_all.sh**
**6.执行脚本**
./ssh_copy_id_to_all.sh

## 三.Windows安装部署hadoop-2.7.5

**(1).编辑“M:\soft\hadoop-2.7.5\etc\hadoop”下的core-site.xml文件，将下列文本粘贴进去，并保存；**

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/M:/soft/hadoop-2.7.5/tmp</value>
    </property>
    <property>
        <name>dfs.name.dir</name>
        <value>/M:/soft/hadoop-2.7.5/name</value>
    </property>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

**(2).编辑“M:\soft\hadoop-2.7.5\etc\hadoop”目录下的mapred-site.xml(没有就将mapred-site.xml.template重命名为mapred-site.xml)文件，粘贴一下内容并保存:**

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
       <name>mapreduce.framework.name</name>
       <value>yarn</value>
    </property>
    <property>
       <name>mapred.job.tracker</name>
       <value>hdfs://localhost:9001</value>
    </property>
</configuration>
```

**(3).编辑“M:\soft\hadoop-2.7.5\etc\hadoop”目录下的hdfs-site.xml文件，粘贴以下内容并保存。请自行创建data目录，在这里我是在HADOOP_HOME目录下创建了data目录:**

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 这个参数设置为1，因为是单机版hadoop -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>/M:/soft/hadoop-2.7.5/data</value>
    </property>
</configuration>
```

**(4).编辑“M:\soft\hadoop-2.7.5\etc\hadoop”目录下的yarn-site.xml文件，粘贴以下内容并保存；**

```
<?xml version="1.0"?>
<configuration>
    <property>
       <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
    </property>
    <property>
       <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
       <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
</configuration>
```

**(5).编辑“M:\soft\hadoop-2.7.5\etc\hadoop”目录下的hadoop-env.cmd文件，将JAVA_HOME用 @rem注释掉，编辑为JAVA_HOME的路径，然后保存:**

```
@rem set JAVA_HOME=%JAVA_HOME%
set JAVA_HOME=M:\soft\Java\jdk1.8.0_101
```

**替换文件** 将下载好的hadooponwindows-master.zip（笔记第一步有下载地址，不知道可以去笔记开头的需求栏目查看）解压，将解压后的**\*bin目录下的所有文件直接覆盖Hadoop的bin目录***。

## 四.hadoop伪分布式安装

**一.安装好java环境**

**二.配置Hadoop**

1.解压缩hadoop-2.7.5.tar.gz，编辑etc/hadoop/hadoop-env.sh文件如下 hadoop-env.sh 文件:

```
export JAVA_HOME=/soft/jdk1.0.8
```

**三.伪分布式配置**

1.配置core-site.xml文件

```
<configuration>
    <property>
        <name>hadoop.tmp.dir</name> 
        <value>file:/soft/hadoop-2.7.5/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

hadoop.tmp.dir用来存放hadoop运行过程中临时文件的目录，目录指定为/usr/local/hadoop/tmp，如果不设置这个目录，那么当hadoop关闭后某些系统临时目录会被清空，当下一次需要启动hadoop时需要重新进行初始化。所以这里人工指定目录可以避免被清空。

df.defaultFS设置好逻辑名称，当我们下次需要访问分布式文件系统的时候，用localhost:9000就可以访问了。

2.配置etc/hadoop/hdfs-site.xml文件

```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/soft/hadoop-2.7.5/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/soft/hadoop-2.7.5/tmp/dfs/data</value>
    </property>
</configuration>
```

dfs.replication表示副本的数量，伪分布式要设置为1。

dfs.namenode.name.dir表示本地磁盘目录，是存储fsimage文件的地方。

dfs.datanode.data.dir表示本地磁盘目录，HDFS数据存放block的地方。

**四、设置SSH 登录** 

1.检测能否登录，如果需要输入密码，需要执行下面的2步骤

```
ssh localhost
```

2.如果不可以，设置本地SSH登录

```
ssh-keygen  -t  rsa  -P ''  -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
```

再重复执行 ssh localhost 不需要输入密码即可

**五、启动hadoop HDFS，执行本地MR任务**

1.格式化HDFS

```
hdfs namenode -format
```

2.开启NameNode和DataNode

```
./hadoop-deamon.sh start namenode 
./hadoop-deamon.sh start datanode
【或者直接启动文件系统的namenode datanode 】
 start-hdfs.sh
```

3.打开NameNode信息管理页面http://localhost:50070/

4.停止HDFS

```
sbin/stop-dfs.sh
```

**六、单独节点上运行YARN**

通过设置一些参数，可以运行MR任务在伪分布式上。

1.配置map-site.xml 

```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property> 
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>127.0.0.1:19888</value>
    </property>
</configuration>
```

2. 配置yarn-site.xml 

```
<configuration>
    <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

3.开启ResourceManager daemon and NodeManager

```
./start-yarn.sh
```

4.打开ResourceManager资源管理，默认地址：http://localhost:8088/cluster









## 五.hadoop集群安装

**（一）.安装环境 ：所有的软件安装在根目录下的/soft目录**

java---/soft/jdk1.0.8
hadoop--/soft/hadoop2.7.4
固定hadoop配置变量(JAVA_HOME,主机名称,hadoop的固定目录)可以不用安装那么多
hadoop-env.sh 文件:
```
export JAVA_HOME=/soft/jdk1.0.8
```
core-site.xml文件: 
```
<name>fs.defaultFS</name>
<value>hdfs://node1:9000</value>
```
hdfs-site.xml文件：
```
<name>dfs.namenode.name.dir</name>
<value>file:/soft/hadoop-2.7.4/tmp/dfs/name</value>
```
slaves文件
```
slave1   slave2 
```

集群规划
主机名           ip          安装的软件         进程
master   192.168.197.255  jdk、hadoop  namenode ressourcemanager
slave1   192.168.197.256  jdk、hadoop  datanode secondnamenode
slave2   192.168.197.257  jdk、hadoop  datanade

**（二）.安装JDK** 

1.下载jdk1.8.0_161
2.在/etc/profile中添加如下配置
sudo vim /etc/profile
```
export JAVA_HOME=/soft/jdk1.8.0_161
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
```
3.使环境变量生效，source /etc/profile
4.安装验证# java -version 


**（三）.准备host文件和修改主机名称**
1.vim /etc/hosts
```
192.168.197.225 master
192.168.197.226 slave1
192.168.197.227 slave2

```
2.拷贝/etc/hosts到其它主机
   scp /etc/hosts slave1:/etc/
   scp /etc/hosts slave2:/etc/

3.修改主机名
   vim /etc/hosts  
   master slave1 slave2 

**（四）.免登录**
1.注意将防火墙关掉
CentOS7 

```
 (1)关闭防火墙：sudo systemctl stop firewalld.service
 (2)关闭开机启动：sudo systemctl disable firewalld.service
 (3)安装iptables防火墙：sudo yum install iptables-services
 (4)设置iptables防火墙开机启动：sudo systemctl enable iptables
```

ubuntu

```
(1)关闭ubuntu的防火墙 ufw disable
(2)开启防火墙 ufw enable
(3)卸载了iptables apt-get remove iptables 
(4)关闭ubuntu中的防火墙的其余命令
    iptables -P INPUT ACCEPT
    iptables -P FORWARD ACCEPT
    iptables -P OUTPUT ACCEPT
    iptables -F
```

2.ssh无密登录,
(1)在集群/etc/ssh/sshd_config 文件去掉以下选项的注释
sudo vim /etc/ssh/sshd_config
```
Port 22
Protocol 2
RSAAuthentication yes      #开启私钥验证
PubkeyAuthentication yes   #开启公钥验证
```
3.生成秘钥
(1)在主从节点(集群的每一个节点节点)输入命令 ，生成 key，一律回车
   ssh-keygen -t rsa -P ''
(2)将从节点(集群的每一个节点节点)公钥收集到一个文件中authorized_keys，并发送到各个节点
从节点配置：
      在slave1的机器：scp /root/.ssh/id_rsa.pub master:/root/.ssh/id_rsa.pub.s1
      在slave2的机器：scp /root/.ssh/id_rsa.pub master:/root/.ssh/id_rsa.pub.s2
主节点配置：
(3)将所有机器的id_rsa.pub文件收集到authorized_keys，并发送到各个节点
   cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
   cat /root/.ssh/id_rsa.pub.s1 >> /root/.ssh/authorized_keys
   cat /root/.ssh/id_rsa.pub.s2 >> /root/.ssh/authorized_keys
(4)最后将生成的包含三个节点的秘钥的authorized_keys 复制到s1和s2的.ssh目录下（ 
   scp /root/.ssh/authorized_keys slave1:/root/.ssh/
   scp /root/.ssh/authorized_keys slave2:/root/.ssh/

验证ssh免密码登录
1.输入命令ssh  localhost(主机名) 根据提示输入“yes” 
2.输入命令exit注销（Logout）
3.再次输入命令ssh localhost即可直接登录


**（五）hadoop的配置**
(1)编辑 hadoop-env.sh 文件,找到 JAVA_HOME 改为 JDK 的安装目录
   sudo vim /soft/hadoop-2.7.4/etc/hadoop/hadoop-env.sh
   export JAVA_HOME=/soft/jdk1.8.0_161
(2)修改 core-site.xml
   sudo vim core-site.xml
   ```
<configuration>
       <property>
           <name>fs.defaultFS</name>
           <value>hdfs://master:9000</value>
       </property>
       <property>
           <name>hadoop.tmp.dir</name>
           <value>file:/soft/hadoop-2.7.4/tmp</value>
       </property>
   </configuration>
   ```
(2)修改 hdfs-site.xml
   sudo vim hdfs-site.xml
```
<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>master:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/soft/hadoop-2.7.4/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/soft/hadoop-2.7.4/tmp/dfs/data</value>
    </property>
</configuration>
```
(3)修改 mapred-site.xml
目录下么没有这个文件,这有一个模板,我们需要先拷贝一份
 cp mapred-site.xml.template mapred-site.xml
 vim  mapred-site.xml
```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
    </property>
</configuration>

```
(4)修改 yarn-site.xml
vi yarn-site.xml
```
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

**（六）配置集群**
(1)配置slaves文件

修改（Master主机）/soft/hadoop-2.7.4/etc/hadoop/slaves该文件指定哪些服务器节点是datanode节点。删除locahost，添加所有datanode节点的主机名
sudo vim /soft/hadoop-2.7.4/etc/hadoop/slaves
```
slave1
slave2
```
(2)复制节点,将hadoop-2.7.4 文件夹重打包后复制到其他子节点

```
scp -rp /soft/hadoop-2.7.4/ root@slave1:/soft
scp -rp /soft/hadoop-2.7.4/ root@slave2:/soft
```

(3)格式化namenode和datanode并启动，（在master上执行就可以了 不需要在slave上执行）

```
cd /soft/hadoop-2.7.4/bin
./hadoop namenode -format
./hadoop datanode -format
```

**（七）启动 hadoop**

cd /soft/hadoop-2.7.4/sbin
./start-dfs.sh
./start-yarn.sh
./mr-jobhistory-daemon.sh start historyserver
或者
./start-all.sh
./mr-jobhistory-daemon.sh start historyserver

**（八）查看进程服务**
查看启动进程,缺少以下任一进程都表示出错
$ jps
2528 NameNode
2720 SecondaryNameNode
2872 ResourceManager
3151 JobHistoryServer
查看端口占用情况
netstat -tnlp | grep java
访问master
http://192.168.197.255:50070
http://192.168.197.255:8088

**（九）停止 hadoop**
cd /soft/hadoop-2.7.4/sbin
./stop-all.sh

#  

#  hbase

## 一.hbase在linux系统本地模式

1.安装好jdk
2.下载hbase hbase 下载地址：http://hbase.apache.org/

```
hbase-2.0.2-bin.tar.gz
```
3.上传到linux服务的/soft目录下
4.tar 开hbase压缩包
```
tar -zxvf /soft/hbase-2.0.2-bin.tar.gz -C /soft/
```
5.修改conf/hbase-env.sh 
vim /soft/hbase-2.0.2/conf/hbase-env.sh
```
export JAVA_HOME=/soft/jdk1.8.0_161
```
6.编辑hbase-site.xml 
```
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>file:///soft/hbase-2.0.2/data</value>
    </property>
</configuration>

```
7.新建hbase数据存放目录
```
mkdir /soft/hbase-2.0.2/data
```
8.启动hbase
```
cd /soft/hbase-2.0.2/bin
./bin/start-hbase.sh
```
9.jps 查看后 出现Hmaster就是启动成功 然后就可以进入shell进行对hbase的操作
```
[root@master hbase-2.0.2]# jps
4359 Main
5532 Jps
4829 HMaster
```
10.进入hbase shell
```
./bin/hbase shell
```
11.访问web
```
http://192.168.197.21:16010/master-status
```


**shell环境测试**
创建表
```
hbase(main):016:0> create 't1', {NAME => 'f1', VERSIONS => 1}
```
查看表
```
hbase(main):017:0> list
TABLE
t1
1 row(s)
Took 0.0053 seconds                                                                   
=> ["t1"]
```
插入一条数据
```
hbase(main):019:0> put 't1', 'r1', 'f1', 'v1'
```
扫描t1表的全数据
```
hbase(main):018:0>  scan 't1'
ROW                       COLUMN+CELL                                                 
r1                       column=f1:, timestamp=1540885480142, value=v1                
1 row(s)
```

## 二.Hbase在linux集群搭建

软件放置路径为初级配置的路径/soft/hbase-1.3.1
1.解压已经安装整理过的压缩包hbase-1.3.1-install.tar.gz
```
tar -zxvf /soft/hbase-1.3.1-install.tar.gz -C /soft/
```
2.修改hbase-env环境变量
vim /soft/hbase-1.3.1/conf/hbase-env.sh
```
export JAVA_HOME=/soft/jdk1.8.0_161
export HBASE_CLASSPATH=/soft/hadoop-2.7.4
export HBASE_MANAGES_ZK=false          # 不使用自带的zk，使用独立的zookeeper
```
3.修改hbase-site.xml
vim /soft/hbase-1.3.1/conf/hbase-site.xml    # 配置站点信息
```
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://master:9000/hbase</value>
    </property>
    <property>
        <name>hbase.master</name>
        <value>master</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.clientPort</name>
        <value>2181</value>                                     # 这里指的是zook的端口
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>                     # 主机名一定要对应上
        <value>master,slave1,slave2</value>
    </property>
    <property>
        <name>zookeeper.session.timeout</name>                  # zook的session超时时长
        <value>60000000</value>
    </property>
    <property>
        <name>dfs.support.append</name>
        <value>true</value>
    </property>
</configuration>
```
4.指定添加regionservers
vim /soft/hbase-1.3.1/conf/regionservers# 配置从节点 一定要对应上
```
master
slave1
slave2
```
5.复制/soft/hbase-1.3.1到各个从的机器
```
scp /soft/hbase-1.3.1 root@slave1:/soft/
scp /soft/hbase-1.3.1 root@slave2:/soft/
```
6.在各个节点添加hbase的环境变量
vim /etc/profile
```
export HBASE_HOME=/soft/hbase-1.3.1
export PATH=$HBASE_HOME/bin:$PATH
```
7.在master启动hbase
```
/soft/hbase-1.3.1/bin/start-hbase.sh
```
8.浏览器检查打开master机器的端口16010
http://192.168.197.231:16010/master-status

## 三.hbase在window环境下安装

1.安装jdk
```
默认JDK已安装并配置好环境变量，本处用的jdk1.8.0_101 
```
2、下载hbase-2.0.2-bin.tar.gz
```
解压到C:\hbase-2.0.2\目录下
```

3、下载hadoop-common-2.2.0-bin-master
```
hadoop-common-2.2.0-bin-master(包含windows端开发Hadoop2.2需要的winutils.exe)，HBase在Windows下部署需要使用到。    
地址：https://github.com/srccodes/hadoop-common-2.2.0-bin，下载hadoop-common-2.2.0-bin-master.zip，解压缩到c:\hadoop-common-2.2.0-bin-master。
```

4、修改HBase下的conf/hbase-env.cmd
```
set JAVA_HOME=M:\soft\Java\jdk1.8.0_101
set HBASE_MANAGES_ZK=true
```

5.修改HBase下的hbase-site.xml
```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>  
		<name>hbase.rootdir</name>  
		<value>file:///C:/hbase-2.0.2/data</value>  
	</property>  
	<property>  
		<name>hbase.tmp.dir</name>  
		<value>C:/hbase-2.0.2/tmp</value>  
	</property>  
	<property>  
		<name>hbase.zookeeper.quorum</name>  
		<value>127.0.0.1</value>  
	</property>  
	<property>  
		<name>hbase.zookeeper.property.dataDir</name>  
		<value>C:/hbase-2.0.2/tmp/zoo</value>  
	</property>  
	<property>  
		<name>hbase.cluster.distributed</name>  
		<value>false</value>  
	</property>
</configuration>

```

6.配置用户变量HADOOP_HOME
```
新建环境变量HADOOP_HOME，值为C:\hadoop-common-2.2.0-bin-master
在path后添加：%HADOOP_HOME%\bin

```

7.启动HBase
```
在C:\hbase-2.0.2\bin下打开命令行，输入start-hbase.cmd，启动HBase。
```
8.测试Shell
```
 HBase启动后，在命令行输入hbase shell，打卡HBase的shell命令行
```
9.打开HBase主页，网址：http://127.0.0.1:16010/master-status
10.可以通过测试命令建表测试等等

## 四.hbase的shell操作 

**1.进入hbase shell**

```
hbase shell
```



**2.命名空间**

```
 alter_namespace 修改命名空间
 create_namespace 创建命名空间
 describe_namespace 查看命名空间
 drop_namespace 删除命名空间
 list_namespace 查看所有命名空间
 list_namespace_tables 列出命名空间下的所有表
```

**2.1 创建namespace**

```
create_namespace 'sc'
```

**2.2 列出所有namespace **

```
list_namespace
```

**2.3 查看namespace**

```
describe_namespace 'sc'
```

**2.4 在namespace下创建表**

```
create 'sc:testtable', 'fm1'
```

**2.5 查看namespace下的表**

```
list_namespace_tables 'sc'
```

**2.6 删除namespace** 

必须空的namespace才能删除  要删除掉里面的表

```
disable 'sc:testtable'
drop 'sc:testtable'
drop_namespace 'sc' 
```



**3.表的DML操作**

**3.1 创建表** 

```
create 'tbl','f1','f2','f3'
```

tbl是表名，f1，f2，f3是列族名

或者指定详细的版本信息

```
create 'tbl',{NAME => 'f1', VERSIONS => '3'} ,{NAME => 'f2', VERSIONS => '3'}
```



**3.2.查看表的结构**

```
desc 'sc:tb1'
describe 'tbl'
```

**3.3.禁用，启用表**

```
disable 'tbl'
enable 'tbl'
```

**3.4.查看表结构是否启用** 

```
is_enabled 'tbl'
```

【允许修改】 因为启用的表不允许修改：

**3.5.增加一个列族**

```
disable 'tbl'
alter 'tbl', NAME=>'f1', VERSIONS=>3
enable 'tbl'
```

**3.6.删除某个列族**

```
disable 'tbl'
alter 'tbl', NAME=>'f1', METHOD=>'delete'         #--------注意大小写（简写：alter 'tbl', 'delete'=>'f1'）
enable 'tbl'
```

**3.7.查看所有的表**

```
【查看sc下命名空间的所有表，后面sc冒号点星 是所有表的意思】
list 'sc:.*'
    sc:tb1  【sc的命名空间下有tb1表】
【查看所有命名空间的表】
list
```

**3.8.查看某一表是否存在**

```
exists 'tbl'
```

**3.9.清空表**

```
truncate 'tbl'
```

**3.10.删除某张表**

```
disable 'tbl'
drop 'tbl'
```



**4.表的DDL操作**

**4.1.put操作**

帮助命令 **help 'put'**

```
put 'tbl', 'r1', 'f1:c1', 'value'    

【下面还有帮助命令中的一些例子】
 hbase> put 'ns1:t1', 'r1', 'c1', 'value'
 hbase> put 't1', 'r1', 'c1', 'value'
 hbase> put 't1', 'r1', 'c1', 'value', ts1
 hbase> put 't1', 'r1', 'c1', 'value', {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> put 't1', 'r1', 'c1', 'value', ts1, {ATTRIBUTES=>{'mykey'=>'myvalue'}}
 hbase> put 't1', 'r1', 'c1', 'value', ts1, {VISIBILITY=>'PRIVATE|SECRET'}
```

>   put 表名,rowkey,列簇,值

列族的列可以不存在，修改数据也是put，只需行健**[rowkey]**和列相同即可

**4.2.get操作**

帮助命令 **help 'get'**

```
【获取rowkey为r1的数据】
 get 'sc:tbl', 'r1'
【获取rowkey为r1的f1列族下的所有列数据】
 get 'sc:tbl', 'r1','f1' 
 get 'sc:tbl', 'r1', {COLUMN => 'f1'}
【获取rowkey为r1两个列族，f1，f2列族下的所有列数据】
 get 'sc:tbl', 'r1','f1','f2'
 get 'sc:tbl', 'r1', ['f1', 'f2']
 get 'sc:tbl', 'r1', {COLUMN => ['f1', 'f2']}
【获取rowkey为r1的f1列族下name列数据】
 get 'sc:tbl', 'r1','f1:name'
【获取rowkey为r1的f1列族下name列数据，f1列族下age列数据】
 get 'sc:tbl', 'r1','f1:name','f1:age'

【下面还有帮助命令中的一些例子】
  hbase> get 't1', 'r1', {TIMERANGE => [ts1, ts2]}
  hbase> get 't1', 'r1', {COLUMN => 'c1', TIMESTAMP => ts1}
  hbase> get 't1', 'r1', {COLUMN => 'c1', TIMERANGE => [ts1, ts2], VERSIONS => 4}
  hbase> get 't1', 'r1', {COLUMN => 'c1', TIMESTAMP => ts1, VERSIONS => 4}
  hbase> get 't1', 'r1', {FILTER => "ValueFilter(=, 'binary:abc')"}
  hbase> get 't1', 'r1', {COLUMN => 'c1', ATTRIBUTES => {'mykey'=>'myvalue'}}
  hbase> get 't1', 'r1', {COLUMN => 'c1', AUTHORIZATIONS => ['PRIVATE','SECRET']}
  hbase> get 't1', 'r1', {CONSISTENCY => 'TIMELINE'}
  hbase> get 't1', 'r1', {CONSISTENCY => 'TIMELINE', REGION_REPLICA_ID => 1}

```

**4.3.scan操作**

帮助命令 **help 'scan'**

```
【获取tb1表的所有行数据】
 scan 'tbl'
【获取tb1表的前3行数据】
 scan 'tbl', {LIMIT=>3}
【分页查询 获取tbl表的从指定位置开始的行】
 scan 'tbl', {STARTROW=>'rowKey', LIMIT=>3}
【获取tb1表的指定列f1:name的所有行数据】
 scan 'tb1', {COLUMNS =>'f1:name'}
 
【下面还有帮助命令中的一些例子】 
  hbase> scan 'hbase:meta'
  hbase> scan 'hbase:meta', {COLUMNS => 'info:regioninfo'}
  hbase> scan 'ns1:t1', {COLUMNS => ['c1', 'c2'], LIMIT => 10, STARTROW => 'xyz'}
  hbase> scan 't1', {COLUMNS => ['c1', 'c2'], LIMIT => 10, STARTROW => 'xyz'}
  hbase> scan 't1', {COLUMNS => 'c1', TIMERANGE => [1303668804000, 1303668904000]}
  hbase> scan 't1', {REVERSED => true}
  hbase> scan 't1', {ALL_METRICS => true}
  hbase> scan 't1', {METRICS => ['RPC_RETRIES', 'ROWS_FILTERED']}
  hbase> scan 't1', {ROWPREFIXFILTER => 'row2', FILTER => "(QualifierFilter (>=, 'binary:xyz')) AND (TimestampsFilter ( 123, 456))"}
  hbase> scan 't1', {FILTER =>org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)}
  hbase> scan 't1', {CONSISTENCY => 'TIMELINE'}
  hbase> scan 't1', { COLUMNS => ['c1', 'c2'], ATTRIBUTES => {'mykey' => 'myvalue'}}
  hbase> scan 't1', { COLUMNS => ['c1', 'c2'], AUTHORIZATIONS => ['PRIVATE','SECRET']}
  hbase> scan 't1', {COLUMNS => ['c1', 'c2'], CACHE_BLOCKS => false}
  hbase> scan 't1', {RAW => true, VERSIONS => 10}
  hbase> scan 't1', {COLUMNS => ['cf:qualifier1:toInt','cf:qualifier2:c(org.apache.hadoop.hbase.util.Bytes).toInt'] }
  hbase> scan 't1', {FORMATTER => 'toString'}
  hbase> scan 't1', {FORMATTER_CLASS => 'org.apache.hadoop.hbase.util.Bytes', FORMATTER => 'toString'}

```

**4.4删除数据**

帮助命令 **help 'delete'**

```
【删除tbl表r1行数据的f1:name列[值]】
 删除tbl表，行健为r1的c1列中，时间戳为ts1的值，如果不指定ts1就删除所有列值[默认保持三个版本]
 delete 'sc:tb1', 'r1', 'fm1:name'
【删除tbl表r1行数据】
 deleteall 'sc:tbl', 'r1'
```

**4.5.统计表的行数**

```
count 'tbl'
```





##  五.hbase shell的filter操作

**1.创建表**
```
create 'test1', 'lf', 'sf'
```

**2.导入数据**
```
put 'test1', 'user1|ts1', 'sf:c1', 'sku1'
put 'test1', 'user1|ts2', 'sf:c1', 'sku188'
put 'test1', 'user1|ts3', 'sf:s1', 'sku123'
put 'test1', 'user2|ts4', 'sf:c1', 'sku2'
put 'test1', 'user2|ts5', 'sf:c2', 'sku288'
put 'test1', 'user2|ts6', 'sf:s1', 'sku222'
```

**3.查询案例：谁的值=sku188**
```
scan 'test1', FILTER=>"ValueFilter(=,'binary:sku188')"
```
**#查询结果
```
ROW                         COLUMN+CELL                    
user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
```

**4.查询案例：谁的值包含88**
```
scan 'test1', FILTER=>"ValueFilter(=,'substring:88')"
```
**#查询结果
```
 ROW                         COLUMN+CELL    
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
 user2|ts5                   column=sf:c2, timestamp=1409122355030, value=sku288
```

**5.查询案例：谁的值包含88**
```
scan 'test1', FILTER=>"ValueFilter(=,'substring:88')"
```
**#查询结果
```
 ROW                         COLUMN+CELL    
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
 user2|ts5                   column=sf:c2, timestamp=1409122355030, value=sku288
```

**6.通过广告点击进来的(column为c2)值包含88的用户**
```
scan 'test1', FILTER=>"ColumnPrefixFilter('c2') AND ValueFilter(=,'substring:88')"
```
**#查询结果
```
 ROW                         COLUMN+CELL
 user2|ts5                   column=sf:c2, timestamp=1409122355030, value=sku288
```

**7.通过搜索进来的(column为s)值包含123或者222的用户**
```
scan 'test1', FILTER=>"ColumnPrefixFilter('s') AND ( ValueFilter(=,'substring:123') OR ValueFilter(=,'substring:222') )"
```
**#查询结果
```
 ROW                         COLUMN+CELL
 user1|ts3                   column=sf:s1, timestamp=1409122354954, value=sku123
 user2|ts6                   column=sf:s1, timestamp=1409122355970, value=sku222
```

**8.rowkey为user1开头的**
```
scan 'test1', FILTER => "PrefixFilter ('user1')"
```
**#查询结果
```
 ROW                        COLUMN+CELL
 user1|ts1                   column=sf:c1, timestamp=1409122354868, value=sku1
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
 user1|ts3                   column=sf:s1, timestamp=1409122354954, value=sku123
```
**9.从user1|ts2开始,找到所有的rowkey以user1开头的**
```
scan 'test1', {STARTROW=>'user1|ts2', FILTER => "PrefixFilter ('user1')"}
```
**#查询结果
```
 ROW                         COLUMN+CELL
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
 user1|ts3                   column=sf:s1, timestamp=1409122354954, value=sku123 
```
**10.从user1|ts2开始,找到所有的到rowkey以user2开头**
```
scan 'test1', {STARTROW=>'user1|ts2', STOPROW=>'user2'}
```
**#查询结果
```
 ROW                          COLUMN+CELL
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
 user1|ts3                   column=sf:s1, timestamp=1409122354954, value=sku123
```
**11.查询rowkey里面包含ts3的**
```
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.SubstringComparator
import org.apache.hadoop.hbase.filter.RowFilter
scan 'test1', {FILTER => RowFilter.new(CompareFilter::CompareOp.valueOf('EQUAL'), SubstringComparator.new('ts3'))}
```
**#查询结果
```
 ROW                          COLUMN+CELL
 user1|ts3                   column=sf:s1, timestamp=1409122354954, value=sku123 
```
**12.查询rowkey里面包含ts的**
```
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.SubstringComparator
import org.apache.hadoop.hbase.filter.RowFilter
scan 'test1', {FILTER => RowFilter.new(CompareFilter::CompareOp.valueOf('EQUAL'), SubstringComparator.new('ts'))}
```
**#查询结果
```
 ROW                         COLUMN+CELL
 user1|ts1                   column=sf:c1, timestamp=1409122354868, value=sku1
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
 user1|ts3                   column=sf:s1, timestamp=1409122354954, value=sku123
 user2|ts4                   column=sf:c1, timestamp=1409122354998, value=sku2
 user2|ts5                   column=sf:c2, timestamp=1409122355030, value=sku288
 user2|ts6                   column=sf:s1, timestamp=1409122355970, value=sku222
```
**13.加入一条测试数据**
```
put 'test1', 'user2|err', 'sf:s1', 'sku999'
```


**14.查询rowkey里面以user开头的，新加入的测试数据并不符合正则表达式的规则，故查询不出来**
```
import org.apache.hadoop.hbase.filter.RegexStringComparator
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.SubstringComparator
import org.apache.hadoop.hbase.filter.RowFilter
scan 'test1', {FILTER => RowFilter.new(CompareFilter::CompareOp.valueOf('EQUAL'),RegexStringComparator.new('^user\d+\|ts\d+$'))}
```
**#查询结果
```
 ROW                         COLUMN+CELL
 user1|ts1                   column=sf:c1, timestamp=1409122354868, value=sku1
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=sku188
 user1|ts3                   column=sf:s1, timestamp=1409122354954, value=sku123
 user2|ts4                   column=sf:c1, timestamp=1409122354998, value=sku2
 user2|ts5                   column=sf:c2, timestamp=1409122355030, value=sku288
 user2|ts6                   column=sf:s1, timestamp=1409122355970, value=sku222
```
**15.加入测试数据**
```
put 'test1', 'user1|ts9', 'sf:b1', 'sku1'
```
**16.b1开头的列中并且值为sku1的**
```
scan 'test1', FILTER=>"ColumnPrefixFilter('b1') AND ValueFilter(=,'binary:sku1')"
```
**#查询结果
```
 ROW                          COLUMN+CELL                                                   user1|ts9                   column=sf:b1, timestamp=1409124908668, value=sku1
```

**17.SingleColumnValueFilter的使用，b1开头的列中并且值为sku1的**
```
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.filter.SingleColumnValueFilter
import org.apache.hadoop.hbase.filter.SubstringComparator
scan 'test1', {COLUMNS => 'sf:b1', FILTER => SingleColumnValueFilter.new(Bytes.toBytes('sf'), Bytes.toBytes('b1'), CompareFilter::CompareOp.valueOf('EQUAL'), Bytes.toBytes('sku1'))}
```
**#查询结果
```
 ROW                         COLUMN+CELL
 user1|ts9                   column=sf:b1, timestamp=1409124908668, value=sku1
```
**18.KeyOnlyFilter: 只要key,不要value**
```
scan 'test1', FILTER=>"FirstKeyOnlyFilter() AND ValueFilter(=,'binary:sku188') AND KeyOnlyFilter()"
```
**#查询结果
```
 ROW                         COLUMN+CELL
 user1|ts2                   column=sf:c1, timestamp=1409122354918, value=
```
FirstKeyOnlyFilter: 一个rowkey可以有多个version,同一个rowkey的同一个column也会有多个的值, 只拿出key中的第一个column的第一个version

##  六.hbase的java操作 maven构建

**1.pom.xml文件添加hbase依赖**
```
<properties>
    <hbase.version>2.0.2</hbase.version>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-server</artifactId>
        <version>${hbase.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-client</artifactId>
        <version>${hbase.version}</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-common</artifactId>
        <version>${hbase.version}</version>
    </dependency>
</dependencies>
```

**2.初始化静态方法**
```
static {
    Configuration configuration = HBaseConfiguration.create();
    configuration.set("hbase.zookeeper.quorum","zookeeper01:2181,zookeeper02:2181");
    try {
        connection = ConnectionFactory.createConnection(configuration);
        admin = connection.getAdmin();
    } catch (Exception e) {
        e.printStackTrace();
    }
}
```
**3.判断表是否存在**
```
public static boolean isExist(String tableName) throws Exception {
    return admin.tableExists(TableName.valueOf(tableName));
}
```
**4.创建表**
```
public static void createTable(String tableName,String ... column) throws Exception {
    if(isExist(tableName)){
        System.out.println(tableName+"已经存在");
        return;
    }
    HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName));
    for (String c : column) {
        HColumnDescriptor hColumnDescriptor = new HColumnDescriptor(c);
        tableDescriptor.addFamily(hColumnDescriptor);
    }
    admin.createTable(tableDescriptor);
    System.out.println(tableName+"创建成功");
    getAllTable();
}
```
**5.删除表**
```
public static void deleteTable(String tableName) throws Exception {
    if(isExist(tableName)){
        admin.disableTable(TableName.valueOf(tableName));
        admin.deleteTable(TableName.valueOf(tableName));
    }
    System.out.println(tableName+"已被删除");
    getAllTable();
 }
```

**6.获取所有表**
```
public static void getAllTable() throws Exception {
    TableName[] tableNames = admin.listTableNames();
    for (TableName tableName : tableNames) {
        System.out.println(Bytes.toString(tableName.getName()));
    }
}
```
**7.添加一行数据**
```
public static void add1Row(String tableName,String rowkey,String cf,String column,String val) throws Exception {
    Table table = connection.getTable(TableName.valueOf(tableName));
    Put put = new Put(Bytes.toBytes(rowkey));
    put.addColumn(Bytes.toBytes(cf),Bytes.toBytes(column),Bytes.toBytes(val));
    table.put(put);
}
```
**8.删除一行数据**
```
public static void del1Row(String tableName,String rowkey) throws Exception {
    Table table = connection.getTable(TableName.valueOf(tableName));
    Delete delete = new Delete(Bytes.toBytes(rowkey));
    table.delete(delete);
}
```
**9.删除多行数据**
```
public static void delMulRow(String tableName,String... rowkeys) throws Exception {
    Table table = connection.getTable(TableName.valueOf(tableName));
    List<Delete> deletes = new ArrayList<Delete>();
    for (String rowkey : rowkeys) {
        Delete delete = new Delete(Bytes.toBytes(rowkey));
        deletes.add(delete);
    }
    table.delete(deletes);
}
```
**10.获取多行数据**
```
public static void getAllrows(String tableName) throws Exception {
    Table table = connection.getTable(TableName.valueOf(tableName));
    Scan scan = new Scan();
    scan.setMaxVersions();
    ResultScanner resultScanner = table.getScanner(scan);
    for (Result result : resultScanner) {
        Cell[] cells = result.rawCells();
        for (Cell cell : cells) {
            System.out.print(Bytes.toString(CellUtil.cloneFamily(cell))+" ");
            System.out.print(Bytes.toString(CellUtil.cloneQualifier(cell))+" ");
            System.out.print(Bytes.toString(CellUtil.cloneRow(cell))+" ");
            System.out.println(Bytes.toString(CellUtil.cloneValue(cell)));
        }
    }
}
```

**11.获取一行数据**
```
public static void getrow(String tableName,String rowkey) throws Exception {
    Table table = connection.getTable(TableName.valueOf(tableName));
    Get get = new Get(Bytes.toBytes(rowkey));
    Result result = table.get(get);
    Cell[] cells = result.rawCells();
    for (Cell cell : cells) {
        System.out.print(Bytes.toString(CellUtil.cloneFamily(cell))+" ");
        System.out.print(Bytes.toString(CellUtil.cloneQualifier(cell))+" ");
        System.out.print(Bytes.toString(CellUtil.cloneRow(cell))+" ");
        System.out.println(Bytes.toString(CellUtil.cloneValue(cell)));
    }
}
```

**hbase的demo**

```
package com.chen;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class App {
    static Admin admin = null;
    static Connection connection = null;

    /**
     * 静态初始化
     */
    static {
        Configuration configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum","zookeeper01:2181,zookeeper02:2181");
        try {
            connection = ConnectionFactory.createConnection(configuration);
            admin = connection.getAdmin();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * 判断表是否存在
     * @param tableName
     * @return
     * @throws Exception
     */
    public static boolean isExist(String tableName) throws Exception {
        return admin.tableExists(TableName.valueOf(tableName));
    }

    /**
     * 创建表
     * @param tableName
     * @param column
     * @throws Exception
     */
    public static void createTable(String tableName,String ... column) throws Exception {
        if(isExist(tableName)){
            System.out.println(tableName+"已经存在");
            return;
        }
        HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName));
        for (String c : column) {
            HColumnDescriptor hColumnDescriptor = new HColumnDescriptor(c);
            tableDescriptor.addFamily(hColumnDescriptor);
        }
        admin.createTable(tableDescriptor);
        System.out.println(tableName+"创建成功");
        getAllTable();
    }

    /**
     * 删除表
     * @param tableName
     * @throws Exception
     */
    public static void deleteTable(String tableName) throws Exception {
        if(isExist(tableName)){
            admin.disableTable(TableName.valueOf(tableName));
            admin.deleteTable(TableName.valueOf(tableName));
        }
        System.out.println(tableName+"已被删除");
        getAllTable();
    }

    /**
     * 获取所有表
     * @throws Exception
     */
    public static void getAllTable() throws Exception {
        TableName[] tableNames = admin.listTableNames();
        for (TableName tableName : tableNames) {
            System.out.println(Bytes.toString(tableName.getName()));
        }
    }


    /**
     * 添加一行数据
     * @param tableName
     * @param rowkey
     * @param cf
     * @param column
     * @param val
     * @throws Exception
     */
    public static void add1Row(String tableName,String rowkey,String cf,String column,String val) throws Exception {
        Table table = connection.getTable(TableName.valueOf(tableName));
        Put put = new Put(Bytes.toBytes(rowkey));
        put.addColumn(Bytes.toBytes(cf),Bytes.toBytes(column),Bytes.toBytes(val));
        table.put(put);
    }


    /**
     * 删除一行数据
     * @param tableName
     * @param rowkey
     * @throws Exception
     */
    public static void del1Row(String tableName,String rowkey) throws Exception {
        Table table = connection.getTable(TableName.valueOf(tableName));
        Delete delete = new Delete(Bytes.toBytes(rowkey));
        table.delete(delete);
    }

    /**
     * 删除多行数据
     * @param tableName
     * @param rowkeys
     * @throws Exception
     */
    public static void delMulRow(String tableName,String... rowkeys) throws Exception {
        Table table = connection.getTable(TableName.valueOf(tableName));
        List&lt;Delete&gt; deletes = new ArrayList&lt;Delete&gt;();
        for (String rowkey : rowkeys) {
            Delete delete = new Delete(Bytes.toBytes(rowkey));
            deletes.add(delete);
        }
        table.delete(deletes);
    }

    /**
     * 获取多行数据
     * @param tableName
     * @throws Exception
     */
    public static void getAllrows(String tableName) throws Exception {
        Table table = connection.getTable(TableName.valueOf(tableName));
        Scan scan = new Scan();
        scan.setMaxVersions();
        ResultScanner resultScanner = table.getScanner(scan);
        for (Result result : resultScanner) {
            Cell[] cells = result.rawCells();
            for (Cell cell : cells) {
                System.out.print(Bytes.toString(CellUtil.cloneFamily(cell))+" ");
                System.out.print(Bytes.toString(CellUtil.cloneQualifier(cell))+" ");
                System.out.print(Bytes.toString(CellUtil.cloneRow(cell))+" ");
                System.out.println(Bytes.toString(CellUtil.cloneValue(cell)));
            }
        }
    }

    public static void main(String[] args) throws Exception {
        /*String table="t3";
        System.out.println(isExist(table));*/
        //createTable("test1","info","extra");
        //deleteTable("test1");
        add1Row("test1","r2","extra","age","11");
        //del1Row("test1","r1");
        getAllrows("test1");
    }
}
```

## 七.spring-boot-starter-hbase整合

**1.准备hbase表环境**

```shell
【进入hbase shell 创建表】
 create 'person','info','other'
 
【往表中添加rk001记录】
 put 'person','rk001','info:name','chenjinhua'
 put 'person','rk001','info:age','22'
 put 'person','rk001','info:sex','男'
 put 'person','rk001','other:school','beijing'
 
【往表中添加rk002记录】
 put 'person','rk002','info:name','lilei'
 put 'person','rk002','info:age','22'
 put 'person','rk002','info:sex','女'
 put 'person','rk002','other:school','哈佛'

【扫描表记录】
  hbase(main):145:0> scan 'person'
  ROW            COLUMN+CELL                                                                       rk001          column=info:age, timestamp=1569515339893, value=22                               rk001          column=info:name, timestamp=1569515327384, value=chenjinhua                       rk001          column=info:sex, timestamp=1569515350544, value=\xE7\x94\xB7                     rk001          column=other:school, timestamp=1569515438784, value=beijing                       rk002          column=info:age, timestamp=1569515540721, value=22                               rk002          column=info:name, timestamp=1569515540703, value=lilei                           rk002          column=info:sex, timestamp=1569515540740, value=\xE5\xA5\xB3                     rk002          column=other:school, timestamp=1569515541858, value=\xE5\x93\x88\xE4\xBD\x9B   
```

**2.引入maven依赖文件**

```java
<dependency>
    <groupId>com.spring4all</groupId>
    <artifactId>spring-boot-starter-hbase</artifactId>
    <version>1.0.0.RELEASE</version>
</dependency>
<dependency>
	<groupId>org.projectlombok</groupId>
	<artifactId>lombok</artifactId>
</dependency>
```

**3.application.properties 文件中加入对应的配置项目**

```
## HBase 配置
spring.data.hbase.quorum=192.168.232.140:2181
spring.data.hbase.rootDir=file:///soft/hbase-2.0.2/data
spring.data.hbase.nodeParent=Hbase
```

具体配置项信息如下：

spring.data.hbase.quorum 指定 HBase 的 zk 地址
spring.data.hbase.rootDir 指定 HBase 在 HDFS 上存储的路径
spring.data.hbase.nodeParent 指定 ZK 中 HBase 的根 ZNode






# flume

## 一.flume的搭建

1.将在qq939598604的百度网盘中的路径，我的软件/apache-flume-1.9.0-bin.tar.gz下载并上传到/soft目录下
2.解压
```
cd /soft
tar –zxvf apache-flume-1.9.0-bin.tar.gz -C /soft/flume-1.9.0
```
3.配置java_home
cp flume-env.sh.template flume-env.sh
vim flume-env.sh
```
export JAVA_HOME=/soft/jdk1.8.0_161
```
4.一个简单的示例，编辑配置文件
vim  test.conf 

```
# 定义一个a1的agent，并指定a1的sources、sinks、channels名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 配置sink类型
a1.sinks.k1.type = logger

# 用内存channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 绑定source、sink、channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```
这个 agent 的名称是 a1。其中该 agent 的 source 监听 44444 端口。channel 采用内存模式，而 slink 直接输出数据到 控制台上（logger）。 

5.启动flume

```
/soft/flume-1.9.0/bin/flume-ng agent \
-c /soft/flume-1.9.0/conf \
-f /soft/flume-1.9.0/conf/test.conf \
-n a1 \
-Dflume.root.logger=INFO,console
```
-c 指定flume的配置目录

-f 指定自定义的配置文件

-n 指定配置文件中的名称

-Dflume.root.logger=INFO,console 打印信息到控制台中

6.安装telnet工具

```
yum install telnet -y
telnet localhost 4444
```
7.在telnet终端发送数据到flume中并查看是否有接收到



## 二  tail方式采集 文件到hdfs

1.编辑flume的配置agent

vim tail2Hdfs.conf

```
#定义一个agent组件
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source
## exec表示flume回去调用给的命令，然后从给的命令的结果中去拿数据
a1.sources.r1.type = exec
## 使用tail这个命令来读数据
a1.sources.r1.command = tail -F /var/log/mysqld.log
a1.sources.r1.channels = c1

# 配置sink hdfs接收数据
a1.sinks.k1.type = hdfs
## sinks.k1只能连接一个channel，source可以配置多个
a1.sinks.k1.channel = c1
## 下面的配置告诉用hdfs去写文件的时候写到什么位置，下面的表示不是写死的，而是可以动态的变化的。表示输出的目录名称是可变的
a1.sinks.k1.hdfs.path =hdfs://192.168.197.21:9000/flume/%y-%m-%d/%H%M/
##表示最后的文件的前缀
a1.sinks.k1.hdfs.filePrefix = events-
## 表示到了需要触发的时间时，是否要更新文件夹，true:表示要
a1.sinks.k1.hdfs.round = true
## 表示每隔1分钟改变一次
a1.sinks.k1.hdfs.roundValue = 1
## 切换文件的时候的时间单位是分钟
a1.sinks.k1.hdfs.roundUnit = minute
## 表示只要过了3秒钟，就切换生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 3
## 如果记录的文件大于20字节时切换一次
a1.sinks.k1.hdfs.rollSize = 20
## 当写了5个事件时触发
a1.sinks.k1.hdfs.rollCount = 5
## 收到了多少条消息往dfs中追加内容
a1.sinks.k1.hdfs.batchSize = 10
## 使用本地时间戳 不加会报错"Expected timestamp in the Flume event headers, but it was null"
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件类型，默认是Sequencefile，可用DataStream：为普通文本
a1.sinks.k1.hdfs.fileType = DataStream

# 配置channel 使用内存的方式
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 绑定通道
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

2.启动agent

```
/soft/flume-1.9.0/bin/flume-ng agent -c /soft/flume-1.9.0/conf -f /soft/flume-1.9.0/conf/tail2Hdfs.conf -n a1 -Dflume.root.logger=INFO,console
```

启动报错总结

**问题1: java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType**

```
[ERROR -org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:150)] Failed to start agent because dependencies were not found in classpath. Error follows.
java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType
	at org.apache.flume.sink.hdfs.HDFSEventSink.configure(HDFSEventSink.java:246)
	at org.apache.flume.conf.Configurables.configure(Configurables.java:41)
	at org.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:453)
	at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:106)
	at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.SequenceFile$CompressionType
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 12 more
```

解决方法：

```
copied comon jar files from hadoop folder to the flume folder.
cp /hadoop/share/hadoop/common/*.jar /root/flume/lib
cp /hadoop/share/hadoop/common/lib/*.jar /root/flume/lib
```

**问题2： Expected timestamp in the Flume event headers, but it was null**

```
java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was null
```

解决方法：

```
a1.sinks.k1.hdfs.useLocalTimeStamp = true
```

 **问题3：java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration** 

```
(SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:447)] process failed
java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<init>(DefaultMetricsSystem.java:38)
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<clinit>(DefaultMetricsSystem.java:36)
    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:106)
    at org.apache.hadoop.security.UserGroupInformation.<clinit>(UserGroupInformation.java:208)
    at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2554)
    at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2546)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2412)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
    at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:240)
    at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:232)
    at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:668)
    at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
    at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:665)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 18 more
```

解决方法： 
缺少的依赖在commons-configuration-1.6.jar包里，这个包在${HADOOP_HOME}share/hadoop/common/lib/下，将其拷贝到flume的lib目录下。

```
cp ${HADOOP_HOME}share/hadoop/common/lib/commons-configuration-1.6.jar ${FLUME_HOME}/lib/

```

**问题4：No FileSystem for scheme: hdfs**

```
 [WARN - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:454)] HDFS IO error
java.io.IOException: No FileSystem for scheme: hdfs
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2658)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2665)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2701)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2683)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:372)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
```

解决方法：原因是缺少依赖：hadoop-hdfs-2.4.0.jar

```
cp ${HADOOP_HOME}share/hadoop/hdfs/hadoop-hdfs-2.4.0.jar ${FLUME_HOME}/lib/
```

**问题5：java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName**

```
2016-11-03 16:41:54,629 (SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:447)] process failed
java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName
```

解决方法： 
缺少hadoop-auth-2.4.0.jar依赖，同样将其拷贝到flume的lib目录下：

```
cp ${HADOOP_HOME}share/hadoop/common/lib/hadoop-auth-2.4.0.jar ${FLUME_HOME}/lib/
```



# zookpeer

##  一.Zookeeper集群搭建

（一）先把Java环境配好，三台机器，配置Zookeeper 
（二）下载zookeeper-3.4.10.tar.gz，并且上传到/soft/目录下，直接解压zookeeper-3.4.10.tar.gz
 tar -zxvf zookeeper-3.4.10.tar.gz
（三）修改配置文件名称

```
mv /soft/zookeeper-3.4.10/conf/zoo_simple.cfg /soft/zookeeper-3.4.10/conf/zoo.cfg
```

（四）编辑配置文件

```
vim /soft/zookeeper-3.4.10/conf/zoo.cfg
```

修改**dataDir=/soft/zookeeper-3.4.10/data**
同时增加

```
server.1=192.168.197.231:2888:3888
server.2=192.168.197.232:2888:3888
server.3=192.168.197.233:2888:3888
```

**server.X=A:B:C**  X-代表服务器编号 A-代表ip  B和C-代表端口，这个端口用来系统之间通信

（五）编辑配置myid文件

vim /soft/zookeeper-3.4.10/data/myid
之后会产生一个新文件，直接在里面写X即可，比如我配置的三个server，当前服务器的ip是多少，myid里面写的X就是server.X=ip:2888:3888 中ip所对应的X

```
server.1=192.168.197.231:2888:3888【192.168.197.231服务器上面的myid填写1】
server.2=192.168.197.232:2888:3888【192.168.197.232服务器上面的myid填写2】
server.3=192.168.197.233:2888:3888【192.168.197.233服务器上面的myid填写3】
```

（六）修改环境

​	vim /etc/profile
　　在export PATH语句前添加两行：

```
ZOOKEEPER=/soft/zookeeper-3.4.10
PATH=PATH:ZOOKEEPER/bin
```

（六）并执行 source /etc/profile
（七）启动zookeeper
分别在3台机器启动 zookeeper

## 二.ZooKeeper客户端

```
./zkCli.sh  -server 192.9.200.242:2181
```



# kafka

##  一.认识

**首先几个概念：**

1. **kafka作为一个集群运**行在一个或多个服务器上。
2. kafka集群存储的消息是以topic为类别记录的。
3. 每个消息（也叫记录record，我习惯叫消息）是由一个key，一个value和时间戳构成。
4. kafka是基于点对点的拉取（pull）模式（看到尚硅谷视频讲到，便记下来）

**kafka有四个核心API：**

- 应用程序使用 `Producer API` 发布消息到1个或多个topic（主题）。

- 应用程序使用 `Consumer API` 来订阅一个或多个topic，并处理产生的消息。

- 应用程序使用 `Streams API` 充当一个流处理器，从1个或多个topic消费输入流，并生产一个输出流到1个或多个输出topic，有效地将输入流转换到输出流。

- `Connector API`允许构建或运行可重复使用的生产者或消费者，将topic连接到现有的应用程序或数据系统。例如，一个关系数据库的连接器可捕获每一个变化。

  ![1565660315040](C:\Users\Administrator\Desktop\Md笔记\pic\product.png)

 

**基本术语：**

**Topic**

Kafka将消息种子(Feed)分门别类，每一类的消息称之为一个主题(Topic).

- Topic是Kafka数据写入操作的基本单元，可以指定副本
- 一个Topic包含一个或多个Partition，建Topic时可手动指定Partition个数，个数必须少于服务器个数
- 每条消息属于且仅属于一个Topic
- Producer发布数据时，必须指定将该消息发布到哪个Topic
- Consumer订阅消息时，也必须指定订阅哪个Topic的信息

 **Partition**

- 每个Partition只会在一个Broker上，物理上每个Partition对应的是一个文件夹
- Kafka默认使用的是hash进行分区，所以会出现不同的分区数据不一样的情况，但是partitioner是可以override的
- Partition包含多个Segment，每个Segment对应一个文件，Segment可以手动指定大小，当Segment达到阈值时，将不再写数据，每个Segment都是大小相同的
- Segment由多个不可变的记录组成，记录只会被append到Segment中，不会被单独删除或者修改，每个Segment中的Message数量不一定相等

 **Producer**

发布消息的对象称之为主题生产者(Kafka topic producer)，写数据只会找leader

**Consumer**

订阅消息并处理发布的消息的对象称之为主题消费者(consumers)

 **Broker**

已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个主题（topic），并从Broker拉数据，从而消费这些已发布的消息。

 **主题**

 Topic是发布的消息的类别或者种子Feed名。对于每一个Topic，Kafka集群维护这一个分区的log，就像下图中的示例： 

![1565660685648](C:\Users\Administrator\Desktop\Md笔记\pic\1565660685648.png)

 每一个分区都是一个顺序的、不可变的消息队列， 并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的。 

Kafka集群保持所有的消息，直到它们过期， 无论消息是否被消费了。 实际上消费者所持有的仅有的元数据就是这个偏移量，也就是消费者在这个log中的位置。 这个偏移量由消费者控制：正常情况当消费者消费消息的时候，偏移量也线性的的增加。但是实际偏移量由消费者控制，消费者可以将偏移量重置为更老的一个偏移量，重新读取消息。 可以看到这种设计对消费者来说操作自如， 一个消费者的操作不会影响其它消费者对此log的处理。 再说说分区。Kafka中采用分区的设计有几个目的。一是可以处理更多的消息，不受单台服务器的限制。Topic拥有多个分区意味着它可以不受限的处理更多的数据。第二，分区可以作为并行处理的单元，稍后会谈到这一点。

kafka存储数据有2个地方：broker里面的patition下的log文件，broker的存储策略是文件的大小和存储时间，zookpeer里面的元数据



##  二.window安装kafka

**（一）zookeeper在Windows下的安装**

1.把下载的zookeeper的文件解压到指定目录：D:\zookeeper-3.4.14

2.复制conf目录下的zoo_sample.cfg文件为zoo.cfg

3.修改内容如下：

```
dataDir=D:\data\zookeeper
server.1=192.168.197.231:2888:3888
```

4.进入到bin目录，双击启动zkServer.cmd

5.启动后jps可以看到QuorumPeerMain的进程

```
D:\zookeeper-3.4.14\bin >jps
```

6.启动客户端运行查看一下

```
D:\zookeeper-3.4.14\bin>zkCli.cmd -server 127.0.0.1:2181
```

**（二）kafka在Windows下的安装**

1.下载kafka

下载kafka，必须scala的版本对应kafka_2.11-2.3.0，其中前面的是2.12是scala的 版本号

https://www.apache.org/dyn/closer.cgi?path=/kafka/2.3.0/kafka_2.11-2.3.0.tgz

2.解压文件（我的目录是D:\kafka_2.11-2.3.0  【这里不要在Program Files等文件名之间有空格的目录下，不然一会执行会不识别路径】）

3.修改D:\kafka_2.11-2.3.0\config下server.properties文件

```
log.dirs=D:\kafka_2.11-2.3.0\kafka-logs
broker.id=0
port=9092
zookeeper.connect=localhost:2181
```

4.进入kafka文件目录D:\kafka_2.11-2.3.0，执行以下命令，启动kafka通讯的服务器broker

```
.\bin\windows\kafka-server-start.bat .\config\server.properties
```

5.进入kafka文件目录D:\kafka_2.11-2.3.0\bin\windows，创建kafka的消息topics

```
kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
```

 6.分别打开两个cmd窗口，进入目录D:\kafka_2.11-2.3.0\bin\windows，创建Producer和Consumer

（1）Producer

进入目录D:\kafka_2.11-2.3.0\bin\windows输入如下命令

```
kafka-console-producer.bat --broker-list localhost:9092 --topic test
```

（2）Consumer

进入目录D:\kafka_2.11-2.3.0\bin\windows输入如下命令

```
kafka-console-consumer.bat --zookeeper localhost:2181 --topic testDemo
```

然后就可以在Producer中发信息，在Consumer中收信息了

**（三）kafka集群在Windows下的安装**

1.复制上面的kafka单机版文件夹

修改文件名为：kafka_2.11-2.3.0--1和 kafka_2.11-2.3.0--2

![1565773682533](C:\Users\Administrator\Desktop\Md笔记\pic\1565773682533.png)

2.修改D:\kafka_2.11-2.3.0--1\config下server.properties文件

```
log.dirs=D:\kafka_2.11-2.3.0--1\kafka-logs
broker.id=1
port=9093
zookeeper.connect=localhost:2181
```

3.修改D:\kafka_2.11-2.3.0--2\config下server.properties文件

```
log.dirs=D:\kafka_2.11-2.3.0--2\kafka-logs
broker.id=2
port=9094
zookeeper.connect=localhost:2181
```

4.分别进入kafka文件目录D:\kafka_2.11-2.3.0和kafka_2.11-2.3.0--1和 kafka_2.11-2.3.0--2三个文件夹，执行以下命令，启动kafka通讯的服务器broker

```
.\bin\windows\kafka-server-start.bat .\config\server.properties
```

  5.查看kafka集群中所有broker节点

broker的配置文件中有zookeeper的地址,也有自己的broker ID 

当broker启动后,会在zookeeper中新建一个znode,访问zk并执行ls /brokers/ids 就可以看到zk中存的所有的broker id list，然后确认你增加的broker的ID是否在list里面 

```
D:\zookeeper-3.4.14\bin>zkCli.cmd -server 127.0.0.1:2181
[zk: 127.0.0.1:2181(CONNECTED) 2] ls /
[cluster, controller_epoch, controller, brokers, zookeeper, admin, isr_change_no
tification, consumers, log_dir_event_notification, latest_producer_id_block, config]
[zk: 127.0.0.1:2181(CONNECTED) 3] ls /brokers
[ids, topics, seqid]
[zk: 127.0.0.1:2181(CONNECTED) 4] ls /brokers/ids
[0, 1, 2]
```

上面可以看到/brokers/ids有[0, 1, 2]，说明是一个集群了

6.windows下kafka集群的批处理启动脚本

```
start d:\windows_install\zookeeper-3.4.14\bin\zkServer.cmd
start d:\windows_install\kafka_2.11-2.3.0\bin\windows\kafka-server-start.bat d:\windows_install\kafka_2.11-2.3.0\config\server.properties
start d:\windows_install\kafka_2.11-2.3.0--1\bin\windows\kafka-server-start.bat d:\windows_install\kafka_2.11-2.3.0--1\config\server.properties
start d:\windows_install\kafka_2.11-2.3.0--2\bin\windows\kafka-server-start.bat d:\windows_install\kafka_2.11-2.3.0--2\config\server.properties
```

##  三.**消费者组案例测试**

**案例一**

生产者：如果topicA只有一个patition，即patition0，启动一个生产者实例

消费者组中有A和B，其中消费者A先启动，A会绑定topicA中的patition0，然后再启动消费者B，启动的时候会提示，没有patition被绑定，则topicA中的生产数据的时候只有消费者A消费。

**案例二**

在前面的kafka集群3台机器中，连接测试192.168.197.30测试，以下测试是在windows的D:\windows_install\kafka_2.11-2.3.0\bin\windows>目录下进行

1.新建一个topic，指定复制因子为3，分区为3

```
kafka-topics.bat --create --zookeeper 192.168.197.30:2181 --replication-factor 3 --partitions 3 --topic test
Created topic test.
```

2.启动一个生产者

```
kafka-console-producer.bat --broker-list 192.168.197.30:9092 --topic test
```

3.启动消费者A，并指定组id为t

```
kafka-console-consumer.bat --bootstrap-server 192.168.197.30:9092 --topic test --group t
```

4.启动消费者B，并指定组id为t

```
kafka-console-consumer.bat --bootstrap-server 192.168.197.30:9092 --topic test --group t
```

5.在生产者端输入消息

```
D:\windows_install\kafka_2.11-2.3.0\bin\windows>kafka-console-producer.bat --broker-list 192.168.197.30:9092 --topic test
>dafads
>sdfa
>fadsads
>dfads
```

6.可以观察到，同个消费者组的消费者，消费消息只能由一个完成，并且只有一次

消费者A获取到的消息如下：

```
D:\windows_install\kafka_2.11-2.3.0\bin\windows>kafka-console-consumer.bat --botstrap-server 192.168.197.30:9092 --topic test --group t
dafads
fadsads
```

消费者B获取到的消息如下：

```
D:\windows_install\kafka_2.11-2.3.0\bin\windows>kafka-console-consumer.bat --bootstrap-server 192.168.197.30:9092 --topic test --group t
sdfa
dfads
```

**同个消费者组的消费者，消费消息只能由一个完成，并且只有一次**



##  四.消费者连接集群只需一个broker

在前面的kafka集群3台机器中，连接测试192.168.197.30测试，以下测试是在windows的D:\windows_install\kafka_2.11-2.3.0\bin\windows>目录下进行

1.新建一个topic，指定复制因子为3，分区为3

```
kafka-topics.bat --create --zookeeper 192.168.197.30:2181 --replication-factor 3 --partitions 3 --topic test
Created topic test.
```

2.启动一个生产者

```
kafka-console-producer.bat --broker-list 192.168.197.30:9092 --topic test
```

3.启动消费者A，连接的bootstrap-server,参数指定为 `--bootstrap-server 192.168.197.30:9092`并指定组id为t

```
kafka-console-consumer.bat --bootstrap-server 192.168.197.30:9092 --topic test --group t
```

4.启动消费者B，连接的bootstrap-server和消费者A不一样,参数指定为 `--bootstrap-server 192.168.197.30:9093`并指定组id为t

```
kafka-console-consumer.bat --bootstrap-server 192.168.197.30:9092 --topic test --group t
```

5.在生产者端输入消息

```
D:\windows_install\kafka_2.11-2.3.0\bin\windows>kafka-console-producer.bat --broker-list 192.168.197.30:9092 --topic test
>ghjhh67
>ljgty
>tyetw
>uioqw
```

6.可以观察到，消费者连接集群只需一个broker，即可获取到整个集群的消息

消费者A获取到的消息如下：

```
D:\windows_install\kafka_2.11-2.3.0\bin\windows>kafka-console-consumer.bat --botstrap-server 192.168.197.30:9092 --topic test --group t
ghjhh67
tyetw
```

消费者B获取到的消息如下：

```
D:\windows_install\kafka_2.11-2.3.0\bin\windows>kafka-console-consumer.bat --bootstrap-server 192.168.197.30:9093 --topic test --group t
ljgty
uioqw
```

**消费者连接集群只需一个broker，即可获取到整个集群的消息**

##  五.生产者的java版本代码编写

**(一)最简单的调用方式**

1.新建一个MyProduce.java，发送消息到test的topic

```java
public class MyProduce {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers","192.168.197.30:9092");
        props.put("acks","all");
        props.put("retries",0);
        props.put("batch.size",16384);
        props.put("linger.ms",1);
        props.put("buffer.memory",33554432);
        props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
        Producer<String, String> producer = new KafkaProducer<>(props);
        for(int i=0;i<10;i++){
          producer.send(new ProducerRecord<>("test", Integer.toString(i), Integer.toString(i)));
        }
        producer.close();
    }
}
```

**(二)有回调的生产者**

```
package com.gzstrong.TestKafka;

import org.apache.kafka.clients.producer.*;

import java.util.Properties;

/**
 * @author 陈锦华
 * @version V1.0
 * @Title:
 * @Description: create by Intellij Idea
 * @date 2019/8/15 0015 上午 9:08
 **/
public class MyProduce {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers","192.168.197.30:9092");
        props.put("acks","all");
        props.put("retries",0);
        props.put("batch.size",16384);
        props.put("linger.ms",1);
        props.put("buffer.memory",33554432);
        props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
        Producer<String, String> producer = new KafkaProducer<>(props);
        for(int i=0;i<10;i++){
            producer.send(new ProducerRecord<>("test", Integer.toString(i), Integer.toString(i)), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                    System.out.println(metadata.partition()+"---"+metadata.offset());
                }
            });

        }
        producer.close();
    }
}
```

**(二)指定分区发送，且有回调的生产者**

1.新建ProducePatition.java实现Partitioner接口

```
public class ProducePatition implements Partitioner {

    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        return 0;
    }

    @Override
    public void close() {

    }

    @Override
    public void configure(Map<String, ?> configs) {

    }
}
```

2.在生产者里面设置参数`props.put("partitioner.class","com.gzstrong.TestKafka.ProducePatition");`

```
public class MyProduce {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers","192.168.197.30:9092");
        props.put("acks","all");
        props.put("retries",0);
        props.put("batch.size",16384);
        props.put("linger.ms",1);
        props.put("buffer.memory",33554432);
        props.put("partitioner.class","com.gzstrong.TestKafka.ProducePatition");
        props.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
        Producer<String, String> producer = new KafkaProducer<>(props);
        for(int i=0;i<10;i++){
            producer.send(new ProducerRecord<>("test", Integer.toString(i), Integer.toString(i)), (metadata, exception) -> System.out.println(metadata.partition()+"---"+metadata.offset()));
        }
        producer.close();
    }
}
```

##  六.消费者java编码

（一）高级消费者

```
public class MyComSumer {
    public static void main(String[] args){
        Properties props = new Properties();
        props.put("bootstrap.servers", "192.168.197.30:9092");
        props.put("group.id", "test");
        props.put("enable.auto.commit", "false");
        props.put("auto.commit.interval.ms", "1000");
        props.put("session.timeout.ms", "30000");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Arrays.asList("test", "bar"));
        final int minBatchSize = 200;
        List<ConsumerRecord<String, String>> buffer = new ArrayList<>();
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(100);
            for (ConsumerRecord<String, String> record : records) {
                System.out.println("offset:"+record.offset()+" partition: "+record.partition()+" record: "+ record);
            }
        }
    }
}
```

























#  spark

##  一.spark集群搭建

**（一）.安装基础环境（JAVA和SCALA环境）**

**1.安装JDK**

(1)下载jdk1.8.0_161
(2)在/etc/profile中添加如下配置
 vim /etc/profile
```
    export JAVA_HOME=/soft/jdk1.8.0_161
    export JRE_HOME=${JAVA_HOME}/jre
    export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
    export PATH=${JAVA_HOME}/bin:$PATH
```
(3)使环境变量生效，source /etc/profile
(4)安装验证# java -version 

**2.安装SCALA**

(1)到http://www.scala-lang.org/download/2.11.8.html下载scala2.11.8.tar.gz
将下载的文件上传到/soft目录下
```
tar -zxvf /soft/scala-2.11.8.tgz -C /soft/
```
(2)在/etc/profile中添加如下配置
  vim /etc/profile
```
  export SCALA_HOME=/soft/scala-2.11.8
  export PATH=$SCALA_HOME/bin:$PATH
```
(3)使环境变量生效，source /etc/profile
(4)安装验证# java -version 

**（二）Hadoop2.7.4完全分布式搭建**

**（三）Spark2.1.0完全分布式环境搭建 **

以下操作都在Master节点进行。
1.下载二进制包spark-2.1.0-bin-hadoop2.7.tgz
2.解压并移动到相应目录，命令如下：
```
tar -zxvf spark-2.2.1-bin-hadoop2.7.tgz -C /soft/
```
3.修改相应的配置文件。
　　在/etc/profile中添加如下配置
      	vim /etc/profile
```　　
  export SPARK_HOME=/soft/spark-2.2.1-bin-hadoop2.7/
  export PATH=$PATH:$SPARK_HOME/bin
```
    使环境变量生效，source /etc/profile
4.复制spark-env.sh.template成spark-env.sh
　  　cp /soft/spark-2.2.1-bin-hadoop2.7/conf/spark-env.sh.template /soft/spark-2.2.1-bin-		hadoop2.7/conf/spark-env.sh
5.修改$SPARK_HOME/conf/spark-env.sh，添加如下内容：
```　
export JAVA_HOME=/soft/jdk1.8.0_161/
export SCALA_HOME=/soft/scala-2.11.8
export HADOOP_HOME=/soft/hadoop-2.7.4
export HADOOP_CONF_DIR=/soft/hadoop-2.7.4/etc/hadoop
export SPARK_MASTER_IP=master
export SPARK_MASTER_HOST=master
export SPARK_WORKER_MEMORY=512m
export SPARK_HOME=/soft/spark-2.2.1-bin-hadoop2.7
export SPARK_DIST_CLASSPATH=$(/soft/hadoop-2.7.4/bin/hadoop classpath)
```
6.复制slaves.template成slaves
7.cp /soft/spark-2.2.1-bin-hadoop2.7/conf/slaves.template /soft/spark-2.2.1-bin-hadoop2.7/conf/slaves
8.修改$SPARK_HOME/conf/slaves，添加如下内容：
```　
master
slave1
slave2
```
9.将配置好的spark文件复制到Slave1和Slave2节点。
	scp /opt/spark-2.1.0-bin-hadoop2.7 root@Slave1:/opt
      	scp /opt/spark-2.1.0-bin-hadoop2.7 root@Slave2:/opt
10.修改Slave1和Slave2配置。
　　在slave1和slave2上分别修改/etc/profile，增加Spark的配置，过程同Master一样。
　　在slave1和slave2修改$SPARK_HOME/conf/spark-env.sh，将export SPARK_LOCAL_IP=114.55.246.88改成slave1和slave2对应节点的IP。
11.在master节点启动集群。
　　/opt/spark-2.1.0-bin-hadoop2.7/sbin/start-all.sh
12.查看集群是否启动成功：
　　jps
　　master在Hadoop的基础上新增了：
　　master
　　slave在Hadoop的基础上新增了：
　　Worker

## 二.spark的maven项目构建（基于idea 和maven）

首先idea安装scala插件
![输入图片说明](https://images.gitee.com/uploads/images/2019/0521/154430_f3a9b447_429848.png "QQ截图20190521154225.png")

新建一个maven项目
开始创建项目体系结构
File --> Project 
![输入图片说明](https://static.oschina.net/uploads/img/201802/11153035_Ac2b.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201802/11153132_Ijqk.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201802/11153403_6Rrj.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201802/11153427_h2XW.png "在这里输入图片标题")

pom.xml

```
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.chen</groupId>
    <artifactId>Spark_Test</artifactId>
    <version>1.0-SNAPSHOT</version>
    <inceptionYear>2008</inceptionYear>
    <properties>
        <scala.version>2.11.0</scala.version>
        <spark.version>2.0.2</spark.version>
        <scala>2.11</scala>
    </properties>

    <repositories>
        <repository>
            <id>scala-tools.org</id>
            <name>Scala-Tools Maven2 Repository</name>
            <url>http://scala-tools.org/repo-releases</url>
        </repository>
    </repositories>

    <pluginRepositories>
        <pluginRepository>
            <id>scala-tools.org</id>
            <name>Scala-Tools Maven2 Repository</name>
            <url>http://scala-tools.org/repo-releases</url>
        </pluginRepository>
    </pluginRepositories>

    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.4</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.specs</groupId>
            <artifactId>specs</artifactId>
            <version>1.2.5</version>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${scala}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${scala}</artifactId>
            <version>${spark.version}</version>
        </dependency>
    </dependencies>

    <build>
        <sourceDirectory>src/main/scala</sourceDirectory>
        <testSourceDirectory>src/test/scala</testSourceDirectory>
        <plugins>
            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
                <configuration>
                    <scalaVersion>2.11</scalaVersion>
                    <args>
                        <arg>-target:jvm-1.5</arg>
                    </args>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.19</version>
                <configuration>
                    <skip>true</skip>
                </configuration>
            </plugin>

            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.0</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                    <excludes>
                        <exclude>**/*.java</exclude>
                    </excludes>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-eclipse-plugin</artifactId>
                <configuration>
                    <downloadSources>true</downloadSources>
                    <buildcommands>
                        <buildcommand>ch.epfl.lamp.sdt.core.scalabuilder</buildcommand>
                    </buildcommands>
                    <additionalProjectnatures>
                        <projectnature>ch.epfl.lamp.sdt.core.scalanature</projectnature>
                    </additionalProjectnatures>
                    <classpathContainers>
                        <classpathContainer>org.eclipse.jdt.launching.JRE_CONTAINER</classpathContainer>
                        <classpathContainer>ch.epfl.lamp.sdt.launching.SCALA_CONTAINER</classpathContainer>
                    </classpathContainers>
                </configuration>
            </plugin>
        </plugins>
    </build>
    <reporting>
        <plugins>
            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <configuration>
                    <scalaVersion>${scala.version}</scalaVersion>
                </configuration>
            </plugin>
        </plugins>
    </reporting>
</project>

```

目录结构如下:
![输入图片说明](https://static.oschina.net/uploads/img/201803/29141709_MjWs.png "在这里输入图片标题")
在src的main目录下新建java和scala文件夹
![输入图片说明](https://static.oschina.net/uploads/img/201803/29142045_fhmF.png "在这里输入图片标题")
同时修改java和scala文件夹为源码文件夹
![输入图片说明](https://static.oschina.net/uploads/img/201803/29142301_rELw.png "在这里输入图片标题")
![输入图片说明](https://static.oschina.net/uploads/img/201803/29142317_bdXa.png "在这里输入图片标题")
在src的test目录下新建java和scala文件夹
![输入图片说明](https://static.oschina.net/uploads/img/201803/29142115_RNRo.png "在这里输入图片标题")
同时修改java和scala文件夹为测试文件夹
![输入图片说明](https://static.oschina.net/uploads/img/201803/29142409_KwUu.png "在这里输入图片标题")

然后设置scal为项目的sdk

## 三.spark的java7代码编写

新建package名称为
![输入图片说明](https://static.oschina.net/uploads/img/201803/29142627_StWH.png "在这里输入图片标题")
新建Java7WordCount.java的文件

Java7WordCount.java

    package com.chen;
    
    import org.apache.spark.SparkConf;
    import org.apache.spark.api.java.JavaPairRDD;
    import org.apache.spark.api.java.JavaRDD;
    import org.apache.spark.api.java.JavaSparkContext;
    import org.apache.spark.api.java.function.FlatMapFunction;
    import org.apache.spark.api.java.function.Function2;
    import org.apache.spark.api.java.function.PairFunction;
    import org.apache.spark.api.java.function.VoidFunction;
    import scala.Tuple2;
    
    import java.util.Arrays;
    import java.util.Iterator;
    
    /**
     * @author 陈锦华
     * @version V1.0
     * @Title:
     * @Description: create by Intellij Idea
     * @date 2018/3/29 0029 上午 10:57
     **/
    public class Java7WordCount {
    
        /**
         * 使用Java的方式开发进行本地测试Spark的WordCount程序
         *
         */
            public static void main(String[] args) {
                /**
                 * * setAppName：设置应用名字，此名字会在Spark web UI显示
                 * setMaster：设置主节点URL，本例使用“local”是指本机单线程，另外还有以下选项：
                 * local[K]：本机K线程
                 * local[*]：本机多线程，线程数与服务器核数相同
                 * spark://HOST:PORT：Spark集群地址和端口，默认端口为7077
                 * mesos://HOST:PORT：Mesos集群地址和端口，默认端口为5050
                 * yarn：YARN集群
                 * 第1步：创建Spark的配置对象SparkConf，设置Spark程序的运行时的配置信息，
                 * 例如说通过setMaster来设置程序要链接的Spark集群的Master的URL,如果设置
                 * 为local，则代表Spark程序在本地运行，特别适合于机器配置条件非常差（例如 只有1G的内存）的初学者 *
                 */
                SparkConf conf = new SparkConf().setAppName("Spark WordCount written by Java").setMaster("local");
                /**
                 * 第2步：创建SparkContext对象
                 * SparkContext是Spark程序所有功能的唯一入口，无论是采用Scala、Java、Python
                 * 、R等都必须有一个SparkContext(不同的语言具体的类名称不同，如果是Java的话则为JavaSparkContext)
                 * SparkContext核心作用：初始化Spark应用程序运行所需要的核心组件，包括DAGScheduler、TaskScheduler、
                 * SchedulerBackend 同时还会负责Spark程序往Master注册程序等
                 * SparkContext是整个Spark应用程序中最为至关重要的一个对象
                 */
                JavaSparkContext sc = new JavaSparkContext(conf); // 其底层实际上就是Scala的SparkContext
                /**
                 * 第3步：根据具体的数据来源（HDFS、HBase、Local FS、DB、S3等）通过JavaSparkContext来创建JavaRDD
                 * JavaRDD的创建基本有三种方式：根据外部的数据来源（例如HDFS）、根据Scala集合、由其它的RDD操作
                 * 数据会被RDD划分成为一系列的Partitions，分配到每个Partition的数据属于一个Task的处理范畴
                 * 注意：文件路径不能直接用Windows路径中的反斜扛\，要改成Linux下的斜扛/
                 */
                JavaRDD<String> lines = sc.textFile("C:\\offline_FtnInfo.txt");
                /**
                 * 第4步：对初始的JavaRDD进行Transformation级别的处理，例如map、filter等高阶函数等的编程，来进行具体的数据计算
                 * 第4.1步：讲每一行的字符串拆分成单个的单词
                 */
                JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
                            @Override
                            public Iterator<String> call(String line) throws Exception {
                                return Arrays.asList(line.split(" ")).iterator();
                            }
                        });
                /**
                 * 第4步：对初始的JavaRDD进行Transformation级别的处理，例如map、filter等高阶函数等的编程，来进行具体的数据计算
                 * 第4.2步：在单词拆分的基础上对每个单词实例计数为1，也就是word => (word, 1)
                 */
                JavaPairRDD<String, Integer> pairs = words.mapToPair(new PairFunction<String, String, Integer>() {
                            public Tuple2<String, Integer> call(String word) throws Exception {
                                return new Tuple2<String, Integer>(word, 1);
                            }
                        });
                /**
                 * 第4步：对初始的RDD进行Transformation级别的处理，例如map、filter等高阶函数等的编程，来进行具体的数据计算
                 * 第4.3步：在每个单词实例计数为1基础之上统计每个单词在文件中出现的总次数
                 */
                JavaPairRDD<String, Integer> wordsCount = pairs.reduceByKey(new Function2<Integer, Integer, Integer>() { // 对相同的Key，进行Value的累计（包括Local和Reducer级别同时Reduce）
                            public Integer call(Integer v1, Integer v2) throws Exception {
                                return v1 + v2;
                            }
                        });
                wordsCount.foreach(new VoidFunction<Tuple2<String, Integer>>() {
                    public void call(Tuple2<String, Integer> pairs) throws Exception {
                        System.out.println(pairs._1 + " : " + pairs._2);
                    }
                });
                sc.close();
            }
        }

## 四.spark的java8代码编写

新建package名称为com.chen

新建Java8WordCount.java的文件

Java8WordCount.java

    package com.chen;
    
    import org.apache.spark.SparkConf;
    import org.apache.spark.api.java.JavaPairRDD;
    import org.apache.spark.api.java.JavaRDD;
    import org.apache.spark.api.java.JavaSparkContext;
    import scala.Tuple2;
    
    import java.util.Arrays;
    import java.util.List;
    
    /**
     * @author 陈锦华
     * @version V1.0
     * @Title:
     * @Description: create by Intellij Idea
     * @date 2018/3/29 0029 上午 10:57
     **/
    public class Java8WordCount {
        /**
         * 使用Java的方式开发进行本地测试Spark的WordCount程序
         */
        public static void main(String[] args) {
            WordCount2();
        }
    
        public static void WordCount1(){
            /**
             * 第4步：对初始的JavaRDD进行Transformation级别的处理，例如map、filter等高阶函数等的编程，来进行具体的数据计算
             * 第4.2步：在单词拆分的基础上对每个单词实例计数为1，也就是word => (word, 1)
             * 第4步：对初始的RDD进行Transformation级别的处理，例如map、filter等高阶函数等的编程，来进行具体的数据计算
             * 第4.3步：在每个单词实例计数为1基础之上统计每个单词在文件中出现的总次数
             */
            SparkConf conf = new SparkConf().setAppName("Spark WordCount written by Java").setMaster("local");
            JavaSparkContext sc = new JavaSparkContext(conf); // 其底层实际上就是Scala的SparkContext
            JavaRDD<String> lines = sc.textFile("C:\\offline_FtnInfo.txt");
            JavaRDD<String> words = lines.flatMap((line) ->Arrays.asList(line.split(" ")).iterator());
            JavaPairRDD<String, Integer> pairs = words.mapToPair((word)->new Tuple2<>(word, 1));
            JavaPairRDD<String, Integer> wordsCount = pairs.reduceByKey((Integer v1, Integer v2) ->v1 + v2);
            wordsCount.foreach((Tuple2<String, Integer> pair)->System.out.println(pair._1 + " : " + pair._2));
            sc.close();
        }
    
        public static void WordCount2(){
            SparkConf conf = new SparkConf().setAppName("Spark WordCount written by Java").setMaster("local");
            JavaSparkContext sc = new JavaSparkContext(conf); // 其底层实际上就是Scala的SparkContext
            JavaRDD<String> lines = sc.textFile("C:\\offline_FtnInfo.txt");
            JavaRDD<String> words = lines.flatMap(line -> Arrays.asList(line.split(" ")).iterator());
            JavaPairRDD<String, Integer> counts = words.mapToPair(w -> new Tuple2<String, Integer>(w, 1))
                .reduceByKey((x, y) -> x + y);
            List<Tuple2<String, Integer>> output = counts.collect();
                for (Tuple2<?, ?> tuple : output) {
                    System.out.println(tuple._1() + ":== " + tuple._2());
                }
            sc.close();
        }
    }    

## 五.spark的scalar代码编写

1.新建package名称为

com.chen

2.新建scalaWordCount.scala的文件

scalaWordCount.scala

    package com.chen
    
    import org.apache.spark.{SparkConf, SparkContext}
    
        object scalaWordCount{
            def main(args: Array[String]) {
                //setMaster("local") 本机的spark就用local，远端的就写ip
                //如果是打成jar包运行则需要去掉 setMaster("local")因为在参数中会指定。
                val conf = new SparkConf().setAppName("local Application").setMaster("local")
                val sc = new SparkContext(conf)
                val rdd = sc.textFile("C:\\offline_FtnInfo.txt")
                val wordcount = rdd
                        .flatMap(_.split(" "))
                        .map((_,1))
                        .reduceByKey(_ + _)
                        .map(x => (x._2,x._1))
                        .sortByKey(false)
                        .map(x => (x._2,x._1)
                    )
                wordcount.foreach(x=>println(x._1+"的数量是:"+x._2))
                sc.stop()
            }
        }

##  六.RDD

###  （一）rdd的创建方式

**1.parallelize从集合创建**

```
var rdd = sc.parallelize(1 to 10)
```

**2.makeRdd创建**

```
val seq = List((1, List("iteblog.com", "sparkhost1.com", "sparkhost2.com")),(2, List("iteblog.com", "sparkhost2.com")))
val rdd = sc.makeRDD(seq)
```

**3.从外部存储创建RDD**

```
var rdd = sc.textFile("hdfs:///tmp/lxw1234/1.txt")
```

**4.从其他HDFS文件格式创建**

hadoopFile

sequenceFile

objectFile

newAPIHadoopFile

从Hadoop接口API创建

hadoopRDD

newAPIHadoopRDD

**5.从HBase创建RDD** 

```scala
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor, TableName}
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor, TableName}
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.client.HBaseAdmin
val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE,"lxw1234")
var hbaseRDD = sc.newAPIHadoopRDD(conf,classOf[org.apache.hadoop.hbase.mapreduce.TableInputFormat],classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],classOf[org.apache.hadoop.hbase.client.Result])
```



## 七.spark Sql

### （一）.spark sql的join

1.spark直接传入List列表用createDataset方法进行创建dataset，然后转DataFrame

```
package com.chen
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}

object TestSpark{
    def main(args: Array[String]): Unit = {
        val spark: SparkSession = SparkSession.builder().master("local[*]").appName("sparkTest").getOrCreate()
        import spark.implicits._
        val lines: Dataset[(String)] = spark.createDataset(List("1,chenjinhua,cn","2,Lisi,usa","3,jony,usa"))
        val empDataset: Dataset[(String, String, String)] = lines.map(line => {
            val field: Array[String] = line.split(",")
            var id = field(0)
            var empName = field(1)
            var code = field(2)
            (id, empName, code)
        })
        val empDataFrame: DataFrame = empDataset.toDF("id", "empName", "code")
        empDataFrame.createTempView("emp")

        val nations: Dataset[(String)] = spark.createDataset(List("cn,中国","usa,美国"))
        val nationDataset: Dataset[(String, String)] = nations.map(line => {
            val field: Array[String] = line.split(",")
            val code = field(0)
            val name = field(1)
            (code, name)
        })
        val nationDataFrame: DataFrame = nationDataset.toDF("code","name")
        nationDataFrame.createTempView("nation")
        val dframe: DataFrame = spark.sql("select * from emp left join nation on emp.code=nation.code")
        dframe.show()
        spark.stop()
    }
}
```

执行之后展示结果如下

```
+---+----------+----+----+----+
| id|   empName|code|code|name|
+---+----------+----+----+----+
|  1|chenjinhua|  cn|  cn|  中国|
|  2|      Lisi| usa| usa|  美国|
|  3|      jony| usa| usa|  美国|
+---+----------+----+----+----+
```

###  （二）spark Sql计算nginx的日志

1.access.log

```
192.168.197.50 - - [20/Dec/2018:15:46:15 +0800] "GET /api/rep/scRepairBusHouse/datagrid?pageNum=1&pageRow=10&busHouseCode=&orgId= HTTP/1.1" 200 1745 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:15 +0800] "GET /api/sys/coreDepart/showCompsTreeByUser HTTP/1.1" 200 809 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:15 +0800] "GET /api/sys/coreDepart/showCompsTreeByUser HTTP/1.1" 200 809 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:26 +0800] "GET /api/rep/scRepairBusHouse/datagrid?pageNum=1&pageRow=10&busHouseCode=&orgId=1 HTTP/1.1" 200 1745 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:28 +0800] "GET /api/rep/scRepairBusHouse/datagrid?pageNum=1&pageRow=10&busHouseCode=&orgId=6 HTTP/1.1" 200 1340 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:29 +0800] "GET /api/rep/scRepairBusHouse/datagrid?pageNum=1&pageRow=10&busHouseCode=&orgId=7 HTTP/1.1" 200 483 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:30 +0800] "GET /api/rep/scRepairBusHouse/datagrid?pageNum=1&pageRow=10&busHouseCode=&orgId=6 HTTP/1.1" 200 1340 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:30 +0800] "GET /api/rep/scRepairBusHouse/datagrid?pageNum=1&pageRow=10&busHouseCode=&orgId=1 HTTP/1.1" 200 1745 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.145 - - [20/Dec/2018:15:46:30 +0800] "GET /static/js/13.c5de91a3a5351c406417.js HTTP/1.1" 304 0 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:31 +0800] "GET /api/rep/scRepairFixPlace/datagrid?pageNum=1&pageRow=10&positionCode=&orgId= HTTP/1.1" 200 1559 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:31 +0800] "GET /api/sys/coreDepart/showCompsTreeByUser HTTP/1.1" 200 809 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:31 +0800] "GET /api/sys/coreDepart/showCompsTreeByUser HTTP/1.1" 200 809 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.50 - - [20/Dec/2018:15:46:31 +0800] "GET /api/rep/scRepairBusHouse/datagrid?pageNum=1&pageRow=10&busHouseCode=&orgId=1 HTTP/1.1" 200 1745 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.145 - - [20/Dec/2018:15:46:33 +0800] "GET /api/rep/scRepairFixitemInfo/datagrid?pageNum=1&pageRow=10&lineCode=&fixItemTypeId= HTTP/1.1" 200 906 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:33 +0800] "GET /api/rep/scRepairFixitemType/datagrid?fixItemTypeCode= HTTP/1.1" 200 861 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:33 +0800] "GET /api/rep/scRepairFixitemType/selectTreeData HTTP/1.1" 200 861 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:34 +0800] "GET /api/rep/scRepairFaultInfo/datagrid?pageNum=1&pageRow=10&lineCode=&faultTypeId= HTTP/1.1" 200 816 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:34 +0800] "GET /api/rep/scRepairFaultType/selectTreeData HTTP/1.1" 200 992 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.145 - - [20/Dec/2018:15:46:34 +0800] "GET /api/rep/scRepairFaultType/datagrid?faultTypeCode= HTTP/1.1" 200 992 "http://192.168.197.28:9090/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299"
192.168.197.50 - - [20/Dec/2018:15:46:37 +0800] "GET /static/js/45.f9d68fececdbd4771c9f.js HTTP/1.1" 200 14268 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
192.168.197.50 - - [20/Dec/2018:15:46:37 +0800] "GET /api/mat/scClRelationClient/datagrid?pageNum=1&pageRow=10&clientName= HTTP/1.1" 200 1815 "http://192.168.197.28:9090/?tdsourcetag=s_pctim_aiomsg" "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36"
```

2.计算每个ip访问的次数，并保存到mysql数据库中

```
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
object SparkSqlTest2{
    def main(args: Array[String]): Unit = {
        val spark: SparkSession = SparkSession.builder().master("local[*]").appName("sparkTest").getOrCreate()
        import spark.implicits._
        val lines: Dataset[String] = spark.read.textFile("file:///C:/Users/Administrator/Desktop/access.log")
        val data = lines.map(line => {
            val fields: Array[String] = line.split(" ")
            var ip = fields(0)
            var method = fields(5)
            var path = fields(6)
            var HTTPmethod = fields(7)
            var status = fields(8)
            var server = fields(10)
            (ip, method, path, HTTPmethod,status,server)
        }).toDF("ip", "method", "path", "HTTPmethod","status", "server")
        data.createTempView("logs")
        val frame: DataFrame = spark.sql("select ip,count(ip) from  logs group by ip")

        val url="jdbc:mysql://localhost:3306/anfu"
        val table="logs"
        val prop=new java.util.Properties()
        prop.put("driver","com.mysql.jdbc.Driver")
        prop.put("user","root")
        prop.put("password","root")

        //表自动创建
        frame.write.jdbc(url,table,prop)
        spark.stop()
    }
}
```

###  （三）dataset创建方式

**1.spark读取text文件**

```
val lines=spark.read.textFile("file:///c:/text")
```

**2.spark传入list创建**

```scala
val lines = spark.createDataset(List("1,chenjinhua,cn","2,Lisi,usa","3,jony,usa"))
```

**3.spark传入Seq创建**

```
val dataset = Seq(
  (1, "zhangyuhang", java.sql.Date.valueOf("2018-05-15")),
  (2, "zhangqiuyue", java.sql.Date.valueOf("2018-05-15"))
)
```

**4.spark传入Rdd创建**

```
val dataset = spark.createDataset(spark.sparkContext.parallelize(1 to 10))
```

###  （四）dataFrame创建方式

**1.使用toDF函数创建DataFrame** 

```
import spark.implicits._
val df = Seq(
  (1, "zhangyuhang", java.sql.Date.valueOf("2018-05-15")),
  (2, "zhangqiuyue", java.sql.Date.valueOf("2018-05-15"))
).toDF("id", "name", "created_time")
```

**2.使用createDataFrame函数创建DataFrame** ，**通过schema + row 来创建** 

可以理解为schema为表的表头，row为表的数据记录 

```
import org.apache.spark.sql.types._
//定义dataframe的结构的schema
val schema = StructType(List(
    StructField("id", IntegerType, nullable = false),
    StructField("name", StringType, nullable = true),
    StructField("create_time", DateType, nullable = true)
))

//定义dataframe内容的rdd
val rdd = sc.parallelize(Seq(
  Row(1, "zhangyuhang", java.sql.Date.valueOf("2018-05-15")),
  Row(2, "zhangqiuyue", java.sql.Date.valueOf("2018-05-15"))
))
//创建dataframe
val df = spark.createDataFrame(rdd, schema)
```

或者

```
import org.apache.spark.sql.types._
//传入属性参数
val schemaString = " id name create_time"
//解析参数变成StructField
val fields = schemaString.split(" ").map(fieldName => StructField(fieldname, StringType, nullable = true))
//定义dataframe的结构的schema
val schema = StructType(fields)
//定义dataframe内容的rdd
val lines = sc.textFile("file:///people.txt")
val rdd = lines.spilt(_.split(",")).map(field=>ROW(field(0),field(1).trim) )
//创建dataframe
val df = spark.createDataFrame(rdd, schema) 
```

**3.通过反射机制创建DataFrame**

 首先要定义一个case class，因为只有case class才能被Spark隐式转化为DataFrame

```
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.Encoder
import spark.implicits._
//创建匹配类
case class Person(id:Int,name:String,age:Long)
//读取文件生成rdd
val rdd = sc.textFile("file:///")
//通过匹配类把rdd转化成dataframe
val df = rdd.map(_.split(",")).map(attributes => Person(attributes(0),attributes(1),attributes(2).trim.toInt)).toDF()　
```



### （五）spark sql设置分区数量

保存文件时可以设置分区为一个，文件数量就会是一个

```
spark.sqlContext.setConf("spark.sql.shuffle.partitions","1")
```

### （六）spark sql数据读取数据

**1.读取parquet文件** 

```
val df = spark.read.parquet("hdfs:/path/to/file")
```

**2.读取json文件 **

```
val df = spark.read.json("examples/src/main/resources/people.json")
```

**3.读取csv**

```
val df = spark.read.format("com.databricks.spark.csv")
        .option("header", "true") //reading the headers
        .option("mode", "DROPMALFORMED")
        .load("csv/file/path")
```

**4.读取Hive表**

```
spark.table("test.person") // 库名.表名 的格式
     .registerTempTable("person")  // 注册成临时表
spark.sql(
      """
        | select *
        | from person
        | limit 10
      """.stripMargin).show()
```

 

###  （七）spark sql数据保存

**1.通过df.write.format().save("file:///")保存** 

write.format()支持输出的格式有parquet、 JSON、csv、JDBC、text等文件格式,save()定义保存的位置

当我们保存成功后可以在保存位置的目录下看到文件，但是这个文件并不是一个文件而是一个目录。

**（1）parquet格式**

```
df.write.mode(SaveMode.Append).format("parquet").save("file:///C:/Users/Administrator/Desktop/parquet")
df.write.mode(SaveMode.Append).parquet("file:///C:/Users/Administrator/Desktop/parquet2")
```

**（2）json格式**

```
df.write.format("json").save("file:///C:/Users/Administrator/Desktop/myjson")
```

**（3）csv格式**

```
df.write.format("csv").save("file:///C:/Users/Administrator/Desktop/mycsv")
```

**（4）jdbc格式，保存到mysql数据库**

```scala
val url="jdbc:mysql://localhost:3306/anfu"
val table="logs"
val prop=new java.util.Properties()
prop.put("driver","com.mysql.jdbc.Driver")
prop.put("user","root")
prop.put("password","root")
//表自动创建
frame.write.jdbc(url,table,prop)
```

**（5）text格式，保存的时候必须是一列，否则报错**

```
df.write.format("text").save("file:///C:/Users/Administrator/Desktop/mytext")
```

**2.通过df.rdd.saveAsTextFile("file:///")转化成rdd再保存**



**我们对于不同格式的文件读写来说，我们一般使用两套对应方式**

## 八.spark Stream

### （一）netcat运用

**1.netcat在windows下使用**

Windows间传输：

1、安装NetCat

2、开启服务端：nc -l -p 9999

3、开启客户端：nc localhost 9999

4、客户端和服务端间通信

**2.netcat在linux下使用**

**（1）netcat的安装**

```
yum install nc -y
```

**（2）netcat使用**

```
nc -lk 9000
```

### （二）spark Stream的socketTextStream

**（1）代码编写**

```
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Duration, StreamingContext}

object SparkStreamTest1{
    def main(args: Array[String]): Unit = {
        val spark: SparkSession = SparkSession.builder().master("local[*]").appName("sparkTest").getOrCreate()
        val sc= spark.sparkContext
        val ssc = new StreamingContext(sc,Duration(5000))
        val line: ReceiverInputDStream[String] = ssc.socketTextStream("192.168.232.140",9000)
        val ds: DStream[(String, Int)] = line.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
        ds.print()
        ssc.start()
        ssc.awaitTermination()
    }
}
```

**（2）在192.168.232.140的netcat下输入数据**

```
[root@master ~]# nc -lk 9000
dsfadsfad
l1l
kj
hjhj
```

**（3）输出结果**

```
(dsfadsfad,1)
(hjhj,1)
(l1l,1)
(kj,1)
```

###  （三）spark Stream的结果集保存到数据库

**（1）获取socketTextStream中的数据进行计算之后保存mysql**

```
package com.chen
import java.sql.{Connection, DriverManager, Statement}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Duration, StreamingContext}

object SparkStream2MysqlTest1{
    def main(args: Array[String]): Unit = {
        Logger.getLogger("com.chen").setLevel(Level.OFF)
        val spark: SparkSession = SparkSession.builder().master("local[*]").appName("sparkTest").getOrCreate()
        val sc= spark.sparkContext
        val ssc = new StreamingContext(sc,Duration(5000))
        val line: ReceiverInputDStream[String] = ssc.socketTextStream("192.168.232.140",9000)
        val ds: DStream[(String, Int)] = line.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)

        ds.foreachRDD(rdd=>rdd.foreachPartition(line=>{
                Class.forName("com.mysql.jdbc.Driver")
                val connection: Connection = DriverManager.getConnection("jdbc:mysql://192.168.197.28:3306/sys","dev","dev@gzstrong")
                try{
                    for (row<-line){
                        val statement: Statement = connection.createStatement()
                        val sql="INSERT INTO `sys`.`test` (`value`, `valueCount`) VALUES ('"+row._1+"','"+row._2+"');"
                        statement.execute(sql)
                    }
                }finally {
                    connection.close()
                }
            })
        )
        ssc.start()
        ssc.awaitTermination()
    }
}
```

**（2）在192.168.232.140的netcat下输入数据**

```
[root@master ~]# nc -lk 9000
test
sichuang
gzstrong
```

**（3）查看数据库中的结果**

|  value   | valuecount |
| :------: | :--------: |
|   test   |     1      |
| sichuang |     1      |
| gzstrong |     1      |



###  （四）spark Stream与kafka的集成





























































































## 九.Structured Streaming





## 十.spark案例分析与编程实现

案例一

a. 案例描述

提起 Word Count(词频数统计)，相信大家都不陌生，就是统计一个或者多个文件中单词出现的次数。本文将此作为一个入门级案例，由浅入深的开启使用 Scala 编写 Spark 大数据处理程序的大门。

b．案例分析

对于词频数统计，用 Spark 提供的算子来实现，我们首先需要将文本文件中的每一行转化成一个个的单词, 其次是对每一个出现的单词进行记一次数，最后就是把所有相同单词的计数相加得到最终的结果。

对于第一步我们自然的想到使用 flatMap 算子把一行文本 split 成多个单词，然后对于第二步我们需要使用 map 算子把单个的单词转化成一个有计数的 Key-Value 对，即 word -> (word,1). 对于最后一步统计相同单词的出现次数，我们需要使用 reduceByKey 算子把相同单词的计数相加得到最终结果。

c. 编程实现

清单 1.SparkWordCount 类源码

SparkWordCount.scala

    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    import org.apache.spark.SparkContext._
     
    object SparkWordCount {
     def FILE_NAME:String = "word_count_results_";
     def main(args:Array[String]) {
     if (args.length < 1) {
     println("Usage:SparkWordCount FileName");
     System.exit(1);
     }
     val conf = new SparkConf().setAppName("Spark Exercise: Spark Version Word Count Program");
     val sc = new SparkContext(conf);
     val textFile = sc.textFile(args(0));
     val wordCounts = textFile.flatMap(line => line.split(" ")).map(
                                            word => (word, 1)).reduceByKey((a, b) => a + b)
     //print the results,for debug use.
     //println("Word Count program running results:");
     //wordCounts.collect().foreach(e => {
     //val (k,v) = e
     //println(k+"="+v)
     //});
     wordCounts.saveAsTextFile(FILE_NAME+System.currentTimeMillis());
     println("Word Count program running results are successfully saved.");
     }
    }


d. 提交到集群执行

本实例中, 我们将统计 HDFS 文件系统中/user/fams 目录下所有 txt 文件中词频数。其中 spark-exercise.jar 是 Spark 工程打包后的 jar 包，这个 jar 包执行时会被上传到目标服务器的/home/fams 目录下。运行此实例的具体命令如下：

    ./spark-submit \
    --class com.ibm.spark.exercise.basic.SparkWordCount \
    --master spark://hadoop036166:7077 \
    --num-executors 3 \
    --driver-memory 6g --executor-memory 2g \
    --executor-cores 2 \
    /home/fams/sparkexercise.jar \
    hdfs://hadoop036166:9000/user/fams/*.txt


**案例二**

a. 案例描述

该案例中，我们将假设我们需要统计一个 1000 万人口的所有人的平均年龄，当然如果您想测试 Spark 对于大数据的处理能力，您可以把人口数放的更大，比如 1 亿人口，当然这个取决于测试所用集群的存储容量。假设这些年龄信息都存储在一个文件里，并且该文件的格式如下，第一列是 ID，第二列是年龄。

案例二age.txt测试数据格式预览

![è¾å¥å¾çè¯´æ](https://static.oschina.net/uploads/img/201803/29143845_eRx4.jpg) 

现在我们需要用 Scala 写一个生成 1000 万人口年龄数据的文件，源程序如下：

清单 3. 年龄信息文件生成类源码

    import java.io.FileWriter
    import java.io.File
    import scala.util.Random
     
    object SampleDataFileGenerator {
     
    def main(args:Array[String]) {
    val writer = new FileWriter(new File("C: \\sample_age_data.txt"),false)
    val rand = new Random()
    for ( i <- 1 to 10000000) {
    writer.write( i + " " + rand.nextInt(100))
    writer.write(System.getProperty("line.separator"))
    }
    writer.flush()
    writer.close()
    }
    }


b. 案例分析

要计算平均年龄，那么首先需要对源文件对应的 RDD 进行处理，也就是将它转化成一个只包含年龄信息的 RDD，其次是计算元素个数即为总人数，然后是把所有年龄数加起来，最后平均年龄=总年龄/人数。

对于第一步我们需要使用 map 算子把源文件对应的 RDD 映射成一个新的只包含年龄数据的 RDD，很显然需要对在 map 算子的传入函数中使用 split 方法，得到数组后只取第二个元素即为年龄信息；第二步计算数据元素总数需要对于第一步映射的结果 RDD 使用 count 算子；第三步则是使用 reduce 算子对只包含年龄信息的 RDD 的所有元素用加法求和；最后使用除法计算平均年龄即可。

由于本例输出结果很简单，所以只打印在控制台即可。

c. 编程实现

清单 4.AvgAgeCalculator 类源码

    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    object AvgAgeCalculator {
     def main(args:Array[String]) {
     if (args.length < 1){
     println("Usage:AvgAgeCalculator datafile")
     System.exit(1)
     }
     val conf = new SparkConf().setAppName("Spark Exercise:Average Age Calculator")
     val sc = new SparkContext(conf)
     val dataFile = sc.textFile(args(0), 5);
     val count = dataFile.count()
     val ageData = dataFile.map(line => line.split(" ")(1))
     val totalAge = ageData.map(age => Integer.parseInt(
                                    String.valueOf(age))).collect().reduce((a,b) => a+b)
     println("Total Age:" + totalAge + ";Number of People:" + count )
     val avgAge : Double = totalAge.toDouble / count.toDouble
     println("Average Age is " + avgAge)
     }
    }


案例三

a. 案例描述

本案例假设我们需要对某个省的人口 (1 亿) 性别还有身高进行统计，需要计算出男女人数，男性中的最高和最低身高，以及女性中的最高和最低身高。本案例中用到的源文件有以下格式, 三列分别是 ID，性别，身高 (cm)。

案例三测试数据格式预览



我们将用以下 Scala 程序生成这个文件，源码如下：

清单 7. 人口信息生成类源码

    import java.io.FileWriter
    import java.io.File
    import scala.util.Random
     
    object PeopleInfoFileGenerator {
     def main(args:Array[String]) {
     val writer = new FileWriter(new File("C:\\LOCAL_DISK_D\\sample_people_info.txt"),false)
     val rand = new Random()
     for ( i <- 1 to 100000000) {
     var height = rand.nextInt(220)
     if (height < 50) {
     height = height + 50
     }
     var gender = getRandomGender
     if (height < 100 && gender == "M")
     height = height + 100
     if (height < 100 && gender == "F")
     height = height + 50
     writer.write( i + " " + getRandomGender + " " + height)
     writer.write(System.getProperty("line.separator"))
     }
     writer.flush()
     writer.close()
     println("People Information File generated successfully.")
     }
      
     def getRandomGender() :String = {
     val rand = new Random()
     val randNum = rand.nextInt(2) + 1
     if (randNum % 2 == 0) {
     "M"
     } else {
     "F"
     }
     }
    }


b. 案例分析

对于这个案例，我们要分别统计男女的信息，那么很自然的想到首先需要对于男女信息从源文件的对应的 RDD 中进行分离，这样会产生两个新的 RDD，分别包含男女信息；其次是分别对男女信息对应的 RDD 的数据进行进一步映射，使其只包含身高数据，这样我们又得到两个 RDD，分别对应男性身高和女性身高；最后需要对这两个 RDD 进行排序，进而得到最高和最低的男性或女性身高。

对于第一步，也就是分离男女信息，我们需要使用 filter 算子，过滤条件就是包含”M” 的行是男性，包含”F”的行是女性；第二步我们需要使用 map 算子把男女各自的身高数据从 RDD 中分离出来；第三步我们需要使用 sortBy 算子对男女身高数据进行排序。

c. 编程实现

在实现上，有一个需要注意的点是在 RDD 转化的过程中需要把身高数据转换成整数，否则 sortBy 算子会把它视为字符串，那么排序结果就会受到影响，例如 身高数据如果是：123,110,84,72,100，那么升序排序结果将会是 100,110,123,72,84，显然这是不对的。

清单 8.PeopleInfoCalculator 类源码

    object PeopleInfoCalculator {
     def main(args:Array[String]) {
     if (args.length < 1){
     println("Usage:PeopleInfoCalculator datafile")
     System.exit(1)
     }
     val conf = new SparkConf().setAppName("Spark Exercise:People Info(Gender & Height) Calculator")
     val sc = new SparkContext(conf)
     val dataFile = sc.textFile(args(0), 5);
     val maleData = dataFile.filter(line => line.contains("M")).map(
                                  line => (line.split(" ")(1) + " " + line.split(" ")(2)))
     val femaleData = dataFile.filter(line => line.contains("F")).map(
                                  line => (line.split(" ")(1) + " " + line.split(" ")(2)))
     //for debug use
     //maleData.collect().foreach { x => println(x)}
     //femaleData.collect().foreach { x => println(x)}
     val maleHeightData = maleData.map(line => line.split(" ")(1).toInt)
     val femaleHeightData = femaleData.map(line => line.split(" ")(1).toInt)
     //for debug use
     //maleHeightData.collect().foreach { x => println(x)}
     //femaleHeightData.collect().foreach { x => println(x)}
     val lowestMale = maleHeightData.sortBy(x => x,true).first()
     val lowestFemale = femaleHeightData.sortBy(x => x,true).first()
     //for debug use
     //maleHeightData.collect().sortBy(x => x).foreach { x => println(x)}
     //femaleHeightData.collect().sortBy(x => x).foreach { x => println(x)}
     val highestMale = maleHeightData.sortBy(x => x, false).first()
     val highestFemale = femaleHeightData.sortBy(x => x, false).first()
     println("Number of Male Peole:" + maleData.count())
     println("Number of Female Peole:" + femaleData.count())
     println("Lowest Male:" + lowestMale)
     println("Lowest Female:" + lowestFemale)
     println("Highest Male:" + highestMale)
     println("Highest Female:" + highestFemale)
     }
    }


案例四

a. 案例描述

该案例中我们假设某搜索引擎公司要统计过去一年搜索频率最高的 K 个科技关键词或词组，为了简化问题，我们假设关键词组已经被整理到一个或者多个文本文件中，并且文档具有以下格式。

图 13. 案例四测试数据格式预览



我们可以看到一个关键词或者词组可能出现多次，并且大小写格式可能不一致。

b. 案例分析

要解决这个问题，首先我们需要对每个关键词出现的次数进行计算，在这个过程中需要识别不同大小写的相同单词或者词组，如”Spark”和“spark” 需要被认定为一个单词。对于出现次数统计的过程和 word count 案例类似；其次我们需要对关键词或者词组按照出现的次数进行降序排序，在排序前需要把 RDD 数据元素从 (k,v) 转化成 (v,k)；最后取排在最前面的 K 个单词或者词组。

对于第一步，我们需要使用 map 算子对源数据对应的 RDD 数据进行全小写转化并且给词组记一次数，然后调用 reduceByKey 算子计算相同词组的出现次数；第二步我们需要对第一步产生的 RDD 的数据元素用 sortByKey 算子进行降序排序；第三步再对排好序的 RDD 数据使用 take 算子获取前 K 个数据元素。

c. 编程实现

清单 10.TopKSearchKeyWords 类源码

    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
     
    object TopKSearchKeyWords {
     def main(args:Array[String]){
     if (args.length < 2) {
     println("Usage:TopKSearchKeyWords KeyWordsFile K");
     System.exit(1)
     }
     val conf = new SparkConf().setAppName("Spark Exercise:Top K Searching Key Words")
     val sc = new SparkContext(conf)
     val srcData = sc.textFile(args(0))
     val countedData = srcData.map(line => (line.toLowerCase(),1)).reduceByKey((a,b) => a+b)
     //for debug use
     //countedData.foreach(x => println(x))
     val sortedData = countedData.map{ case (k,v) => (v,k) }.sortByKey(false)
     val topKData = sortedData.take(args(1).toInt).map{ case (v,k) => (k,v) }
     topKData.foreach(println)
     }
    }

**案例二的age.txt文件**

1 16

2 73

3 74

4 76 

5 75

6 78

7 66

8 55

9 85

11 25

12 43

13 45

14 61

15 35

16 38

17 69

18 45

19 55

20 45

21 16

22 73

23 74

24 76 

25 75

26 78

27 66

28 55

29 85

30 85

31 25

32 43

33 45

34 61

35 35

36 38

37 69

38 45

39 55

40 45

七.算子reduceByKey和groupByKey，sortByKey和sortBy区别

1. Spark算子reduceByKey深度解析
   那么这就基本奠定了reduceByKey的作用域是key-value类型的键值对，并且是只对每个key的value进行处理，如果含有多个key的话，那么就对多个values进行处理。这里的函数是我们自己传入的，也就是说是可人为控制的【其实这是废话，人为控制不了这算子一点用没有】。那么举个例子：

    scala> val x = sc.parallelize(Array(("a", 1), ("b", 1), ("a", 1),
         | ("a", 1), ("b", 1), ("b", 1),
         | ("b", 1), ("b", 1)), 3)


我们创建了一个Array的字符串，并把其存入spark的集群上，设置了三个分区【这里我们不关注分区，只关注操作】。那么我们调用reduceByKey并且传入函数进行相应操作【本处我们对相同key的value进行相加操作，类似于统计单词出现次数】：

    scala> val y = x.reduceByKey((pre, after) => (pre + after))


这里两个参数我们逻辑上让他分别代表同一个key的两个不同values，那么结果想必大家应该猜到了： 

    scala> y.collect
    res0: Array[(String, Int)] = Array((a,3), (b,5))


1. reduceByKey和groupByKey区别与用法
   首先，看一看spark官网[1]是怎么解释的：
   reduceByKey(func, numPartitions=None)
   reduceByKey用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义。

groupByKey(numPartitions=None)

也就是，groupByKey也是对每个key进行操作，但只生成一个sequence。需要特别注意“Note”中的话，它告诉我们：如果需要对sequence进行aggregation操作（注意，groupByKey本身不能自定义操作函数），那么，选择reduceByKey/aggregateByKey更好。这是因为groupByKey不能自定义函数，我们需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作。 

    val words = Array("one", "two", "two", "three", "three", "three")
    val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))
    val wordCountsWithReduce = wordPairsRDD.reduceByKey(_ + _)
    val wordCountsWithGroup = wordPairsRDD.groupByKey().map(t => (t._1, t._2.sum))


上面得到的wordCountsWithReduce和wordCountsWithGroup是完全一样的，但是，它们的内部运算过程是不同的。 

（1）当采用reduceByKeyt时，Spark可以在每个分区移动数据之前将待输出数据与一个共用的key结合。借助下图可以理解在reduceByKey里究竟发生了什么。 注意在数据对被搬移前同一机器上同样的key是怎样被组合的(reduceByKey中的lamdba函数)。然后lamdba函数在每个区上被再次调用来将所有值reduce成一个最终结果。整个过程如下：



（2）当采用groupByKey时，由于它不接收函数，spark只能先将所有的键值对(key-value pair)都移动，这样的后果是集群节点之间的开销很大，导致传输延时。整个过程如下：



因此，在对大数据进行复杂计算时，reduceByKey优于groupByKey。

另外，如果仅仅是group处理，那么以下函数应该优先于 groupByKey ：

 　　（1）、combineByKey 组合数据，但是组合之后的数据类型与输入时值的类型不一样。

 　　（2）、foldByKey合并每一个 key 的所有值，在级联函数和“零值”中使用。

1. sortByKey和sortBy区别
   SortByKey()函数

sortBy函数是在org.apache.spark.rdd.RDD类中实现的，它的实现如下：

    def sortBy[K](f: (T) => K,ascending: Boolean = true,
        numPartitions: Int = this.partitions.size)
        (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] =
        this.keyBy[K](f).sortByKey(ascending, numPartitions).values


　该函数最多可以传三个参数：

　　第一个参数是一个函数，该函数的也有一个带T泛型的参数，返回类型和RDD中元素的类型是一致的；

　　第二个参数是ascending，从字面的意思大家应该可以猜到，是的，这参数决定排序后RDD中的元素是升序还是降序，默认是true，也就是升序；

　　第三个参数是numPartitions，该参数决定排序后的RDD的分区个数，默认排序后的分区个数和排序之前的个数相等，即为this.partitions.size。

　　从sortBy函数的实现可以看出，第一个参数是必须传入的，而后面的两个参数可以不传入。而且sortBy函数函数的实现依赖于sortByKey函数，关于sortByKey函数后面会进行说明。

   那么，如何使用sortBy函数呢？

    scala> val data = List(3,1,90,3,5,12)
    data: List[Int] = List(3, 1, 90, 3, 5, 12)
     
    scala> val rdd = sc.parallelize(data)
    rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:14
     
    scala> rdd.collect
    res0: Array[Int] = Array(3, 1, 90, 3, 5, 12)
     
    scala> rdd.sortBy(x => x).collect
    res1: Array[Int] = Array(1, 3, 3, 5, 12, 90)
     
    scala> rdd.sortBy(x => x, false).collect
    res3: Array[Int] = Array(90, 12, 5, 3, 3, 1)
     
    scala> val result = rdd.sortBy(x => x, false)
    result: org.apache.spark.rdd.RDD[Int] = MappedRDD[23] at sortBy at <console>:16
     
    scala> result.partitions.size
    res9: Int = 2
     
    scala> val result = rdd.sortBy(x => x, false, 1)
    result: org.apache.spark.rdd.RDD[Int] = MappedRDD[26] at sortBy at <console>:16
     
    scala> result.partitions.size
    res10: Int = 1


上面的实例对rdd中的元素进行升序排序。并对排序后的RDD的分区个数进行了修改，上面的result就是排序后的RDD，默认的分区个数是2，而我们对它进行了修改，所以最后变成了1。

    val data = sc.parallelize(Array(("cc",12),("bb",32),("cc",22),("aa",18),("bb",16),("dd",16),("ee",54),("cc",1),("ff",13),("gg",68),("bb",4)))
    var sortbykey=data.sortByKey(false).collect
    sortbykey.foreach(x=>(println(x._1+" "+x._2)))


结果如下



测到的测试结果如上图所示，显然是根据Key进行了排序。

SortBy()函数

sortByKey函数作用于Key-Value形式的RDD，并对Key进行排序。它是在org.apache.spark.rdd.OrderedRDDFunctions中实现的，实现如下

    def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size)
        : RDD[(K, V)] =
    {
      val part = new RangePartitioner(numPartitions, self, ascending)
      new ShuffledRDD[K, V, V](self, part)
        .setKeyOrdering(if (ascending) ordering else ordering.reverse)
    }


从函数的实现可以看出，它主要接受两个函数，含义和sortBy一样，这里就不进行解释了。该函数返回的RDD一定是ShuffledRDD类型的，因为对源RDD进行排序，必须进行Shuffle操作，而Shuffle操作的结果RDD就是ShuffledRDD。其实这个函数的实现很优雅，里面用到了RangePartitioner，它可以使得相应的范围Key数据分到同一个partition中，然后内部用到了mapPartitions对每个partition中的数据进行排序，而每个partition中数据的排序用到了标准的sort机制，避免了大量数据的shuffle。下面对sortByKey的使用进行说明：

    scala> val a = sc.parallelize(List("wyp", "iteblog", "com", "397090770", "test"), 2)
    a: org.apache.spark.rdd.RDD[String] =ParallelCollectionRDD[30] at parallelize at <console>:12
     
    scala> val b = sc. parallelize (1 to a.count.toInt , 2)
    b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at <console>:14
     
    scala> val c = a.zip(b)
    c: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[32] at zip at <console>:16
     
    scala> c.sortByKey().collect
    res11: Array[(String, Int)] = Array((397090770,4), (com,3), (iteblog,2), (test,5), (wyp,1))


    val data = sc.parallelize(Array(("cc",12),("bb",32),("cc",22),("aa",18),("bb",16),("dd",16),("ee",54),("cc",1),("ff",13),("gg",68),("bb",4)))
    var sort=data.reduceByKey(_+_).sortBy(_._2,false).collect()
    sort.foreach(x=>(println(x._1+" "+x._2)))


结果如下



 显然，上图显示的结果是根据Value中的数据进行的排序。





# 海量数据算法

**数据倾斜的算子**

数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。

## （一）mapjoin解析

利用hive进行join连接操作，相较于MR有两种执行方案，一种为common join，另一种为map join ，map join是相对于common join的一种优化，省去shullfe和reduce的过程，大大的降低的作业运行的时间。 

select f.a,f.b from A t join B f  on ( f.a=t.a and f.ftime=20110802)  

该语句中B表有30亿行记录，A表只有100行记录，而且B表中数据倾斜特别严重，有一个key上有15亿行记录，在运行过程中特别的慢，而且在reduece的过程中遇有内存不够而报错。

为了解决用户的这个问题，考虑使用mapjoin,mapjoin的原理： 

> **MAPJION会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在map是进行了join操作，省去了reduce运行的效率也会高很多** 

这样就不会由于数据倾斜导致某个reduce上落数据太多而失败。于是原来的sql可以通过使用hint的方式指定join时使用mapjoin。 

> select /*+ mapjoin(A)*/ f.a,f.b from A t join B f  on ( f.a=t.a and f.ftime=20110802)  

再运行发现执行的效率比以前的写法高了好多。 

mapjoin还有一个很大的好处是能够进行不等连接的join操作，如果将不等条件写在where中，那么mapreduce过程中会进行笛卡尔积，运行效率特别低，如果使用mapjoin操作，在map的过程中就完成了不等值的join操作，效率会高很多。 

例子： 

select A.a ,A.b from A join B where A.a>B.a 

**简单总结一下，mapjoin的使用场景：**

1.关联操作中有一张表非常小

 2.不等值的链接操作

 **MapJoin原理**

![1565833762509](C:\Users\Administrator\Desktop\Md笔记\pic\1565833762509.png)



MapJoin简单说就是在Map阶段将小表读入内存，顺序扫描大表完成Join。

上图是Hive MapJoin的原理图，出自Facebook工程师Liyin Tang的一篇介绍Join优化的slice，从图中可以看出MapJoin分为两个阶段：

1. 通过MapReduce Local Task，将小表读入内存，生成HashTableFiles上传至Distributed Cache中，这里会对HashTableFiles进行压缩。
2. MapReduce Job在Map阶段，每个Mapper从Distributed Cache读取HashTableFiles到内存中，顺序扫描大表，在Map阶段直接进行Join，将数据传递给下一个MapReduce任务。

##  （二）美团网的spark调优

基础版  https://tech.meituan.com/2016/04/29/spark-tuning-basic.html

高级版  https://tech.meituan.com/2016/05/12/spark-tuning-pro.html

##  （三） 两阶段聚合（局部聚合+全局聚合）

**方案适用场景：**对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。

**方案实现思路：**这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。

**方案实现原理：**将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。

**方案优点：**对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。

**方案缺点：**仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。