# 002.Flume各组件的配置

## 1.Source

### 1.1 exec source

```shell
# 配置source
## exec表示flume回去调用给的命令，然后从给的命令的结果中去拿数据
a1.sources.r1.type = exec
## 使用tail这个命令来读数据
a1.sources.r1.command = tail -F /var/log/mysqld.log
a1.sources.r1.channels = c1
```

###  1.2 necat source

```shell
# 配置source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
```

### 1.3 spooldir source

```shell
# 配置spooldir source
a1.sources.r1.type = spooldir
## 指定要读的文件夹,读取/spooldir目录的数据
a1.sources.r1.spoolDir = /spooldir
```

### 1.4 Taildir source

```shell
# 配置taildir source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1 f2
# 配置taildir文件匹配模式y*.log(0索引位置不能用*) 或者指定某个文件
a1.sources.r1.filegroups.f1 =/taildir/y*.log
a1.sources.r1.filegroups.f2 =/taildir/yum.log
a1.sources.r1.positionFile=/soft/flume-1.9.0/position.json
```

### 1.5 avro source

```shell
# 配置avro source
a1.sources.r1.type = avro
# 主机地址
a1.sources.r1.bind = flume2
# 主机端口
a1.sources.r1.port = 8888
```

### 1.6 kafka source

```properties
a1.sources = r1
a1.channels = c1
a1.sinks = k1

a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.channels = channel1
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = kafka.gzstrong.com:9092
a1.sources.r1.kafka.topics = test
a1.sources.r1.kafka.consumer.group.id = custom.gid

# channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 10000
a1.channels.c1.transactionCapacity = 5000

# bind
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

a1.sinks.k1.type = logger
```



## 2.channel

###  2.1  memory channel

```shell
# 配置channel 使用内存的方式
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
```

### 2.2 file channel

```shell

```

### 2.3 channel副本策略

```shell
#定义一个agent组件
a1.sources = r1
#副本机制是两个channel c1和c2
a1.channels = c1 c2
#两个sink分别连接两个channel
a1.sinks = k1 k2

# 配置source
a1.sources.r1.type = ......

# 配置channel
# 配置channel1使用内存的方式
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# 配置channel2使用内存的方式
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# 配置sink1
a1.sinks.k1.type = ......
# 配置sink2
a1.sinks.k2.type = ......

# 将数据流复制给所有的channel 默认是这个设置可不设
a1.sources.r1.selector.type = replicating

# 通道source绑定c1 c2
a1.sources.r1.channels = c1 c2
# k1绑定c1
a1.sinks.k1.channel = c1
# k2绑定c2
a1.sinks.k2.channel = c2
```



## 3.sink

### 3.1 logger sink

```shell
# 配置sink类型
a1.sinks.k1.type = logger
```

### 3.2 avro sink

```
# 配置avro sink
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = flume2
a1.sinks.k1.port = 8888
```

### 3.3 file_roll sink

```shell
# 配置file_roll sink
a1.sinks.k1.type = file_roll
# 指定的本地文件系统的目录 不会自动创建，需手动创建
a1.sinks.k1.sink.directory = /file_roll
```

### 3.4 hdfs sink

```shell
# 配置sink hdfs接收数据
a1.sinks.k1.type = hdfs
## sinks.k1只能连接一个channel，source可以配置多个
a1.sinks.k1.channel = c1
## 下面的配置告诉用hdfs去写文件的时候写到什么位置，下面的表示不是写死的，而是可以动态的变化的。表示输出的目录名称是可变的
a1.sinks.k1.hdfs.path =hdfs://192.168.197.218:9000/flume/%y-%m-%d/%H%M/
##表示最后的文件的前缀
a1.sinks.k1.hdfs.filePrefix = events-
## 表示到了需要触发的时间时，是否要更新文件夹，true:表示要
a1.sinks.k1.hdfs.round = true
## 表示每隔1分钟改变一次
a1.sinks.k1.hdfs.roundValue = 1
## 切换文件的时候的时间单位是分钟
a1.sinks.k1.hdfs.roundUnit = minute
## 表示只要过了3秒钟，就切换生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 3
## 如果记录的文件大于20字节时切换一次
a1.sinks.k1.hdfs.rollSize = 20
## 当写了5个事件时触发
a1.sinks.k1.hdfs.rollCount = 5
## 收到了多少条消息往dfs中追加内容
a1.sinks.k1.hdfs.batchSize = 10
## 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件类型，默认是Sequencefile，可用DataStream：为普通文本
a1.sinks.k1.hdfs.fileType = DataStream
```

### 3.5 Sink组故障转移

```shell
#定义一个agent组件
a1.sources = r1
a1.channels = c1
#两个sink分别连接channel
a1.sinks = k1 k2
#定义组名称g1
a1.sinkgroups=g1

# 配置sources
a1.sources.r1.type = ......

# 配置channel 
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 配置avro sink
a1.sinks.k1.type = ......

a1.sinks.k2.type = ......

# 配置组策略
a1.sinkgroups.g1.sinks=k1 k2
a1.sinkgroups.g1.processor.type=failover
a1.sinkgroups.g1.processor.priority.k1=5
a1.sinkgroups.g1.processor.priority.k2=10
a1.sinkgroups.g1.processor.maxpenalty=10000

# 通道source绑定c1
a1.sources.r1.channels = c1
# k1绑定c1
a1.sinks.k1.channel = c1
# k2绑定c1
a1.sinks.k2.channel = c1
```

### 3.6 Sink组负载均衡

```shell
#定义一个agent组件
a1.sources = r1
a1.channels = c1
#两个sink分别连接channel
a1.sinks = k1 k2
#定义组名称g1
a1.sinkgroups=g1

# 配置sources
a1.sources.r1.type = ......

# 配置channel 
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 配置avro sink
a1.sinks.k1.type = ......

a1.sinks.k2.type = ......

# 配置组策略
a1.sinkgroups.g1.sinks=k1 k2
a1.sinkgroups.g1.processor.type=load_balance
a1.sinkgroups.g1.processor.backoff=true
a1.sinkgroups.g1.processor.selector=random

# 通道source绑定c1
a1.sources.r1.channels = c1
# k1绑定c1
a1.sinks.k1.channel = c1
# k2绑定c1
a1.sinks.k2.channel = c1
```
### 3.7 kafka sink

```
a1.sources=r1
a1.channels=c1
a1.sinks=k1

a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.bootstrap.servers = kafka.gzstrong.com:9092
a1.sinks.k1.kafka.topic = test
a1.sinks.k1.kafka.flumeBatchSize = 20 
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1

# channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# bind
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

