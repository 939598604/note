# 003.案例_tail采集本地文件数据到HDFS

## 1.编辑配置文件

在/soft/flume-1.9.0/conf目录下，新建一个flume的配置文件tail2Hdfs.conf

vim tail2Hdfs.conf

```shell
#定义一个agent组件
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source
## exec表示flume回去调用给的命令，然后从给的命令的结果中去拿数据
a1.sources.r1.type = exec
## 使用tail这个命令来读数据
a1.sources.r1.command = tail -F /var/log/mysqld.log
a1.sources.r1.channels = c1

# 配置sink hdfs接收数据
a1.sinks.k1.type = hdfs
## sinks.k1只能连接一个channel，source可以配置多个
a1.sinks.k1.channel = c1
## 下面的配置告诉用hdfs去写文件的时候写到什么位置，下面的表示不是写死的，而是可以动态的变化的。表示输出的目录名称是可变的
a1.sinks.k1.hdfs.path =hdfs://192.168.197.218:9000/flume/%y-%m-%d/%H%M/
##表示最后的文件的前缀
a1.sinks.k1.hdfs.filePrefix = events-
## 表示到了需要触发的时间时，是否要更新文件夹，true:表示要
a1.sinks.k1.hdfs.round = true
## 表示每隔1分钟改变一次
a1.sinks.k1.hdfs.roundValue = 1
## 切换文件的时候的时间单位是分钟
a1.sinks.k1.hdfs.roundUnit = minute
## 表示只要过了3秒钟，就切换生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 3
## 如果记录的文件大于20字节时切换一次
a1.sinks.k1.hdfs.rollSize = 20
## 当写了5个事件时触发
a1.sinks.k1.hdfs.rollCount = 5
## 收到了多少条消息往dfs中追加内容
a1.sinks.k1.hdfs.batchSize = 10
## 使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件类型，默认是Sequencefile，可用DataStream：为普通文本
a1.sinks.k1.hdfs.fileType = DataStream

# 配置channel 使用内存的方式
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 绑定通道
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

## 2.复制hdfs依赖的相关jar文件

（1）找需要的使用的hdfs机器，在机器复制${HADOOP_HOME}/share/hadoop/common文件夹下的所有jar 到安装flume agent机器的FLUME_HOME/lib目录

```shell
scp ${HADOOP_HOME}/share/hadoop/common/*.jar root@flume_host:/${FLUME_HOME}/lib
scp ${HADOOP_HOME}/share/hadoop/common/lib/*.jar root@flume_host:/${FLUME_HOME}/lib
```

（2）复制commons-configuration-1.6.jar到flume的lib目录。

```shell
scp ${HADOOP_HOME}/share/hadoop/common/lib/commons-configuration-1.6.jar  root@flume_host:/${FLUME_HOME}/lib/
```

（3）复制hadoop-auth-2.7.5.jar到flume的lib目录

```shell
scp ${HADOOP_HOME}/share/hadoop/common/lib/hadoop-auth-2.7.5.jar root@flume_host:/${FLUME_HOME}/lib/
```

或者执行下面的这个，不需要执行（1）、（2）、（3）了，接着执行第四步

```shell
cd ${HADOOP_HOME}/share/hadoop/common
scp *.jar lib/*.jar root@flume_host:/soft/flume-1.9.0/lib/
```

（4）复制hadoop-hdfs-2.7.5.jar到flume的lib目录

```
scp ${HADOOP_HOME}/share/hadoop/hdfs/hadoop-hdfs-2.7.5.jar root@flume_host:/${FLUME_HOME}/lib/
```

##  3.启动agent

```shell
/soft/flume-1.9.0/bin/flume-ng agent -c /soft/flume-1.9.0/conf -f /soft/flume-1.9.0/conf/tail2Hdfs.conf -n a1 -Dflume.root.logger=INFO,console
```

## 4.编写一直写入测试文件

vim write.sh

```
#/bin/bash
while true
do
echo $(date)> /var/log/mysqld.log
sleep 5
done
```

chmod +x write.sh

执行脚本 ./write.sh

## 5.查看HDFS浏览器目录

![](https://gitee.com/chenjinhua_939598604/resources/raw/img/static/QQ截图20191011173450.png)

## 6.hdfs命令行查看

```
[root@master ~]# /soft/hadoop-2.7.5/bin/hdfs dfs -ls /flume/19-10-11
Found 8 items
drwxr-xr-x   - root supergroup          0 2019-10-11 17:25 /flume/19-10-11/1725
drwxr-xr-x   - root supergroup          0 2019-10-11 17:27 /flume/19-10-11/1727
drwxr-xr-x   - root supergroup          0 2019-10-11 17:28 /flume/19-10-11/1728
drwxr-xr-x   - root supergroup          0 2019-10-11 17:32 /flume/19-10-11/1732
drwxr-xr-x   - root supergroup          0 2019-10-11 17:33 /flume/19-10-11/1733
drwxr-xr-x   - root supergroup          0 2019-10-11 17:34 /flume/19-10-11/1734
drwxr-xr-x   - root supergroup          0 2019-10-11 17:35 /flume/19-10-11/1735
drwxr-xr-x   - root supergroup          0 2019-10-11 17:36 /flume/19-10-11/1736
[root@master ~]# /soft/hadoop-2.7.5/bin/hdfs dfs -ls /flume/19-10-11/1736
Found 4 items
-rw-r--r--   3 root supergroup         43 2019-10-11 17:36 /flume/19-10-11/1736/events-.1570786561942
-rw-r--r--   3 root supergroup         43 2019-10-11 17:36 /flume/19-10-11/1736/events-.1570786566945
-rw-r--r--   3 root supergroup         43 2019-10-11 17:36 /flume/19-10-11/1736/events-.1570786571947
-rw-r--r--   3 root supergroup         43 2019-10-11 17:36 /flume/19-10-11/1736/events-.1570786576950
[root@master ~]# /soft/hadoop-2.7.5/bin/hdfs dfs -cat /flume/19-10-11/1736/events*
2019年 10月 11日 星期五 17:36:01 CST
2019年 10月 11日 星期五 17:36:06 CST
2019年 10月 11日 星期五 17:36:11 CST
2019年 10月 11日 星期五 17:36:16 CST
```

## 7.实验过程中遇到的问题总结

启动报错总结

**问题1: java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType**

```
[ERROR -org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:150)] Failed to start agent because dependencies were not found in classpath. Error follows.
java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType
	at org.apache.flume.sink.hdfs.HDFSEventSink.configure(HDFSEventSink.java:246)
	at org.apache.flume.conf.Configurables.configure(Configurables.java:41)
	at org.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:453)
	at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:106)
	at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.SequenceFile$CompressionType
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 12 more

```

解决方法：

```
copied comon jar files from hadoop folder to the flume folder.
cp /hadoop/share/hadoop/common/*.jar /root/flume/lib
cp /hadoop/share/hadoop/common/lib/*.jar /root/flume/lib

```

**问题2： Expected timestamp in the Flume event headers, but it was null**

```
java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was null

```

解决方法：

```
a1.sinks.k1.hdfs.useLocalTimeStamp = true

```

 **问题3：java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration** 

```
(SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:447)] process failed
java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<init>(DefaultMetricsSystem.java:38)
    at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.<clinit>(DefaultMetricsSystem.java:36)
    at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:106)
    at org.apache.hadoop.security.UserGroupInformation.<clinit>(UserGroupInformation.java:208)
    at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2554)
    at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2546)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2412)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
    at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:240)
    at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:232)
    at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:668)
    at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
    at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:665)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.configuration.Configuration
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 18 more

```

解决方法： 
缺少的依赖在commons-configuration-1.6.jar包里，这个包在${HADOOP_HOME}share/hadoop/common/lib/下，将其拷贝到flume的lib目录下。

```
cp ${HADOOP_HOME}share/hadoop/common/lib/commons-configuration-1.6.jar ${FLUME_HOME}/lib/


```

**问题4：No FileSystem for scheme: hdfs**

```
 [WARN - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:454)] HDFS IO error
java.io.IOException: No FileSystem for scheme: hdfs
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2658)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2665)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2701)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2683)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:372)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

```

解决方法：原因是缺少依赖：hadoop-hdfs-2.4.0.jar

```
cp ${HADOOP_HOME}share/hadoop/hdfs/hadoop-hdfs-2.4.0.jar ${FLUME_HOME}/lib/

```

**问题5：java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName**

```
2016-11-03 16:41:54,629 (SinkRunner-PollingRunner-DefaultSinkProcessor) [ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:447)] process failed
java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName

```

解决方法： 
缺少hadoop-auth-2.4.0.jar依赖，同样将其拷贝到flume的lib目录下：

```
cp ${HADOOP_HOME}share/hadoop/common/lib/hadoop-auth-2.4.0.jar ${FLUME_HOME}/lib/

```

**面试题：请将一下本地数据监控中的tail -F、spooldir 、Taildir的区别。**

- tail -F的方式默认是从第10行开始，如果agent挂掉了，会造成数据的丢失，不能保证数据不丢失。
- spooldir能够保证数据不丢失，而且能够实现断点续传，但是延迟较高，不能实时监控
- Taildir能够保证数据不丢失，而且能够实现断点续传，但是延迟较高，还能够进行实时监控



